{"posts":[{"fileName":"mla-collaborative-filtering","abstract":"<p>&quot;Collaborative filtering can be divided into users or items filtering. With its excellent speed and robustness, collaborative filtering is hot in the global Internet field. 😃😃&quot;</p>\n","description":"&quot;Collaborative filtering can be divided into users or items filtering. With its excellent speed and robustness, col...","title":"MLA -- collaborative filtering","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-collaborative-filtering.png","link":"https://zl-wu.github.io/post/mla-collaborative-filtering/","stats":{"text":"17 min read","time":989000,"words":2656,"minutes":17},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#i-user-based-cf\">I. User-Based CF</a>\n<ul>\n<li><a href=\"#1-find-users-with-similar-preferences\">1. Find users with similar preferences</a></li>\n<li><a href=\"#2-similarity-calculation\">2. Similarity Calculation</a>\n<ul>\n<li><a href=\"#21-euclidean-distance-evaluation\">2.1. Euclidean distance evaluation</a></li>\n<li><a href=\"#22-pearson-correlation-evaluation\">2.2. Pearson correlation evaluation</a></li>\n</ul>\n</li>\n<li><a href=\"#3-recommend-products-for-users\">3. Recommend products for users</a></li>\n<li><a href=\"#4-pros-and-cons-of-the-algorithm\">4. Pros and cons of the Algorithm</a></li>\n<li><a href=\"#5-code-example-python\">5. Code Example (Python)</a></li>\n</ul>\n</li>\n<li><a href=\"#ii-item-based-cf\">II. Item-Based CF</a><br>\n*\n<ul>\n<li><a href=\"#1-advantages\">1. Advantages</a></li>\n<li><a href=\"#2-disadvantages\">2. Disadvantages</a></li>\n</ul>\n</li>\n<li><a href=\"#iii-conclusion\">III. Conclusion</a></li>\n<li><a href=\"#iv-reference\">IV. Reference</a></li>\n</ul>\n","date":"2020-07-04 00:35:35","dateFormat":"2020-07-04"},{"fileName":"mla_decision_tree","abstract":"<p>Decision Tree is one of classical algorithms in machine learning. It can be used as both a classification model and a regression model. It is also suitable as a ensemble model, such as Random Forest. The history of Desicion Tree has three stages: ID3, C4.5 and CART</p>\n","description":"Decision Tree is one of classical algorithms in machine learning. It can be used as both a classification model and a re...","title":"MLA -- Decision Tree","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla_decision_tree.png","link":"https://zl-wu.github.io/post/mla_decision_tree/","stats":{"text":"39 min read","time":2330000,"words":6217,"minutes":39},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-the-information-theory-foundation-of-decision-tree-id3\">1. The Information Theory Foundation of Decision Tree ID3</a></li>\n<li><a href=\"#2-decision-tree-id3-algorithm-principle\">2. Decision Tree ID3 Algorithm Principle</a></li>\n<li><a href=\"#3-deficiency-of-decision-tree-id3-algorithm\">3. Deficiency of decision tree ID3 algorithm</a></li>\n<li><a href=\"#4-improvement-of-the-decision-tree-c45-algorithm\">4. Improvement of the Decision Tree C4.5 algorithm</a>\n<ul>\n<li><a href=\"#1-cannot-handle-continuous-features\">(1) Cannot handle continuous features:</a></li>\n<li><a href=\"#2-bias-of-features-with-more-kinds-of-values\">(2) Bias of features with more kinds of values:</a></li>\n<li><a href=\"#3-problem-of-missing-values\">(3) Problem of missing values:</a>\n<ul>\n<li><a href=\"#a-feature-choosing-with-missing-value\">a. Feature Choosing with missing value</a></li>\n<li><a href=\"#b-data-spliting-with-missing-value-in-a\">b. Data Spliting with missing value in A</a></li>\n</ul>\n</li>\n<li><a href=\"#4-overfitting-problem\">(4) Overfitting problem:</a></li>\n</ul>\n</li>\n<li><a href=\"#5-deficiency-of-decision-tree-c45-algorithm\">5. Deficiency of decision tree C4.5 algorithm</a></li>\n<li><a href=\"#6-cart-classification-tree-algorithm-gini-coefficient\">6. CART Classification Tree Algorithm -- Gini Coefficient</a></li>\n<li><a href=\"#7-cart-algorithm-on-continuous-features-and-discrete-features\">7. CART Algorithm on continuous features and discrete features</a><br>\n*\n<ul>\n<li><a href=\"#continuous-features\">continuous features</a></li>\n<li><a href=\"#discrete-features\">discrete features</a></li>\n</ul>\n</li>\n<li><a href=\"#8-cart-classification-tree-establishment-algorithm-specific-process\">8. CART classification tree establishment algorithm specific process</a></li>\n<li><a href=\"#9-cart-regression-tree-building-algorithm\">9. CART regression tree building algorithm</a></li>\n<li><a href=\"#10-pruning-of-cart-tree-algorithm\">10. Pruning of CART tree algorithm</a><br>\n*\n<ul>\n<li><a href=\"#a-loss-function-of-decision-tree\">a. loss function of decision tree</a></li>\n<li><a href=\"#b-idea-of-pruning\">b. idea of pruning</a></li>\n<li><a href=\"#c-cart-pruning-algorithm-process\">c. CART pruning algorithm process</a></li>\n</ul>\n</li>\n<li><a href=\"#11-summary-of-cart-algorithm\">11. Summary of CART algorithm</a></li>\n<li><a href=\"#12-summary-of-decision-tree\">12. Summary of Decision Tree</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-20 00:12:11","dateFormat":"2020-06-20"},{"fileName":"mla-logistic-regression-sklearn-instruction","abstract":"<p>This is a summary of using logistic regression libraries in sklearn. Focus on the matters that should be paid attention to in the tuning.</p>\n","description":"This is a summary of using logistic regression libraries in sklearn. Focus on the matters that should be paid attention ...","title":"MLA -- Logistic Regression (sklearn instruction)","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"","link":"https://zl-wu.github.io/post/mla-logistic-regression-sklearn-instruction/","stats":{"text":"12 min read","time":670000,"words":1788,"minutes":12},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-overview\">1. Overview</a></li>\n<li><a href=\"#2-regularization-parameter-penalty\">2. Regularization Parameter: Penalty</a></li>\n<li><a href=\"#3-optimization-algorithm-parameter-solver\">3. Optimization Algorithm Parameter: Solver</a></li>\n<li><a href=\"#4-classification-type-parameter-multi_class\">4. Classification Type Parameter: Multi_class</a></li>\n<li><a href=\"#5-type-weight-parameter-class_weight\">5. Type Weight Parameter: class_weight</a></li>\n<li><a href=\"#6-sample_weight\">6. sample_weight</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-11 00:42:45","dateFormat":"2020-06-11"},{"fileName":"mla-regularization-lasso-regression","abstract":"<p>&quot;What is the lasso regression? Add L2 norm into loss function.&quot;</p>\n","description":"&quot;What is the lasso regression? Add L2 norm into loss function.&quot; Norm is a commonly used concept in mathematic...","title":"MLA -- Regularization Lasso Regression","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-regularization-lasso-regression.png","link":"https://zl-wu.github.io/post/mla-regularization-lasso-regression/","stats":{"text":"9 min read","time":492000,"words":1314,"minutes":9},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#common-norm\">Common Norm:</a></li>\n<li><a href=\"#regularization-in-regression\">Regularization in Regression</a>\n<ul>\n<li><a href=\"#1-linear-regression-review\">1. Linear Regression Review</a></li>\n<li><a href=\"#2-ridge-regression-review\">2. Ridge Regression Review</a></li>\n<li><a href=\"#2-lasso-regression\">2. Lasso Regression</a></li>\n<li><a href=\"#3-coordinate-descent-method-for-lasso-regression\">3. Coordinate Descent method for Lasso Regression</a></li>\n<li><a href=\"#4-least-angle-regression-method-for-lasso-regression\">4. Least Angle Regression method for Lasso Regression</a><br>\n*\n<ul>\n<li><a href=\"#41-forward-selection-algorithm\">4.1 Forward Selection Algorithm</a></li>\n<li><a href=\"#42-forward-stagewise-algorithm\">4.2 Forward Stagewise Algorithm</a></li>\n<li><a href=\"#43-least-angle-regression-algorithm-lars\">4.3 Least Angle Regression Algorithm (LARS)</a></li>\n</ul>\n</li>\n<li><a href=\"#5-conclusion\">5. Conclusion</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-10 00:50:51","dateFormat":"2020-06-10"},{"fileName":"mla-logistic-regression","abstract":"<p>&quot;Logistic Regression is a kind of very classical and basic classification algorithm.&quot;</p>\n","description":"&quot;Logistic Regression is a kind of very classical and basic classification algorithm.&quot; Logistic Regression is ...","title":"MLA -- Logistic Regression","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-logistic-regression.png","link":"https://zl-wu.github.io/post/mla-logistic-regression/","stats":{"text":"20 min read","time":1173000,"words":3129,"minutes":20},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-from-linear-regression-to-logistic-regression\">1. From Linear Regression to Logistic Regression</a></li>\n<li><a href=\"#2-binary-logistic-regression-model\">2. Binary Logistic Regression model</a></li>\n<li><a href=\"#3-loss-function-of-binary-logistic-regression\">3. Loss Function of Binary Logistic Regression</a></li>\n<li><a href=\"#4-optimization-method-of-loss-function\">4. Optimization method of Loss Function</a><br>\n*\n<ul>\n<li><a href=\"#41-algebraic-expression\">4.1 Algebraic Expression</a></li>\n<li><a href=\"#42-matrix-expression\">4.2 Matrix Expression</a></li>\n</ul>\n</li>\n<li><a href=\"#5-regularization-of-logistic-regression\">5. Regularization of Logistic Regression</a></li>\n<li><a href=\"#6-promotion-of-binary-multiple-logistic-regression\">6. Promotion of binary : Multiple Logistic Regression</a></li>\n<li><a href=\"#7-conclusion\">7. Conclusion</a>\n<ul>\n<li><a href=\"#question-for-logistic-regression-why-is-the-square-sum-of-error-non-convex-and-not-suitable-as-the-loss-function\">Question: For logistic regression, why is the Square Sum of Error non-convex and not suitable as the loss function?</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-10 00:43:57","dateFormat":"2020-06-10"},{"fileName":"mla-linear-regression-principle","abstract":"<p>&quot;Linear Regression is the beginning and the most basic algorithm.&quot;</p>\n","description":"&quot;Linear Regression is the beginning and the most basic algorithm.&quot; Linear Regression is the most basic questi...","title":"MLA -- Linear Regression Principle","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-linear-regression-principle.png","link":"https://zl-wu.github.io/post/mla-linear-regression-principle/","stats":{"text":"9 min read","time":502000,"words":1341,"minutes":9},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-linear-regression-question\">1. Linear Regression Question</a></li>\n<li><a href=\"#2-linear-regression-model\">2. Linear Regression Model</a></li>\n<li><a href=\"#3-linear-regression-algorithm\">3. Linear Regression Algorithm</a></li>\n<li><a href=\"#41-generalization-of-linear-regression-polynomial-regression\">4.1. Generalization of linear regression: Polynomial Regression</a></li>\n<li><a href=\"#42-generalization-of-linear-regression-generalized-linear-regression\">4.2. Generalization of linear regression: Generalized Linear Regression</a></li>\n<li><a href=\"#5-regularization-of-linear-regression\">5. Regularization of linear regression</a>\n<ul>\n<li><a href=\"#51-l1-regularization\">5.1 L1 regularization</a></li>\n<li><a href=\"#52-l2-regularization\">5.2 L2 regularization</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-01 00:41:08","dateFormat":"2020-06-01"},{"fileName":"mla-least-square-method","abstract":"<p>&quot;A standard approach in regression analysis to approximate the solution of overdetermined systems.&quot;</p>\n","description":"&quot;A standard approach in regression analysis to approximate the solution of overdetermined systems.&quot; The metho...","title":"MLA -- Least Square method","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-least-square-method.jpg","link":"https://zl-wu.github.io/post/mla-least-square-method/","stats":{"text":"8 min read","time":472000,"words":1259,"minutes":8},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-least-square-principle\">1. Least Square principle</a></li>\n<li><a href=\"#2-least-squre-solution\">2. Least Squre solution</a><br>\n*\n<ul>\n<li><a href=\"#21-algebraic-way\">2.1 Algebraic way</a></li>\n<li><a href=\"#22-matrix-way\">2.2 Matrix way</a></li>\n</ul>\n</li>\n<li><a href=\"#3-limitations-and-applicable-scenarios-of-least-square-method\">3. Limitations and applicable scenarios of least square method</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-05-25 00:39:24","dateFormat":"2020-05-25"},{"fileName":"mla-gradient-descent-method","abstract":"<p>When improving the model parameters of machine learning algorithms, that is, unconstrained optimalization problem.<br>\n<strong>Gradient Descent method</strong> is one of most common used methods. The other one is the least square method. Here is a complete summary of the gradient descent method.</p>\n","description":"When improving the model parameters of machine learning algorithms, that is, unconstrained optimalization problem. Gradi...","title":"MLA -- Gradient Descent method","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-gradient-descent-method.png","link":"https://zl-wu.github.io/post/mla-gradient-descent-method/","stats":{"text":"24 min read","time":1402000,"words":3740,"minutes":24},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-what-is-gradient\">1. What is gradient?</a></li>\n<li><a href=\"#2-gradient-descent\">2. Gradient Descent</a></li>\n<li><a href=\"#3-concepts-of-gradient-descent\">3. Concepts of gradient descent</a></li>\n<li><a href=\"#4-gradient-descent-algorithm\">4. Gradient Descent Algorithm</a><br>\n*\n<ul>\n<li><a href=\"#41-algebraic-expression\">4.1 Algebraic Expression</a></li>\n<li><a href=\"#42-matrix-expression\">4.2 Matrix Expression</a></li>\n</ul>\n</li>\n<li><a href=\"#5-tuning-of-gradient-descent-algorithm\">5. Tuning of gradient descent algorithm</a></li>\n<li><a href=\"#6-gradient-descent-family-bgd-sgd-mbgd\">6. Gradient Descent Family (BGD, SGD, MBGD)</a><br>\n*\n<ul>\n<li><a href=\"#61-bgd-batch-gradient-descent\">6.1 BGD (Batch Gradient Descent)</a></li>\n<li><a href=\"#62-sgd-stochastic-gradient-descent\">6.2 SGD (Stochastic Gradient Descent)</a></li>\n<li><a href=\"#63-mbgd-mini-batch-gradient-descent\">6.3 MBGD (Mini-batch Gradient Descent)</a></li>\n</ul>\n</li>\n<li><a href=\"#7-comparison-of-gradient-descent-method-and-other-optimization-algorithms\">7. Comparison of gradient descent method and other optimization algorithms</a></li>\n<li><a href=\"#python-implementation-example\">Python implementation Example</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-05-16 00:37:42","dateFormat":"2020-05-16"},{"fileName":"mla-naive-bayes-theory","abstract":"<p>&quot;Priori Probability + data = Posterior probability&quot;</p>\n","description":"&quot;Priori Probability + data = Posterior probability&quot; 条件概率： P(X|Y) or P(Y|X) 联合概率： P(X,Y), P(XY) or P(XnY) 边缘概率...","title":"MLA -- Naive Bayes theory","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-naive-bayes-theory.jpg","link":"https://zl-wu.github.io/post/mla-naive-bayes-theory/","stats":{"text":"14 min read","time":781000,"words":2106,"minutes":14},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#bayesian-interpretation\">Bayesian interpretation</a></li>\n<li><a href=\"#naive-bayes-parameter-estimation\">Naive Bayes parameter estimation</a></li>\n<li><a href=\"#naive-bayes-algorithm-process\">Naive Bayes algorithm process</a></li>\n<li><a href=\"#pros-and-cons-conclusion\">Pros and cons Conclusion</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-05-15 00:47:58","dateFormat":"2020-05-15"},{"fileName":"python-code-of-binomial-distribution","abstract":"<p>&quot;python code to play with binomial distribution&quot;</p>\n","description":"&quot;python code to play with binomial distribution&quot; Binomial Distribution import numpy as np import matplotlib.p...","title":"Python code of binomial distribution","tags":[{"name":"python","slug":"aIIIsuMtu","used":true,"link":"https://zl-wu.github.io/tag/aIIIsuMtu/"},{"name":"statistic","slug":"x8V5slgayJ","used":true,"link":"https://zl-wu.github.io/tag/x8V5slgayJ/"}],"feature":"","link":"https://zl-wu.github.io/post/python-code-of-binomial-distribution/","stats":{"text":"2 min read","time":76000,"words":237,"minutes":2},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li>\n<ul>\n<li><a href=\"#binomial-distribution\">Binomial Distribution</a></li>\n<li><a href=\"#bernoulli-distribution\">Bernoulli Distribution</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","date":"2020-04-20 01:57:30","dateFormat":"2020-04-20"},{"fileName":"markdown-syntax-note-continuously-updating","abstract":"","description":"LaTex Greek Alphabet correspondence table: Alphabet LaTex Code α\\alphaα $\\alpha$ β\\betaβ $\\beta$ γ\\gammaγ $\\g...","title":"Markdown syntax note (Continuously updating)","tags":[{"name":"LaTex","slug":"rZoZvwq3o","used":true,"link":"https://zl-wu.github.io/tag/rZoZvwq3o/"}],"feature":"","link":"https://zl-wu.github.io/post/markdown-syntax-note-continuously-updating/","stats":{"text":"1 min read","time":46000,"words":125,"minutes":1},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#latex-greek-alphabet-correspondence-table\">LaTex Greek Alphabet correspondence table:</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-04-11 01:49:44","dateFormat":"2020-04-11"},{"fileName":"topic-modeling-in-gensim","abstract":"<p>&quot;Research the source code of Topic Modeling in gensim&quot;</p>\n","description":"&quot;Research the source code of Topic Modeling in gensim&quot; A Simple Example Code from nltk.tokenize import regexp...","title":"Topic modeling in Gensim","tags":[{"name":"gensim","slug":"gMp6EUZzT","used":true,"link":"https://zl-wu.github.io/tag/gMp6EUZzT/"},{"name":"topic modeling","slug":"jkstOb8bp0","used":true,"link":"https://zl-wu.github.io/tag/jkstOb8bp0/"}],"feature":"","link":"https://zl-wu.github.io/post/topic-modeling-in-gensim/","stats":{"text":"10 min read","time":578000,"words":1545,"minutes":10},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#a-simple-example-code\">A Simple Example Code</a></li>\n<li><a href=\"#1-dictionary-source\">1. Dictionary  (source)</a></li>\n<li><a href=\"#2-tfidfmodel-source\">2. TfidfModel (source)</a></li>\n<li><a href=\"#3-lsimodel-source\">3. LsiModel (source)</a></li>\n<li><a href=\"#4-ldamodel\">4. LdaModel</a></li>\n<li><a href=\"#5-ldamulticore\">5. LdaMulticore</a></li>\n<li><a href=\"#6-coherencemodel\">6. CoherenceModel</a></li>\n<li><a href=\"#example-of-sklearn-topic-modeling\">Example of sklearn topic modeling</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-03-24 01:38:53","dateFormat":"2020-03-24"},{"fileName":"compare-nltk-and-textblobs-sentiment-analysis","abstract":"<p>&quot;This is a comparison report between nltk and textblob in sentiment analysis.&quot;</p>\n","description":"&quot;This is a comparison report between nltk and textblob in sentiment analysis.&quot; Overview lexicon 1. NLTK's vad...","title":"Compare nltk and textblob's sentiment analysis","tags":[{"name":"NLP","slug":"krjdQmA74","used":true,"link":"https://zl-wu.github.io/tag/krjdQmA74/"},{"name":"Source Code","slug":"8gpF3Gc2sD","used":true,"link":"https://zl-wu.github.io/tag/8gpF3Gc2sD/"}],"feature":"","link":"https://zl-wu.github.io/post/compare-nltk-and-textblobs-sentiment-analysis/","stats":{"text":"16 min read","time":919000,"words":2452,"minutes":16},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#overview\">Overview</a></li>\n<li><a href=\"#lexicon\">lexicon</a><br>\n*\n<ul>\n<li><a href=\"#1-nltks-vader_lexicontxt-nltks-default-lexicon-file-online-source\">1. NLTK's vader_lexicon.txt : NLTK's default lexicon file (online source)</a></li>\n<li><a href=\"#2-textblobs-en-sentimentxml-textblobs-default-lexicon-file-online-source\">2. TextBlob's en-sentiment.xml : TextBlob's default lexicon file (online source)</a></li>\n<li><a href=\"#summary\">Summary</a></li>\n</ul>\n</li>\n<li><a href=\"#main-algorithm\">Main Algorithm</a><br>\n*\n<ul>\n<li><a href=\"#1-nltks-methods\">1. NLTK's methods</a></li>\n<li><a href=\"#2-textblobs-methods\">2. TextBlob's methods</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","date":"2020-03-21 01:54:37","dateFormat":"2020-03-21"},{"fileName":"text-analysis-nltks-vader-library","abstract":"<p>&quot;Topic: How does NLTK's vader library work to calculate sentiment value for text?&quot;</p>\n","description":"&quot;Topic: How does NLTK's vader library work to calculate sentiment value for text?&quot; Topic: How does NLTK's vad...","title":"Text Analysis -- NLTK's vader library","tags":[{"name":"NLP","slug":"krjdQmA74","used":true,"link":"https://zl-wu.github.io/tag/krjdQmA74/"},{"name":"Source Code","slug":"8gpF3Gc2sD","used":true,"link":"https://zl-wu.github.io/tag/8gpF3Gc2sD/"}],"feature":"https://zl-wu.github.io/post-images/text-analysis-nltks-vader-library.png","link":"https://zl-wu.github.io/post/text-analysis-nltks-vader-library/","stats":{"text":"20 min read","time":1176000,"words":3136,"minutes":20},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#topic-how-does-nltks-vader-library-work-to-calculate-sentiment-value-for-text\">Topic: How does NLTK's vader library work to calculate sentiment value for text?</a></li>\n<li><a href=\"#overview\">Overview</a>\n<ul>\n<li><a href=\"#one-simple-example-of-using-vader\">One simple example of using vader</a></li>\n</ul>\n</li>\n<li><a href=\"#class-sentimentintensityanalyzer\">Class &quot;SentimentIntensityAnalyzer&quot;</a>\n<ul>\n<li><a href=\"#1-method-polarity_scores\">1. Method &quot;polarity_scores()&quot;:</a></li>\n<li><a href=\"#2-method-sentiment_valence\">2. Method &quot;sentiment_valence()&quot;:</a></li>\n<li><a href=\"#3-method-score_valence\">3. Method &quot;score_valence()&quot;:</a></li>\n</ul>\n</li>\n<li><a href=\"#class-sentitext\">Class &quot;SentiText&quot;</a></li>\n<li><a href=\"#class-vaderconstants\">Class &quot;VaderConstants&quot;</a></li>\n<li><a href=\"#database-vader_lexicontxt\">Database &quot;vader_lexicon.txt&quot;</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-03-20 01:46:36","dateFormat":"2020-03-20"},{"fileName":"text-analysis-textblob-sentiment","abstract":"<p>&quot;Topic: How to calculate sentiment value in TextBlob library?&quot;</p>\n","description":"&quot;Topic: How to calculate sentiment value in TextBlob library?&quot; Topic: How to calculate sentiment value in Tex...","title":"Text Analysis -- TextBlob sentiment","tags":[{"name":"NLP","slug":"krjdQmA74","used":true,"link":"https://zl-wu.github.io/tag/krjdQmA74/"},{"name":"Source Code","slug":"8gpF3Gc2sD","used":true,"link":"https://zl-wu.github.io/tag/8gpF3Gc2sD/"}],"feature":"https://zl-wu.github.io/post-images/text-analysis-textblob-sentiment.png","link":"https://zl-wu.github.io/post/text-analysis-textblob-sentiment/","stats":{"text":"24 min read","time":1411000,"words":3763,"minutes":24},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#topic-how-to-calculate-sentiment-value-in-textblob-library\">Topic: How to calculate sentiment value in TextBlob library?</a></li>\n<li><a href=\"#a-simple-example-code\">A Simple Example Code</a></li>\n<li><a href=\"#class-textblobbaseblob-source-code\">Class TextBlob(BaseBlob) source code</a></li>\n<li><a href=\"#class-baseblobstringlikemixin-blobcomparablemixin-source-code\">Class BaseBlob(StringlikeMixin, BlobComparableMixin) source code</a></li>\n<li><a href=\"#class-patternanalyzerbasesentimentanalyzer-source-code\">Class PatternAnalyzer(BaseSentimentAnalyzer) source code</a></li>\n<li><a href=\"#class-sentimentlazydict-source-code\">Class Sentiment(lazydict) source code</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-03-19 01:42:41","dateFormat":"2020-03-19"},{"fileName":"git-you-er-yuan-ru-men","abstract":"","description":"&quot;这只是一个 Git 新手基础入门（幼儿园级别）&quot; Git Mind map: VCS（版本管理系统）发展 服务器文档式（VCS出现之前） 集中式VCS 分布式VCS （Git [Linux支付创造] ） Git ...","title":"Git 幼儿园入门","tags":[{"name":"git","slug":"GbFSc4g52","used":true,"link":"https://zl-wu.github.io/tag/GbFSc4g52/"}],"feature":"","link":"https://zl-wu.github.io/post/git-you-er-yuan-ru-men/","stats":{"text":"27 min read","time":1580000,"words":5513,"minutes":27},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#git\">Git</a>\n<ul>\n<li><a href=\"#vcs%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E5%8F%91%E5%B1%95\">VCS（版本管理系统）发展</a></li>\n<li><a href=\"#git-%E7%9A%84%E7%89%B9%E7%82%B9\">Git 的特点</a></li>\n<li><a href=\"#devops-%E5%B7%A5%E5%85%B7%E5%85%A8%E6%B5%81%E7%A8%8B%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F\">DevOps 工具[全流程生命周期]</a></li>\n<li><a href=\"#%E5%AE%89%E8%A3%85git\">安装Git</a>\n<ul>\n<li><a href=\"#%E9%85%8D%E7%BD%AEuser\">配置USER</a></li>\n<li><a href=\"#config%E7%9A%84%E4%B8%89%E4%B8%AA%E4%BD%9C%E7%94%A8%E5%9F%9F\">config的三个作用域</a></li>\n</ul>\n</li>\n<li><a href=\"#%E5%BB%BA%E7%AB%8Bgit%E4%BB%93%E5%BA%93\">建立Git仓库</a></li>\n<li><a href=\"#git%E7%9A%84%E6%9A%82%E5%AD%98%E5%8C%BA%E6%A6%82%E5%BF%B5\">Git的暂存区概念</a>\n<ul>\n<li><a href=\"#git-%E6%96%87%E4%BB%B6%E9%87%8D%E5%91%BD%E5%90%8D\">Git 文件重命名</a></li>\n<li><a href=\"#git-log%E6%9F%A5%E7%9C%8B%E7%89%88%E6%9C%AC%E6%BC%94%E5%8F%98%E5%8E%86%E5%8F%B2\">Git log查看版本演变历史</a></li>\n<li><a href=\"#author-committer\">Author &amp; Committer</a></li>\n<li><a href=\"#%E6%8E%A2%E7%A9%B6-git-%E7%9B%AE%E5%BD%95\"><em><strong>探究 .git 目录</strong></em></a></li>\n</ul>\n</li>\n<li><a href=\"#commit-tree-blob%E4%B8%89%E4%B8%AA%E5%AF%B9%E8%B1%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB\">commit、tree、blob三个对象之间的关系</a></li>\n<li><a href=\"#git-%E5%91%BD%E4%BB%A4%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E6%95%B0%E4%B8%80%E6%95%B0tree%E7%9A%84%E4%B8%AA%E6%95%B0\">Git 命令深度解析（数一数tree的个数）</a></li>\n<li><a href=\"#%E5%88%86%E7%A6%BB%E5%A4%B4%E6%8C%87%E9%92%88%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9\">分离头指针注意事项？</a></li>\n<li><a href=\"#%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%90%86%E8%A7%A3head%E5%92%8Cbranch\">进一步理解head和branch</a></li>\n</ul>\n</li>\n</ul>\n","date":"2019-11-20 01:59:28","dateFormat":"2019-11-20"}],"tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/","count":9},{"name":"python","slug":"aIIIsuMtu","used":true,"link":"https://zl-wu.github.io/tag/aIIIsuMtu/","count":1},{"name":"statistic","slug":"x8V5slgayJ","used":true,"link":"https://zl-wu.github.io/tag/x8V5slgayJ/","count":1},{"name":"LaTex","slug":"rZoZvwq3o","used":true,"link":"https://zl-wu.github.io/tag/rZoZvwq3o/","count":1},{"name":"gensim","slug":"gMp6EUZzT","used":true,"link":"https://zl-wu.github.io/tag/gMp6EUZzT/","count":1},{"name":"topic modeling","slug":"jkstOb8bp0","used":true,"link":"https://zl-wu.github.io/tag/jkstOb8bp0/","count":1},{"name":"NLP","slug":"krjdQmA74","used":true,"link":"https://zl-wu.github.io/tag/krjdQmA74/","count":3},{"name":"Source Code","slug":"8gpF3Gc2sD","used":true,"link":"https://zl-wu.github.io/tag/8gpF3Gc2sD/","count":3},{"name":"git","slug":"GbFSc4g52","used":true,"link":"https://zl-wu.github.io/tag/GbFSc4g52/","count":1}],"menus":[{"link":"/","name":"首页","openType":"Internal"},{"link":"/archives","name":"归档","openType":"Internal"},{"link":"/tags","name":"标签","openType":"Internal"},{"link":"/post/about","name":"关于","openType":"Internal"}],"themeConfig":{"themeName":"gridea-theme-pure","postPageSize":10,"archivesPageSize":50,"siteName":"ZL Wu's notebook","siteDescription":"不废话，就是干","footerInfo":"Powered by <a href=\"https://github.com/getgridea/gridea\" target=\"_blank\">Gridea</a>","showFeatureImage":true,"domain":"https://zl-wu.github.io","postUrlFormat":"SLUG","tagUrlFormat":"SHORT_ID","dateFormat":"YYYY-MM-DD","feedFullText":false,"feedCount":10,"archivesPath":"archives","postPath":"post","tagPath":"tag"},"customConfig":{"APP_ID":"","APP_KEY":"","about":"","avatar":"","caf":"#84fab0","ccf":"#5f6169","ccs":"#999fa7","ctf":"#ffffff","cts":"#dddddd","customCss":"","descfriend":"","dribbble":"","facebook":"","friends":[],"ga":"","github":"","isEnabledCustomColor":false,"pageSize":"5","placeholder":"Just Go Go","recordIp":false,"skin":"gray","twitter":"","vMaxWidth":"1000","vPadding":"2.5%","vPercentWidth":"100","valine":false,"visitor":true,"weibo":"","zhihu":""},"utils":{"now":1611475703816}}
