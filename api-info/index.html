{"posts":[{"fileName":"mla-collaborative-filtering","abstract":"<p>&quot;Collaborative filtering can be divided into users or items filtering. With its excellent speed and robustness, collaborative filtering is hot in the global Internet field. üòÉüòÉ&quot;</p>\n","description":"&quot;Collaborative filtering can be divided into users or items filtering. With its excellent speed and robustness, col...","title":"MLA -- collaborative filtering","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-collaborative-filtering.png","link":"https://zl-wu.github.io/post/mla-collaborative-filtering/","stats":{"text":"17 min read","time":989000,"words":2656,"minutes":17},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#i-user-based-cf\">I. User-Based CF</a>\n<ul>\n<li><a href=\"#1-find-users-with-similar-preferences\">1. Find users with similar preferences</a></li>\n<li><a href=\"#2-similarity-calculation\">2. Similarity Calculation</a>\n<ul>\n<li><a href=\"#21-euclidean-distance-evaluation\">2.1. Euclidean distance evaluation</a></li>\n<li><a href=\"#22-pearson-correlation-evaluation\">2.2. Pearson correlation evaluation</a></li>\n</ul>\n</li>\n<li><a href=\"#3-recommend-products-for-users\">3. Recommend products for users</a></li>\n<li><a href=\"#4-pros-and-cons-of-the-algorithm\">4. Pros and cons of the Algorithm</a></li>\n<li><a href=\"#5-code-example-python\">5. Code Example (Python)</a></li>\n</ul>\n</li>\n<li><a href=\"#ii-item-based-cf\">II. Item-Based CF</a><br>\n*\n<ul>\n<li><a href=\"#1-advantages\">1. Advantages</a></li>\n<li><a href=\"#2-disadvantages\">2. Disadvantages</a></li>\n</ul>\n</li>\n<li><a href=\"#iii-conclusion\">III. Conclusion</a></li>\n<li><a href=\"#iv-reference\">IV. Reference</a></li>\n</ul>\n","date":"2020-07-04 00:35:35","dateFormat":"2020-07-04"},{"fileName":"mla_decision_tree","abstract":"<p>Decision Tree is one of classical algorithms in machine learning. It can be used as both a classification model and a regression model. It is also suitable as a ensemble model, such as Random Forest. The history of Desicion Tree has three stages: ID3, C4.5 and CART</p>\n","description":"Decision Tree is one of classical algorithms in machine learning. It can be used as both a classification model and a re...","title":"MLA -- Decision Tree","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla_decision_tree.png","link":"https://zl-wu.github.io/post/mla_decision_tree/","stats":{"text":"39 min read","time":2330000,"words":6217,"minutes":39},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-the-information-theory-foundation-of-decision-tree-id3\">1. The Information Theory Foundation of Decision Tree ID3</a></li>\n<li><a href=\"#2-decision-tree-id3-algorithm-principle\">2. Decision Tree ID3 Algorithm Principle</a></li>\n<li><a href=\"#3-deficiency-of-decision-tree-id3-algorithm\">3. Deficiency of decision tree ID3 algorithm</a></li>\n<li><a href=\"#4-improvement-of-the-decision-tree-c45-algorithm\">4. Improvement of the Decision Tree C4.5 algorithm</a>\n<ul>\n<li><a href=\"#1-cannot-handle-continuous-features\">(1) Cannot handle continuous features:</a></li>\n<li><a href=\"#2-bias-of-features-with-more-kinds-of-values\">(2) Bias of features with more kinds of values:</a></li>\n<li><a href=\"#3-problem-of-missing-values\">(3) Problem of missing values:</a>\n<ul>\n<li><a href=\"#a-feature-choosing-with-missing-value\">a. Feature Choosing with missing value</a></li>\n<li><a href=\"#b-data-spliting-with-missing-value-in-a\">b. Data Spliting with missing value in A</a></li>\n</ul>\n</li>\n<li><a href=\"#4-overfitting-problem\">(4) Overfitting problem:</a></li>\n</ul>\n</li>\n<li><a href=\"#5-deficiency-of-decision-tree-c45-algorithm\">5. Deficiency of decision tree C4.5 algorithm</a></li>\n<li><a href=\"#6-cart-classification-tree-algorithm-gini-coefficient\">6. CART Classification Tree Algorithm -- Gini Coefficient</a></li>\n<li><a href=\"#7-cart-algorithm-on-continuous-features-and-discrete-features\">7. CART Algorithm on continuous features and discrete features</a><br>\n*\n<ul>\n<li><a href=\"#continuous-features\">continuous features</a></li>\n<li><a href=\"#discrete-features\">discrete features</a></li>\n</ul>\n</li>\n<li><a href=\"#8-cart-classification-tree-establishment-algorithm-specific-process\">8. CART classification tree establishment algorithm specific process</a></li>\n<li><a href=\"#9-cart-regression-tree-building-algorithm\">9. CART regression tree building algorithm</a></li>\n<li><a href=\"#10-pruning-of-cart-tree-algorithm\">10. Pruning of CART tree algorithm</a><br>\n*\n<ul>\n<li><a href=\"#a-loss-function-of-decision-tree\">a. loss function of decision tree</a></li>\n<li><a href=\"#b-idea-of-pruning\">b. idea of pruning</a></li>\n<li><a href=\"#c-cart-pruning-algorithm-process\">c. CART pruning algorithm process</a></li>\n</ul>\n</li>\n<li><a href=\"#11-summary-of-cart-algorithm\">11. Summary of CART algorithm</a></li>\n<li><a href=\"#12-summary-of-decision-tree\">12. Summary of Decision Tree</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-20 00:12:11","dateFormat":"2020-06-20"},{"fileName":"mla-logistic-regression-sklearn-instruction","abstract":"<p>This is a summary of using logistic regression libraries in sklearn. Focus on the matters that should be paid attention to in the tuning.</p>\n","description":"This is a summary of using logistic regression libraries in sklearn. Focus on the matters that should be paid attention ...","title":"MLA -- Logistic Regression (sklearn instruction)","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"","link":"https://zl-wu.github.io/post/mla-logistic-regression-sklearn-instruction/","stats":{"text":"12 min read","time":670000,"words":1788,"minutes":12},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-overview\">1. Overview</a></li>\n<li><a href=\"#2-regularization-parameter-penalty\">2. Regularization Parameter: Penalty</a></li>\n<li><a href=\"#3-optimization-algorithm-parameter-solver\">3. Optimization Algorithm Parameter: Solver</a></li>\n<li><a href=\"#4-classification-type-parameter-multi_class\">4. Classification Type Parameter: Multi_class</a></li>\n<li><a href=\"#5-type-weight-parameter-class_weight\">5. Type Weight Parameter: class_weight</a></li>\n<li><a href=\"#6-sample_weight\">6. sample_weight</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-11 00:42:45","dateFormat":"2020-06-11"},{"fileName":"mla-regularization-lasso-regression","abstract":"<p>&quot;What is the lasso regression? Add L2 norm into loss function.&quot;</p>\n","description":"&quot;What is the lasso regression? Add L2 norm into loss function.&quot; Norm is a commonly used concept in mathematic...","title":"MLA -- Regularization Lasso Regression","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-regularization-lasso-regression.png","link":"https://zl-wu.github.io/post/mla-regularization-lasso-regression/","stats":{"text":"9 min read","time":492000,"words":1314,"minutes":9},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#common-norm\">Common Norm:</a></li>\n<li><a href=\"#regularization-in-regression\">Regularization in Regression</a>\n<ul>\n<li><a href=\"#1-linear-regression-review\">1. Linear Regression Review</a></li>\n<li><a href=\"#2-ridge-regression-review\">2. Ridge Regression Review</a></li>\n<li><a href=\"#2-lasso-regression\">2. Lasso Regression</a></li>\n<li><a href=\"#3-coordinate-descent-method-for-lasso-regression\">3. Coordinate Descent method for Lasso Regression</a></li>\n<li><a href=\"#4-least-angle-regression-method-for-lasso-regression\">4. Least Angle Regression method for Lasso Regression</a><br>\n*\n<ul>\n<li><a href=\"#41-forward-selection-algorithm\">4.1 Forward Selection Algorithm</a></li>\n<li><a href=\"#42-forward-stagewise-algorithm\">4.2 Forward Stagewise Algorithm</a></li>\n<li><a href=\"#43-least-angle-regression-algorithm-lars\">4.3 Least Angle Regression Algorithm (LARS)</a></li>\n</ul>\n</li>\n<li><a href=\"#5-conclusion\">5. Conclusion</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-10 00:50:51","dateFormat":"2020-06-10"},{"fileName":"mla-logistic-regression","abstract":"<p>&quot;Logistic Regression is a kind of very classical and basic classification algorithm.&quot;</p>\n","description":"&quot;Logistic Regression is a kind of very classical and basic classification algorithm.&quot; Logistic Regression is ...","title":"MLA -- Logistic Regression","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-logistic-regression.png","link":"https://zl-wu.github.io/post/mla-logistic-regression/","stats":{"text":"20 min read","time":1173000,"words":3129,"minutes":20},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-from-linear-regression-to-logistic-regression\">1. From Linear Regression to Logistic Regression</a></li>\n<li><a href=\"#2-binary-logistic-regression-model\">2. Binary Logistic Regression model</a></li>\n<li><a href=\"#3-loss-function-of-binary-logistic-regression\">3. Loss Function of Binary Logistic Regression</a></li>\n<li><a href=\"#4-optimization-method-of-loss-function\">4. Optimization method of Loss Function</a><br>\n*\n<ul>\n<li><a href=\"#41-algebraic-expression\">4.1 Algebraic Expression</a></li>\n<li><a href=\"#42-matrix-expression\">4.2 Matrix Expression</a></li>\n</ul>\n</li>\n<li><a href=\"#5-regularization-of-logistic-regression\">5. Regularization of Logistic Regression</a></li>\n<li><a href=\"#6-promotion-of-binary-multiple-logistic-regression\">6. Promotion of binary : Multiple Logistic Regression</a></li>\n<li><a href=\"#7-conclusion\">7. Conclusion</a>\n<ul>\n<li><a href=\"#question-for-logistic-regression-why-is-the-square-sum-of-error-non-convex-and-not-suitable-as-the-loss-function\">Question: For logistic regression, why is the Square Sum of Error non-convex and not suitable as the loss function?</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-10 00:43:57","dateFormat":"2020-06-10"},{"fileName":"mla-linear-regression-principle","abstract":"<p>&quot;Linear Regression is the beginning and the most basic algorithm.&quot;</p>\n","description":"&quot;Linear Regression is the beginning and the most basic algorithm.&quot; Linear Regression is the most basic questi...","title":"MLA -- Linear Regression Principle","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-linear-regression-principle.png","link":"https://zl-wu.github.io/post/mla-linear-regression-principle/","stats":{"text":"9 min read","time":502000,"words":1341,"minutes":9},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-linear-regression-question\">1. Linear Regression Question</a></li>\n<li><a href=\"#2-linear-regression-model\">2. Linear Regression Model</a></li>\n<li><a href=\"#3-linear-regression-algorithm\">3. Linear Regression Algorithm</a></li>\n<li><a href=\"#41-generalization-of-linear-regression-polynomial-regression\">4.1. Generalization of linear regression: Polynomial Regression</a></li>\n<li><a href=\"#42-generalization-of-linear-regression-generalized-linear-regression\">4.2. Generalization of linear regression: Generalized Linear Regression</a></li>\n<li><a href=\"#5-regularization-of-linear-regression\">5. Regularization of linear regression</a>\n<ul>\n<li><a href=\"#51-l1-regularization\">5.1 L1 regularization</a></li>\n<li><a href=\"#52-l2-regularization\">5.2 L2 regularization</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","date":"2020-06-01 00:41:08","dateFormat":"2020-06-01"},{"fileName":"mla-least-square-method","abstract":"<p>&quot;A standard approach in regression analysis to approximate the solution of overdetermined systems.&quot;</p>\n","description":"&quot;A standard approach in regression analysis to approximate the solution of overdetermined systems.&quot; The metho...","title":"MLA -- Least Square method","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-least-square-method.jpg","link":"https://zl-wu.github.io/post/mla-least-square-method/","stats":{"text":"8 min read","time":472000,"words":1259,"minutes":8},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-least-square-principle\">1. Least Square principle</a></li>\n<li><a href=\"#2-least-squre-solution\">2. Least Squre solution</a><br>\n*\n<ul>\n<li><a href=\"#21-algebraic-way\">2.1 Algebraic way</a></li>\n<li><a href=\"#22-matrix-way\">2.2 Matrix way</a></li>\n</ul>\n</li>\n<li><a href=\"#3-limitations-and-applicable-scenarios-of-least-square-method\">3. Limitations and applicable scenarios of least square method</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-05-25 00:39:24","dateFormat":"2020-05-25"},{"fileName":"mla-gradient-descent-method","abstract":"<p>When improving the model parameters of machine learning algorithms, that is, unconstrained optimalization problem.<br>\n<strong>Gradient Descent method</strong> is one of most common used methods. The other one is the least square method. Here is a complete summary of the gradient descent method.</p>\n","description":"When improving the model parameters of machine learning algorithms, that is, unconstrained optimalization problem. Gradi...","title":"MLA -- Gradient Descent method","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-gradient-descent-method.png","link":"https://zl-wu.github.io/post/mla-gradient-descent-method/","stats":{"text":"24 min read","time":1402000,"words":3740,"minutes":24},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-what-is-gradient\">1. What is gradient?</a></li>\n<li><a href=\"#2-gradient-descent\">2. Gradient Descent</a></li>\n<li><a href=\"#3-concepts-of-gradient-descent\">3. Concepts of gradient descent</a></li>\n<li><a href=\"#4-gradient-descent-algorithm\">4. Gradient Descent Algorithm</a><br>\n*\n<ul>\n<li><a href=\"#41-algebraic-expression\">4.1 Algebraic Expression</a></li>\n<li><a href=\"#42-matrix-expression\">4.2 Matrix Expression</a></li>\n</ul>\n</li>\n<li><a href=\"#5-tuning-of-gradient-descent-algorithm\">5. Tuning of gradient descent algorithm</a></li>\n<li><a href=\"#6-gradient-descent-family-bgd-sgd-mbgd\">6. Gradient Descent Family (BGD, SGD, MBGD)</a><br>\n*\n<ul>\n<li><a href=\"#61-bgd-batch-gradient-descent\">6.1 BGD (Batch Gradient Descent)</a></li>\n<li><a href=\"#62-sgd-stochastic-gradient-descent\">6.2 SGD (Stochastic Gradient Descent)</a></li>\n<li><a href=\"#63-mbgd-mini-batch-gradient-descent\">6.3 MBGD (Mini-batch Gradient Descent)</a></li>\n</ul>\n</li>\n<li><a href=\"#7-comparison-of-gradient-descent-method-and-other-optimization-algorithms\">7. Comparison of gradient descent method and other optimization algorithms</a></li>\n<li><a href=\"#python-implementation-example\">Python implementation Example</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-05-16 00:37:42","dateFormat":"2020-05-16"},{"fileName":"mla-naive-bayes-theory","abstract":"<p>&quot;Priori Probability + data = Posterior probability&quot;</p>\n","description":"&quot;Priori Probability + data = Posterior probability&quot; Êù°‰ª∂Ê¶ÇÁéáÔºö P(X|Y) or P(Y|X) ËÅîÂêàÊ¶ÇÁéáÔºö P(X,Y), P(XY) or P(XnY) ËæπÁºòÊ¶ÇÁéá...","title":"MLA -- Naive Bayes theory","tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/"}],"feature":"https://zl-wu.github.io/post-images/mla-naive-bayes-theory.jpg","link":"https://zl-wu.github.io/post/mla-naive-bayes-theory/","stats":{"text":"14 min read","time":781000,"words":2106,"minutes":14},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#bayesian-interpretation\">Bayesian interpretation</a></li>\n<li><a href=\"#naive-bayes-parameter-estimation\">Naive Bayes parameter estimation</a></li>\n<li><a href=\"#naive-bayes-algorithm-process\">Naive Bayes algorithm process</a></li>\n<li><a href=\"#pros-and-cons-conclusion\">Pros and cons Conclusion</a></li>\n</ul>\n</li>\n</ul>\n","date":"2020-05-15 00:47:58","dateFormat":"2020-05-15"}],"tags":[{"name":"Machine Learning","slug":"NsWENf9vU","used":true,"link":"https://zl-wu.github.io/tag/NsWENf9vU/","count":9}],"menus":[{"link":"/","name":"È¶ñÈ°µ","openType":"Internal"},{"link":"/archives","name":"ÂΩíÊ°£","openType":"Internal"},{"link":"/tags","name":"Ê†áÁ≠æ","openType":"Internal"},{"link":"/post/about","name":"ÂÖ≥‰∫é","openType":"Internal"}],"themeConfig":{"themeName":"gridea-theme-pure","postPageSize":10,"archivesPageSize":50,"siteName":"ZL Wu's notebook","siteDescription":"‰∏çÂ∫üËØùÔºåÂ∞±ÊòØÂπ≤","footerInfo":"Powered by <a href=\"https://github.com/getgridea/gridea\" target=\"_blank\">Gridea</a>","showFeatureImage":true,"domain":"https://zl-wu.github.io","postUrlFormat":"SLUG","tagUrlFormat":"SHORT_ID","dateFormat":"YYYY-MM-DD","feedFullText":false,"feedCount":10,"archivesPath":"archives","postPath":"post","tagPath":"tag"},"customConfig":{"APP_ID":"","APP_KEY":"","about":"","avatar":"","caf":"#84fab0","ccf":"#5f6169","ccs":"#999fa7","ctf":"#ffffff","cts":"#dddddd","customCss":"","descfriend":"","dribbble":"","facebook":"","friends":[],"ga":"","github":"","isEnabledCustomColor":false,"pageSize":"5","placeholder":"Just Go Go","recordIp":false,"skin":"gray","twitter":"","vMaxWidth":"1000","vPadding":"2.5%","vPercentWidth":"100","valine":false,"visitor":false,"weibo":"","zhihu":""},"utils":{"now":1611336842849}}
