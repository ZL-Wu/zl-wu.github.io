{"posts":[{"title":"MLA -- collaborative filtering","content":"&quot;Collaborative filtering can be divided into users or items filtering. With its excellent speed and robustness, collaborative filtering is hot in the global Internet field. 😃😃&quot; [toc] In general speaking, Collaborative Filtering is to use some behavior information from a group of people who has similar interests and common experience to recommend new information that one user in the group may be interested in. Individuals give a response to the information (such as scoring) through the cooperative mechanism and record it to acheive the purpose of filtering and then help others to filter the information. Collaborative filtering can be divided into users or items filtering. With its excellent speed and robustness, collaborative filtering is hot in the global Internet field. 😃😃 I. User-Based CF The user-based collaborative filtering algorithm is to discover the user's preference of the product or content based on the user's historical behavior data (such as product purchase, collection, content review or sharing), and measure or score to quantize these preferences. According to the attitudes and preferences of different users towards the same product or content, the relationship between users can be calculated. Then we can recommend products or contents among users with similar preference. To be simiply, if user A and user B all purchased books &quot;x&quot;, &quot;y&quot; and &quot;z&quot;, and also all gave them 5 stars comments. Then user A and user B should belong to a same group. And the book &quot;w&quot; that user A read and like could be recommended to the user B. (This is the benifit of user-based CF.) 1. Find users with similar preferences Let's simulate a situation that 5 users score 2 products. The score here may indicate a real purchase, or it may be a quantitative indicator of the user's different behaviors of the product. For example, the number of times you browse products, recommend products to friends, bookmark, share, or comment. These behaviors can indicate the user's attitude and preference to the product. Product 1 Product 2 User 1 3.3 6.5 User 2 5.8 2.6 User 3 3.6 6.3 User 4 3.4 5.8 User 5 5.2 3.1 From the data in the table above, it is a bit hard to figure out the relationship among these five users. However, if we plot all these five points in a scatter chart, the conclusion is obvious. User 1, 3 and 4 is one group, and User 2 and 5 is another group. # Python code for drawing the scatter plot above import matplotlib.pyplot as plt import numpy as np x = [3.3, 5.8, 3.6, 3.4, 5.2] y = [6.5, 2.6, 6.3, 5.8, 3.1] colors = np.random.rand(5) plt.scatter(x,y,c=colors,alpha=0.8) plt.xlim(0,7); plt.xlabel(&quot;Product 1&quot;) plt.ylim(0,7); plt.ylabel(&quot;Product 2&quot;) for i in range(5): plt.annotate(&quot;User &quot;+str(i+1), xy=(x[i],y[i]), xytext = (x[i]+0.1, y[i]+0.0)) # 这里xy是需要标记的坐标，xytext是对应的标签坐标 plt.show() 2. Similarity Calculation Although the scatter plot is intuitive, it cannot be put into production. Therefore, we need to accurately measure the relationship of users through real numbers, and complete the recommendation of products based on these relationship conclusions. 2.1. Euclidean distance evaluation Euclidean distance is a relatively simple method to evaluate the relationship among users. The principle is to calculate the distance of each two points in the scatter chart. Distance Reciprocal User 1 &amp; 2 4.63 0.22 User 1 &amp; 3 0.36 2.77 User 1 &amp; 4 0.71 1.41 User 1 &amp; 5 3.89 0.26 User 2 &amp; 3 4.30 0.23 User 2 &amp; 4 4.00 0.25 User 2 &amp; 5 0.78 1.28 User 3 &amp; 4 0.54 1.86 User 3 &amp; 5 3.58 0.28 User 4 &amp; 5 3.24 0.31 From the calculation above, if we defined a threshold, we can easily cluster these users. 2.2. Pearson correlation evaluation Pearson correlation evaluation is another method to calculate the relationship between users. It is a bit more complicated than the calculation of Euclidean distance, but Pearson correlation evaluation can give a better result when the score data is not standardized. The formula of Pearson correlation coefficient between variable x and y is: ρxy=cov(x,y)σxσy=E[(x−μx)(y−μy)]σxσy\\begin{aligned} \\rho_{xy} &amp;= \\frac{cov(x, y)}{\\sigma_x \\sigma_y} \\\\ &amp;= \\frac{E[(x-\\mu_x)(y-\\mu_y)]}{\\sigma_x \\sigma_y} \\end{aligned}ρxy​​=σx​σy​cov(x,y)​=σx​σy​E[(x−μx​)(y−μy​)]​​ μx\\mu_xμx​ and μy\\mu_yμy​ are the mean value of variable x and variable y. ρxy\\rho_{xy}ρxy​ is tne overall correlation coefficient. If we estimate the covariance and standard deviation of the sample, we can also get the sample's Pearson correlation coefficient &quot;r&quot;. r=∑i=1n(Xi−X‾)(Yi−Y‾)∑i=1n(Xi−X‾)2∑i=1n(Yi−Y‾)2=1n−1∗∑i=1n(Xi−X‾)(Yi−Y‾)σXσY=1n−1∗∑i=1n[(Xi−X‾σX)(Yi−Y‾σY)]\\begin{aligned} r &amp;= \\frac{\\sum_{i=1}^n(X_i - \\overline{X})(Y_i - \\overline{Y})} {\\sqrt{\\sum_{i=1}^n(X_i-\\overline{X})^2}\\sqrt{\\sum_{i=1}^n(Y_i-\\overline{Y})^2}} \\\\ &amp;= \\frac{1}{n-1} * \\frac{\\sum_{i=1}^n(X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sigma_X \\sigma_Y} \\\\ &amp;= \\frac{1}{n-1} * \\sum_{i=1}^n \\left[ \\left(\\frac{X_i-\\overline{X}}{\\sigma_X}\\right) \\left(\\frac{Y_i-\\overline{Y}}{\\sigma_Y}\\right) \\right] \\end{aligned}r​=∑i=1n​(Xi​−X)2​∑i=1n​(Yi​−Y)2​∑i=1n​(Xi​−X)(Yi​−Y)​=n−11​∗σX​σY​∑i=1n​(Xi​−X)(Yi​−Y)​=n−11​∗i=1∑n​[(σX​Xi​−X​)(σY​Yi​−Y​)]​ (Xi−X‾σX)\\left(\\frac{X_i-\\overline{X}}{\\sigma_X}\\right)(σX​Xi​−X​) is the standard score (Z) of sample variable X. The Pearson correlation coefficient always falls in -1 and 1. The Pearson coefficient is symmetrical: corr(X,Y) = corr(Y,X). An important mathematical characteristic of the Pearson correlation coefficient is that any change of the position and scale of the two variables will not cause the change of the coefficient. That is, if we move X to a+bX, and move Y to c+dY, (a,b,c,d are all constants), the correlation coefficient of X and Y doesn't change. μx=E(X)\\mu_x = E(X) μx​=E(X) σx2=E[(X−E(X))2]=E[(X2−2E(X)X+E2(X))]=E(X2)−2E2(X)+E2(X)=E(X2)−E2(X)=E(X2)−μX2\\begin{aligned} \\sigma_x^2 &amp;= E[(X-E(X))^2] \\\\ &amp;= E[(X^2-2E(X)X+E^2(X))] \\\\ &amp;= E(X^2)-2E^2(X)+E^2(X) \\\\ &amp;= E(X^2)-E^2(X) \\\\ &amp;= E(X^2)-\\mu_X^2 \\end{aligned}σx2​​=E[(X−E(X))2]=E[(X2−2E(X)X+E2(X))]=E(X2)−2E2(X)+E2(X)=E(X2)−E2(X)=E(X2)−μX2​​ E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y)−E(X)E(Y)+E(X)E(Y)=E(XY)−E(X)E(Y)\\begin{aligned} &amp;\\quad\\ E[(X-E(X))(Y-E(Y))] \\\\ &amp;= E(XY)-E(X)E(Y)-E(X)E(Y)+E(X)E(Y) \\\\ &amp;= E(XY)-E(X)E(Y) \\end{aligned}​ E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y)−E(X)E(Y)+E(X)E(Y)=E(XY)−E(X)E(Y)​ Therefore, the Pearson correlation coefficient can be also writen as: ρxy=E(XY)−E(X)E(Y)E(X2)−E2(X)E(Y2)−E2(Y)\\rho_{xy} = \\frac{E(XY)-E(X)E(Y)}{\\sqrt{E(X^2)-E^2(X)}\\sqrt{E(Y^2)-E^2(Y)}} ρxy​=E(X2)−E2(X)​E(Y2)−E2(Y)​E(XY)−E(X)E(Y)​ And the sample's correlation coefficient is: r=∑i=1nxiyi−nxˉyˉ(n−1)sxsy=n2[∑xiyin−∑xi∑yin2]n2∑xi2n−(∑xin)2∑yi2n−(∑yin)2=n∑xiyi−∑xi∑yin∑xi2−(∑xi)2n∑yi2−(∑yi)2\\begin{aligned} r &amp;= \\frac{\\sum_{i=1}^nx_iy_i - n\\bar{x}\\bar{y}}{(n-1)s_xs_y} \\\\ &amp;= \\frac{n^2 \\left[\\frac{\\sum x_iy_i}{n} - \\frac{\\sum x_i\\sum y_i}{n^2} \\right]} {n^2 \\sqrt{\\frac{\\sum x_i^2}{n} - \\left(\\frac{\\sum x_i}{n}\\right)^2} \\sqrt{\\frac{\\sum y_i^2}{n} - \\left(\\frac{\\sum y_i}{n}\\right)^2}} \\\\ &amp;= \\frac{n \\sum x_iy_i - \\sum x_i \\sum y_i} {\\sqrt{n \\sum x_i^2 - (\\sum x_i)^2} \\sqrt{n \\sum y_i^2 - (\\sum y_i)^2}} \\end{aligned}r​=(n−1)sx​sy​∑i=1n​xi​yi​−nxˉyˉ​​=n2n∑xi2​​−(n∑xi​​)2​n∑yi2​​−(n∑yi​​)2​n2[n∑xi​yi​​−n2∑xi​∑yi​​]​=n∑xi2​−(∑xi​)2​n∑yi2​−(∑yi​)2​n∑xi​yi​−∑xi​∑yi​​​ Suppose X and Y are the variables that represents various rating data to a same set of products from User X and User Y. As we all know, different users has different attitudes, someone is conservative and someone is aggresive to their rating. A good thing of Pearson correlation coefficient is that it could remove these difference and standardize variable X and Y. And the coefficient measures how does Y change when X increase? increase or decrease? Exactly the same change means 1, and completely opposite change means -1. Give a example, which is closer to the real case. Product 1 Product 2 Product 3 Product 4 Product 5 User A 3.3 6.5 2.8 3.4 5.5 User B 3.5 5.8 3.1 3.6 5.1 User C 5.6 3.3 4.5 5.2 3.2 User D 5.4 2.8 4.1 4.9 2.8 User E 5.2 3.1 4.7 5.3 3.1 We calculated the similarity data between users by calculating the ratings of 5 products by 5 users above. Here you can see that users A&amp;B, C&amp;D, C&amp;E and D&amp;E have a high similarity. Next, we can recommend products to users based on similarity. Similarity User A&amp;B 0.9998 User A&amp;C -0.8478 User A&amp;D -0.8418 User A&amp;E -0.9152 User B&amp;C -0.8417 User B&amp;D -0.8353 User B&amp;E -0.9100 User C&amp;D 0.9990 User C&amp;E 0.9763 User D&amp;E 0.9698 3. Recommend products for users Continue the above example, if we want to recommend some new products to User C. We can find out the most similar user of him or her at first, that is, user D at above table. Then we can browse and screen out those products that User D likes but User C doesn't try before, and then recomemnd to User C. This is a very intuitive way, however, in real case, there might be some more complicated calculation to weight and sort these new products, then choose the best one product to recommend. For example, find out 2 or 3 most similar users and combine their rating records to weight new products to recommend. 4. Pros and cons of the Algorithm Data sparsity. A large-scale e-commerce recommendation system usually has a lot of items, and users may buy less than 1% of the items. The overlap of items bought by different users is low, resulting in the algorithm unable to find a user's neighbors, that is, similar preferences Users. Algorithm scalability. The calculation amount of the nearest neighbor algorithm increases with the increase of the number of users and items, and is not suitable for use in a large amount of data. 5. Code Example (Python) Dataset Example: There are 138493 users' records of rating to the different movies they have watched. Try to use User-Based CF Algorithm to recommend movies to any user. userId movieId rating 1 2 3.5 1 29 3.5 1 32 3.5 1 47 3.5 1 50 3.5 ... ... ... 138493 68319 4.5 138493 68954 4.5 138493 69526 4.5 138493 69644 3.0 138493 70286 5.0 138493 71619 2.5 Solution: https://github.com/ZL-Wu/python-code-pool/blob/master/collaborative_filtering.ipynb II. Item-Based CF Item-Based CF's principle is basically the same as that of User-Based CF. In User-Based CF, consider the User as Item, and Item as User. According to the various rating data from various users to several product, we can calculate the similarity between each two products. Then for each user, this algorithm can recommend new and most similar product based on those products he or she used before. 1. Advantages Can filter information that the machine is difficult to analyze automatically, such as artwork, music, etc. Share the experience of others, avoid incomplete or inaccurate content analysis, and can filter based on some complex and difficult to express concepts (such as information quality, personal taste). The ability to recommend new information. It can be found that the content is completely dissimilar, and the content of the recommended information is not expected by the user in advance. You can discover the user's potential interest preferences that you have not yet discovered. Recommend personalization and high degree of automation. Can effectively use the feedback information of other similar users. Accelerate the speed of personalized learning. 2. Disadvantages New User Problem (New User Problem) The quality of the recommendation at the beginning of the system is poor. New Item Problem The quality depends on the historical data set. Sparsity System scalability (Scalability). III. Conclusion According to the different data source, Recommendation Engines can be can be divided into three categories: Demographic-based Recommendation Content-based Recommendation Collaborative Filtering-based Recommendation (IBM's introduction of recommendation engines) Content-based Recommendation is based on the metadata of item or content to find the relation among them. For example, we define various feature of movies, such as category, length and so on, then build a model to group similar movies. Finally, recommend users new movies by looking for similar movies of the user's favorite movies. Collaborative Filtering-based Recommendation can also be divided into three categories: User-based CF: User A likes Items 1,2,3. User C likes Items 1,2. And User A and User C are similar. Therefore, recommend Item 3 to User C. Item-based CF: User A likes Items 1,2,3. User C likes Item 1,2. And Item 1 and Item 3 are similar. Therefore, recommend Item3 to User C. Model-based CF: Based on the sample user preference information, train a recommendation model, and then make prediction recommendations based on real-time user preference information. Content-based Recommendation needs to know the contents and details of items or products. However, CF Recommendation is totally a statistical model. We don't need to know the details about the product and the content. What we did is only research user's behavior. IV. Reference https://www.zhihu.com/question/19971859 https://www.jianshu.com/p/d15ba37755d1 https://baike.baidu.com/item/协同过滤/4732213?fr=aladdin https://wiki.mbalib.com/wiki/长尾理论 https://baike.baidu.com/item/皮尔逊相关系数/12712835?fr=aladdin https://baike.baidu.com/item/协方差 ","link":"https://zl-wu.github.io/post/mla-collaborative-filtering/"},{"title":"MLA -- Decision Tree","content":"Decision Tree is one of classical algorithms in machine learning. It can be used as both a classification model and a regression model. It is also suitable as a ensemble model, such as Random Forest. The history of Desicion Tree has three stages: ID3, C4.5 and CART [toc] 1. The Information Theory Foundation of Decision Tree ID3 It all originated from a simple &quot;if...else...&quot; statment that every programmer almost uses every day. Data researchers want to use a certain condition (if...else...) to split dataset into two distinct subsets. Then there are two questions we need to consider: A dataset usually has lots of features. So how to choose the feature that we need to use first in &quot;if...else...&quot;? And second, third features? How to quantitatively evaluate the quality of a certain binary division on the feature? In 1970s, a genius Ross Quinlan invented a method to guide and evaluate the process of decision tree by using entropy in information theory. As soon as the methods came out, its simplicity and efficiency cause a sensation. Quinlan called it ID3 algorithm. Entropy measures the uncertainty of things, the more uncertain things, the greater the entropy of it. Specifically, the expression of the entropy of a random discrete variable X is as follows: H(X)=−∑i=1npilog(pi)H(X) = - \\sum_{i=1}^np_ilog(p_i) H(X)=−i=1∑n​pi​log(pi​) n represents the &quot;n&quot; kinds of discrete values. (counts of unique value of X) pip_ipi​ is the probability that X takes the value &quot;i&quot;. log usually is the logarithm based on 2 or e. For example, if X only has two kind of values, the entropy is the largest when the probabilities of this two values are 1/2, which means X has the largest uncertainty. H(X) = -(1/2*log(1/2) + 1/2*log(1/2)) = log(2) if the probabilities of this two values are 1/3 and 2/3, H(X) = -(1/3*log(1/3) + 2/3*log(2/3)) = log(3) - 2/3*log(2) &lt; log(2) Then multivariable entropy: H(X,Y)=−∑i=1np(xi,yi)log(p(xi,yi))H(X,Y) = - \\sum_{i=1}^n p(x_i,y_i)log(p(x_i,y_i)) H(X,Y)=−i=1∑n​p(xi​,yi​)log(p(xi​,yi​)) And conditional entropy H(X|Y), which is the uncertainty of X after knowing the uncertainty of Y: H(X∣Y)=−∑i=1np(xi,yi)log(p(xi∣yi))=∑j=1np(yj)H(X∣yj)\\begin{aligned} H(X|Y) &amp;= - \\sum_{i=1}^n p(x_i,y_i)log(p(x_i|y_i)) \\\\ &amp;= \\sum_{j=1}^n p(y_j)H(X|y_j) \\end{aligned} H(X∣Y)​=−i=1∑n​p(xi​,yi​)log(p(xi​∣yi​))=j=1∑n​p(yj​)H(X∣yj​)​ H(X) represents the uncertainty of X. H(X|Y) represents the left uncertainty of X after knowing the Y. Therfore, [H(X) - H(X|Y)] represents the degree of reduction in uncertainty of X after knowing the Y. It is also called mutual information in information theory, which is wrote as I(X,Y). (互信息) In decision tree ID3 algorithm, mutual info is called information gain (信息增益). ID3 algorithm uses information gain to determin what feature should be used in current node to build the decidion tree. The larger the information gain, the better it is for classification in current node. From the graph above, we can easily figure out the relationship among them. The ellipse on the left represents H(X) [uncertainty of X]. And the ellipse on the left represents H(Y). The overlapping section in the middle is the mutual information (or information gain) I(x,y). The left section of left ellipse H(X|Y) The union of two ellipse is H(X,Y) If Y is &quot;a&quot;, almost all X is &quot;b&quot;. This means when Y is &quot;a&quot;, the uncertainty of X is very low (H(X|Y) is very small, I(X,Y) is large). Because the randomness of the X value is very small when Y is &quot;a&quot;. Then X is suitabale as a feature of classification of Y. 2. Decision Tree ID3 Algorithm Principle Take an example for ID3 algorithm to see how it uses information gain to build a decision tree model. Suppose there are 15 samples data D, the output is 0 or 1, and 9 of them are 1, 6 of them are 0. There is a feature A in the sample, the values are A1, A2 or A3. When A is A1, the output has 3 &quot;1&quot; and 2 &quot;0&quot; When A is A2, the output has 2 &quot;1&quot; and 3 &quot;0&quot; When A is A3, the output has 4 &quot;1&quot; and 1 &quot;0&quot; The entropy of sample D is: H(D)=−(915log2915+615log2615)=0.971H(D) = - (\\frac{9}{15}log_2\\frac{9}{15} + \\frac{6}{15}log_2\\frac{6}{15}) = 0.971 H(D)=−(159​log2​159​+156​log2​156​)=0.971 The conditional entropy of sample D in the feature A is: H(D∣A)=515H(DA1)+515H(DA2)+515H(DA3)=−515(35log235+25log225)−515(25log225+35log235)−515(45log245+15log215)=0.888\\begin{aligned} H(D|A) &amp;= \\frac{5}{15}H(D_{A1}) + \\frac{5}{15}H(D_{A2}) + \\frac{5}{15}H(D_{A3}) \\\\ &amp;= -\\frac{5}{15}(\\frac{3}{5}log_2\\frac{3}{5}+\\frac{2}{5}log_2\\frac{2}{5}) -\\frac{5}{15}(\\frac{2}{5}log_2\\frac{2}{5}+\\frac{3}{5}log_2\\frac{3}{5}) -\\frac{5}{15}(\\frac{4}{5}log_2\\frac{4}{5}+\\frac{1}{5}log_2\\frac{1}{5}) \\\\ &amp;= 0.888 \\end{aligned} H(D∣A)​=155​H(DA1​)+155​H(DA2​)+155​H(DA3​)=−155​(53​log2​53​+52​log2​52​)−155​(52​log2​52​+53​log2​53​)−155​(54​log2​54​+51​log2​51​)=0.888​ Then the information gain H(D) - H(D|A) = 0.971 - 0.888 = 0.083 The specific algorithm process looks like: Input is &quot;m” samples, the sample output set is &quot;D&quot;. Each sample has &quot;n&quot; discrete features, the feature set is &quot;A&quot;. The final output of ID3 algorithm is a decision tree &quot;T&quot; The process is: Initialize the threshold of information gain ϵ\\epsilonϵ Read the output set D. If all output value are same &quot;Di&quot;, return a single node tree T, marked as category &quot;Di&quot;. Read the feature set A. If A is null, return a single node tree T, marked as the category that has the largest counts number in D. [argmax(counts(Di))] Traverse and Calculate the information gain of each feature in A (n features). Select the feature &quot;AgA_gAg​&quot; which has the largest information gain. If the information gain of &quot;AgA_gAg​&quot; is less than the threshold ϵ\\epsilonϵ, return a single node tree T, marked as the category that has the largest counts number in D. [argmax(counts(Di))] If not (else), according to the different values in AgA_gAg​, split the total sample into several different categories subset DjD_jDj​. Each category is a child node. And return the tree T with multiple nodes. For each child nodes, let D=Dj,A=A−{Ag}D=D_j, A=A-\\{A_g\\}D=Dj​,A=A−{Ag​}, then recursively call the step 2-6 to get and return the subtree TiT_iTi​. 3. Deficiency of decision tree ID3 algorithm Althogh ID3 algorithm proposed new ideas, there are still many areas worthy of improvement. (1) ID3 doesn't consider continuous features, such as length, density or other continuous features. This greatly limits the use of ID3. (2) ID3 uses the features of large information gain to preferentially establish the nodes of the decision tree. However, under the same conditions, the feature with more kinds of values has larger information gain. For example, if the feature A has 2 kinds of values, each of them is 1/2, H(A)=log(2); if the feature A has 3 kinds of values, each of them is 1/3, H(A)=log(3). How to correct this problem? (3) ID3 doesn't consider the condition of missing values. (4) ID3 doesn't consider the overfitting problem. Ross Quinlan has improved the ID3 algorithm based on the above deficiencies. This is C4.5 algorithm. The reason that why the new algorithm wasn't named ID4 or IDn: Decision tree was too popular when it came out, then lots of people started the second innovation and occupied ID4 and ID5 soon. So Quinlan took a new path and named it the C4.0 algorithm. And later, the advanced version was C4.5 algorithm. 4. Improvement of the Decision Tree C4.5 algorithm According to the 4 deficiencies of ID3 above, C4.5 improved them through the following: (1) Cannot handle continuous features: The idea of C4.5 is to discretize continuous features. For example, the feature A of m samples has m values, sort from small to large as a1,a2,...,ama_1, a_2, ..., a_ma1​,a2​,...,am​. C4.5 take the average value between two neighbor values as a dividing point, then there are (m-1) dividing points. The i-th dividing point TiT_iTi​ is Ti=ai+ai+12T_i=\\frac{a_i+a_{i+1}}{2}Ti​=2ai​+ai+1​​. For these (m-1) dividing points, C4.5 calculates the information gain using each dividding point as the binary classification point. Finally, select the point with the largest information gain as the binary discrete classification point for the continuous feature. Pay attention: unlike the discrete attribute, if the current node is a continuous attribute, then this attribute can also participate in the generation and selection process of child nodes. (2) Bias of features with more kinds of values: C4.5 inroduces a new variable of information gain ratio IR(X,Y)I_R(X,Y)IR​(X,Y). It is the ratio of information gain and feature entropy. IR(D,A)=I(A,D)HA(D)I_R(D,A) = \\frac{I(A,D)}{H_A(D)} IR​(D,A)=HA​(D)I(A,D)​ D is the output set of samples, A is the feature set. And the feature entropy is: HA(D)=−∑i=1n∣Di∣∣D∣log2∣Di∣∣D∣H_A(D) = -\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|} HA​(D)=−i=1∑n​∣D∣∣Di​∣​log2​∣D∣∣Di​∣​ n is the number of categories of feature A. DiD_iDi​ is the number of samples corresponding to the i-th value of feature A. D is the total number of all samples. The feature with more kinds of values has a larger feature entropy. It serves as a denominator to correct the problem that information gain tends to be biased toward features with more values. (3) Problem of missing values: There are two sub problems needed to be solved: a. How to choose the feature to build chind node when some features of the sample is missing? b. If the feature is selected, how to deal with the samples with missing values on this feature? a. Feature Choosing with missing value For a featur A with missing values, C4.5 set a weight for each sample (including those with missing value in A), the initial weight could be 1. Then C4.5 splits the data into 2 subsets, one is the data D1 without missing value, the other one is the data D2 with missing value. For no missing value data D1, calculate the information gain ratio with weight, and then multiply by a coefficient, which is the ratio of weighted samples with no feature A missing and weighted total samples. b. Data Spliting with missing value in A C4.5 can divide the samples with missing value A into all child nodes at the same time. But the weight of this sample is reassigned according to the proportion of the number of samples of each child node. For example, suppose there is a sample &quot;ms&quot; with missing value in feature A. The feature A has 3 kinds of value: A1, A2 and A3, and the number of samples are respectively 2,3,4, that is, the proportion of A1, A2 and A3 are 2/9, 3/9 and 4/9. Then the corresponding weights of missing value sample are adjusted to 2/9, 3/9 and 4/9. (4) Overfitting problem: C4.5 introduced regularization coefficients for preliminary pruning, which will be discussed in detail later. 5. Deficiency of decision tree C4.5 algorithm Althogh C4.5 algorithm improves ID3 a lot, there are still some areas worthy of improvement. (1) ID3 and C4.5 algorithms generate a multi-fork tree. Discrete feature with more than 2 kinds of value leads multiple child nodes. In many cases, the binary tree model in the computer will be more efficient than the multi-tree operation. Binary tree can improve efficiency. (2) C4.5 can only be used for classification. If the decision tree can be also used for regression, its scope of use can be expanded. (3) C4.5 uses the entropy model, there are lots of time-consuming logarithmic operations. If it is continuous feature, there is also a time-consuming sorting operation. It would be better if the model simplification can reduce the computational intensity without sacrificing too much accuracy. (4) Decision tree algorithm is very easy to overfit, the generated decision tree must be pruned. C4.5's pruning method still has room for optimization. There are two main ideas for pruning: pre-pruning, which is to decide whether to pruning when a decision tree is generated. post-pruning, that is, the decision tree is generated first, and then pruned tree through cross-validation. In the next section when we talk about the CART tree, we will specifically introduce the idea of reducing the branch of the decision tree, which mainly uses post-pruning plus cross-validation to select the most suitable decision tree. These 4 problems has been improved in CART algorithm. So if not consider integrated learning at present, in the ordinary decision tree algorithm, the CART algorithm is considered to be the better algorithm. 6. CART Classification Tree Algorithm -- Gini Coefficient Let's review at first, as we all know now: ID3 uses information gain to select features. I(D,A)=H(D)−H(D∣A)I(D,A) = H(D)-H(D|A)I(D,A)=H(D)−H(D∣A) C4.5 uses information gain ratio to select features. IR(D,A)=I(A.D)HA(D)I_R(D,A)=\\frac{I(A.D)}{H_A(D)}IR​(D,A)=HA​(D)I(A.D)​ Both of them are entropy models based on the information theory, H(X)=−∑i=1npi∗log(pi)H(X) = -\\sum_{i=1}^n p_i*log(p_i)H(X)=−∑i=1n​pi​∗log(pi​), which has lots of logarithmic computation and decrease the efficiency a lot. Is there any method we can simplify the model to increase the efficiency but without sacrificing too much accuracy? Yes, It is Gini Coefficient of CART. The Gini Coefficient represents the impureness of the node. The smaller the Gini, the lower the unpurification, the better classfication. This is opposite to Information Gain (Ratio) Specifically, in a classification problem, supposed there are K categories, and the probability of i-th category is pip_ipi​. Then the Gini coefficient is: Gini(p)=∑i=1Kpi(1−pi)=1−∑i=1Kpi2Gini(p) = \\sum_{i=1}^K p_i(1-p_i) = 1-\\sum_{i=1}^Kp_i^2 Gini(p)=i=1∑K​pi​(1−pi​)=1−i=1∑K​pi2​ If there is a binary classification, and the probability of first category is p, the Gini Coefficient is super easy: Gini(p)=2p(1−p)Gini(p) = 2p(1-p)Gini(p)=2p(1−p) For a sample D, if it has K categories, and the counts of i-th category is ∣Ci∣|C_i|∣Ci​∣, and the total number of D is |D|, then the Gini Coefficient of sample D is: Gini(D)=1−∑i=1K(∣Ci∣∣D∣)2Gini(D) = 1 - \\sum_{i=1}^K \\left(\\frac{|C_i|}{|D|}\\right)^2 Gini(D)=1−i=1∑K​(∣D∣∣Ci​∣​)2 And if the feature A (binary discrete or continuous) split the data into two subsets D1 and D2 by a certain value a of A. Then in the condition of feature A, the Gini Coefficient of D is: Gini(D,A)=∣D1∣∣D∣Gini(D1)+∣D2∣∣D∣Gini(D2)Gini(D,A) = \\frac{|D_1|}{|D|}Gini(D_1) + \\frac{|D_2|}{|D|}Gini(D_2) Gini(D,A)=∣D∣∣D1​∣​Gini(D1​)+∣D∣∣D2​∣​Gini(D2​) Comparing with entropy model formulas, the Quadratic computation of Gini Coefficient is easier a lot than logrithmic computation, especially in the binary classification. And so, Compared with the entropy model, how does the Gini coefficient perform? Take a look at the plot below. As we can see from the plot, the curves of Gini and Entropy (scaled) are very close, and the error is only slightly larger near the angle of 45 degrees. Therefore, the Gini can be used as an approximate substitute for the entropy model. In fact, the CART use Gini to select features of decision tree. And for further simplification, CART only divides each node to binary child nodes but not multi-nodes, so that the CART classification tree algorithm builds a binary tree instead of a multi-tree. In this way, the calculation of the Gini coefficient can be further simplified, and a more elegant binary tree model can be established. 7. CART Algorithm on continuous features and discrete features continuous features The idea of CART on continuous features is same with C4.5. Suppose the sample m has m values in continous feature A, CART sorts them from small to large, a1,a2,...,ama_1,a_2,...,a_ma1​,a2​,...,am​ and finds out (m-1) dividing points, which is the average value of each pair of neighbor points. For example, the i-th dividing point TiT_iTi​ is ai+ai+12\\frac{a_i+a_{i+1}}{2}2ai​+ai+1​​. CART calculates the Gini Coefficient value of each one of the dividing points, and selects the point with the lowest Gini Coefficient value. Pay attention: Unlike ID3, if the current node is the continuous feature, it will be still used in the following steps to create other child nodes. discrete features Unlike ID3 and C4.5, CART uses a non-stop binary classification, no matter how many kinds of values the feature has. For example, if the feature A has A1, A2, A3 three kinds of values, ID3 or C4.5 will build three child nodes {A1},{A2} and {A3}. But CART will consider 3 binary child nodes conditions: {A1} and {A2,A3}, {A2} and {A1,A3}, {A3} and {A1,A2}, then select the condition with lowest Gini Coefficient. And if CART selects {A2} and {A1,A3}, due to the value of feature A is not completely separated this time, CART will have the opportunity to continue to select feature A to divide A1 and A3 in the child node. Pay attention: In ID3 or C4.5, discrete feature will only participate in the establishment of a node once, because they builds multi-division tree. Unlike them, CART will reuse each discrete feature. Regardless of continuous features or discrete features, after the current node uses this feature to build a subtree, CART will still use this feature in the future subtree establishment. 8. CART classification tree establishment algorithm specific process The input of CART algorithm are training set D, threshold of Gini Coefficient, threshold of number of samples The outpuf of CART is the decision tree T CART algorithm starts from the root node, and uses the training set to recursively build the CART tree: For the dataset D of current node, if the number of records is smaller than the threshold number of samples, or there are no features, then return the decision subtree T, and current node stops recursive. Calculate the Gini Coefficient of dataset D of current node, Gini(D)Gini(D)Gini(D). If the Gini Coefficient is smaller than the threshold, then return the decision subtree T, and current node stops recursive. Calculate the Gini Coefficient of each feature value of the current node on the data set D, Gini(D,A)Gini(D,A)Gini(D,A). The processing of missing values is the same as that described in the C4.5 algorithm. Among all calculated Gini Coefficients of the pair of each feature and each feature's possible dividing point, choose the feature with the appropriate dividing point value that has smallest Gini Coefficient. According to this optimal feature and optimal feature value (dividing point), the dataset of current node is divided into two nodes D1 and D2, and the left and right nodes of the current node are established at the same time, the dataset D of the left node is D1, and the dataset D of the right node is D2. Steps 1-4 are recursively called on each of the left and right child nodes to generate a decision tree. When doing the prediction on the generated decision tree, if the sample &quot;P&quot; in prediction set falls into a certain leaf node, and there are multiple training samples in each leaf node, then the category prediction of sample &quot;P&quot; is the category with the highest probability in this leaf node. 9. CART regression tree building algorithm One advantage of CART is that it can be used not only as a classification model but also as a regression model. If the output of sample is the discrete value, we should train a classification tree. However, if the output of sample is the continuous value, we should train a regression tree. The algorithms for building CART regression trees and CART classification trees are mostly similar, so here we only discuss the differences between the algorithms for building CART regression trees and CART classification trees. The main difference between the establishment and prediction of CART regression tree and CART classification tree: Different methods for processing features. Different ways of making predictions after the decision tree is established. In feature selection and division, CART classification tree uses the Gini Coefficient to measure the pros and cons of each division node, which is suitable for classification problem. However, for the regression problem, we use variance measurement methods. The goal of the CART regression tree is that: For the data sets D1 and D2 divided on both sides of the arbitrary division feature &quot;A&quot; and the division point &quot;s&quot; Find the feature and feature division point corresponding to the minimum mean square deviation of each set of D1 and D2, and the minimum sum of mean square errors of D1 and D2. min⁡A,s[min⁡c1∑xi∈D1(A,s)(yi−c1)2+min⁡c2∑xi∈D2(A,s)(yi−c2)2]\\min_{A,s}\\left[\\min_{c_1} \\sum_{x_i \\in D_1(A,s)}(y_i-c_1)^2 + \\min_{c_2} \\sum_{x_i \\in D_2(A,s)}(y_i-c_2)^2 \\right] A,smin​⎣⎡​c1​min​xi​∈D1​(A,s)∑​(yi​−c1​)2+c2​min​xi​∈D2​(A,s)∑​(yi​−c2​)2⎦⎤​ (A,s) is the optimal feature and optimal division point of optimal feature. This is what we need in building decision tree. c1c_1c1​ and c2c_2c2​ are the average output value of dataset D1 and D2. (A,s) should minimize the above formula (Loss Function) In the prediction after the decision tree is established, the output of the regression tree is not a category. It uses the mean or median of the output value in final leaves to predict the output. Except for the above, there is no difference between CART regression tree and CART classification tree building algorithm and prediction. 10. Pruning of CART tree algorithm Since the decision tree algo is easy to overfitting after training, resulting in poor generalization ability. In order to preventing the overfitting problem, we need to prune the CART tree, that is, similar to regularization of the linear regression to increase the generalization ability. The pruning algorithm of the CART tree can be summarized in two steps: Generate various pruned decision tree from the original decision tree. Use cross-validation to test the prediction ability for all pruned tree. And the tree with the best generalization prediction ability is selected as the final CART tree. a. loss function of decision tree The pruning strategies of the CART regression tree and the CART classification tree are that one uses the mean square error and one uses the Gini coefficient when measuring the loss. There is a Classification tree T, |T| is the number of leaf nodes in the tree. &quot;t&quot; is one of leaf node of tree T: NtN_tNt​ is the number of samples in leaf node &quot;t&quot;, Ht(T)H_t(T)Ht​(T) is the entropy of leaf node &quot;t&quot;. In leaf node &quot;t&quot;, the number of k-th category samples is NtkN_{tk}Ntk​, k=1,2,...,K. (There are K categories in the nod) α≥0\\alpha \\ge 0α≥0 is the regularization coefficient Then the loss function of this classification decision tree Cα(T)C_{\\alpha}(T)Cα​(T) is: Cα(T)=∑t=1∣T∣NtHt(T)+α∣T∣C_{\\alpha}(T) = \\sum_{t=1}^{|T|}N_tH_t(T) + \\alpha|T| Cα​(T)=t=1∑∣T∣​Nt​Ht​(T)+α∣T∣ The entropy of node &quot;t&quot;, Ht(T)H_t(T)Ht​(T) is: Ht(T)=−∑k=1KNtkNtlog(NtkNt)H_t(T) = - \\sum_{k=1}^K \\frac{N_{tk}}{N_t} log(\\frac{N_{tk}}{N_t}) Ht​(T)=−k=1∑K​Nt​Ntk​​log(Nt​Ntk​​) We defined C(T) as the prediction error of the model on the training data, which can also represents the degree of fitting between the model and the training data. C(T)=∑t=1∣T∣NtHt(T)=−∑t=1∣T∣∑k=1KNtk∗log(NtkNt)\\begin{aligned} C(T) &amp;= \\sum_{t=1}^{|T|}N_tH_t(T) \\\\ &amp;= - \\sum_{t=1}^{|T|} \\sum_{k=1}^K N_{tk}*log(\\frac{N_{tk}}{N_t}) \\end{aligned}C(T)​=t=1∑∣T∣​Nt​Ht​(T)=−t=1∑∣T∣​k=1∑K​Ntk​∗log(Nt​Ntk​​)​ Then the loss function Cα(T)C_{\\alpha}(T)Cα​(T) can be written as: Cα(T)=C(T)+α∣T∣C_{\\alpha}(T) = C(T) + \\alpha|T| Cα​(T)=C(T)+α∣T∣ Whether it is Classification Tree (CT) or Regression Tree (RT), their loss function formula is same: $ C_{\\alpha}(T) = C(T) + \\alpha|T| $. The only difference is the calculation of prediction error C(T), which uses entroy or Gini coefficient in CT, and mean squared error in RT. Emphasize again, the loss function of decision tree: Cα(T)=C(T)+α∣T∣C_{\\alpha}(T) = C(T) + \\alpha|T| Cα​(T)=C(T)+α∣T∣ C(T) is the prediction error of the decision tree, or the degree of fitting between training data and the tree model. |T| is the number of leaf nodes in the tree, which can represent the complexity of the tree model. So we can easily find that, C(T) and |T| are two controdictory values. A very small C(T) means that the degree of fitting between the model and the training data is very high, and the prediction result of the training set is very accurate (but the performance of test data is hard to say). Then complexity |T| should be very large. Similarly, a simple tree with low |T| (low complexity) has large C(T) (high prediction error in training set). The goal is to balance the accuracy and complexity of the model so that the combined loss function of the two is minimal. α≥0\\alpha \\ge 0α≥0 is also an important coefficient in the loss function. α\\alphaα is large =&gt; prompt to choose a simpler tree (small |T|) α\\alphaα is small =&gt; prompt to choose a more complex tree (large |T|) α\\alphaα is 0 =&gt; No pruning, current tree is the optimal. Only consider the fitting between model and data, and ignore the model complexity. (easily cause overfitting) α\\alphaα is +∞+\\infty+∞ =&gt; All pruning, only the root node is left. So pruning means when α\\alphaα is determined, select a Tree model with the least loss function value Cα(T)C_{\\alpha}(T)Cα​(T). General speaking, the larger the α\\alphaα is, the stronger the pruning is, and the smaller the optimal tree is (compared with the original decision tree) For a fixed α\\alphaα, there must be a unique subtree that minimizes the loss function Cα(T)C_{\\alpha}(T)Cα​(T). b. idea of pruning All above is the method of measuring the loss function of pruning tree. And the next is the idea of pruning. For any subtree TtT_tTt​ at the node t, if no pruning, its original loss is: Cα(Tt)=C(Tt)+α∣Tt∣C_{\\alpha}(T_t) = C(T_t) + \\alpha|T_t| Cα​(Tt​)=C(Tt​)+α∣Tt​∣ ∣Tt∣|T_t|∣Tt​∣ is the total number of leaf nodes of the node &quot;t&quot;. If prune the node &quot;t&quot; and only the root node is left, its loss after pruning is: Cα(t)=C(t)+αC_{\\alpha}(t) = C(t) + \\alpha Cα​(t)=C(t)+α Since, only the root node as one leaf node is left after pruning. α∣t∣=α\\alpha|t| = \\alphaα∣t∣=α Subtree &quot;TtT_tTt​&quot; is more complex than subtree &quot;t&quot;, the error C(Tt)≤C(t)C(T_t) \\le C(t)C(Tt​)≤C(t). So, if α\\alphaα is 0 or very small, Cα(Tt)≤Cα(t)C_{\\alpha}(T_t) \\le C_{\\alpha}(t)Cα​(Tt​)≤Cα​(t). When α\\alphaα increases to a certain degree, it will meet: Cα(Tt)=Cα(t)C_{\\alpha}(T_t) = C_{\\alpha}(t) Cα​(Tt​)=Cα​(t) If α\\alphaα continues to increase, then Cα(Tt)≥Cα(t)C_{\\alpha}(T_t) \\ge C_{\\alpha}(t)Cα​(Tt​)≥Cα​(t). In other words, the critical value of α\\alphaα derived from the equality before and after pruning is: C(Tt)+α∣T∣=C(t)+αα=C(t)−C(Tt)∣T∣−1\\begin{aligned} C(T_t) + \\alpha|T| &amp;= C(t) + \\alpha \\\\ \\alpha &amp;= \\frac{C(t)-C(T_t)}{|T|-1} \\end{aligned}C(Tt​)+α∣T∣α​=C(t)+α=∣T∣−1C(t)−C(Tt​)​​ In the pruning of node &quot;t&quot; Before pruning, lost of &quot;t&quot;: Cα(Tt)C_{\\alpha}(T_t)Cα​(Tt​) After pruning, lost of &quot;t&quot;: Cα(t)C_{\\alpha}(t)Cα​(t) There are 3 conditions of loss value after pruning: increase, unchange or decrease. If the loss value doesn't increase after pruning, that is, if Cα(Tt)≥Cα(t)C_{\\alpha}(T_t) \\ge C_{\\alpha}(t)Cα​(Tt​)≥Cα​(t), then pruning the subtree is better. It shows that complexity plays a key role, and the loss function plays a small role. In simple terms, the leaf nodes before pruning do not improve the accuracy but bring more complicated trees. c. CART pruning algorithm process Since, for a fixed α\\alphaα, there must be a unique subtree that minimizes the loss function Cα(T)C_{\\alpha}(T)Cα​(T). We can mark this subtree as TαT_{\\alpha}Tα​ Breiman has proved that the tree can be pruned recursively. Increase α\\alphaα from small to large, a0&lt;a1&lt;...&lt;an&lt;+∞a_0 &lt; a_1 &lt; ... &lt; a_n &lt; +\\inftya0​&lt;a1​&lt;...&lt;an​&lt;+∞, producing a series of intervals [ai,ai+1)[a_i,a_{i+1})[ai​,ai+1​), i=0,1,2,...,n. The optimal subtree sequence obtained by pruning is {T0,T1,...,TnT_0,T_1,...,T_nT0​,T1​,...,Tn​} Then in the optimal subtree sequence {T0,T1,...,TnT_0,T_1,...,T_nT0​,T1​,...,Tn​}, select the optimal subtree TαT_{\\alpha}Tα​ through cross-validation. The CART pruning algorithm: Input: the original decision tree T0T_0T0​ obtained by the CART tree algorithm in section 8. Output: Optimal decision subtree TαT_{\\alpha}Tα​ The algorithm process: Initialize αmin=+∞\\alpha_{min}=+\\inftyαmin​=+∞, Set of optimal subtree w={T0}w = \\{T_0\\}w={T0​} Calculate Cα(Tt),∣Tt∣,αC_{\\alpha}(T_t), |T_t|, \\alphaCα​(Tt​),∣Tt​∣,α Calculate the training error loss function Cα(Tt)C_{\\alpha}(T_t)Cα​(Tt​) of each internal node &quot;t&quot; from the leaf node from bottom to top. (The regression tree is the mean square error, and the classification tree is the Gini coefficient). Calculate the total number of leaf nodes of &quot;t&quot;, ∣Tt∣|T_t|∣Tt​∣, and Calculate the threshold of regularization cofficient α=min⁡{C(t)−C(Tt)∣Tt∣−1,αmin}\\alpha= \\min \\left\\{\\frac{C(t)-C(T_t)}{|T_t|-1}, \\alpha_{min} \\right\\}α=min{∣Tt​∣−1C(t)−C(Tt​)​,αmin​} Update the αmin=α\\alpha_{min} = \\alphaαmin​=α Get the set M of α\\alphaα values for all nodes. Chosse the smallest regularization coefficient value αk\\alpha_kαk​ from the set M, then access the internal nodes &quot;t&quot; from top to bottom. If C(t)−C(Tt)∣Tt∣−1≤αk\\frac{C(t)-C(T_t)}{|T_t|-1} \\le \\alpha_{k}∣Tt​∣−1C(t)−C(Tt​)​≤αk​, pruning is performed. And determine the value of the leaf node t. If it is a classification tree, it is the category with the highest probability; if it is a regression tree, it is the average of all sample outputs. The optimal subtree corresponding to the regularization coefficient αk\\alpha_{k}αk​ is obtained TkT_kTk​ Update the optimal subtree set w=w∪Tkw = w \\cup T_kw=w∪Tk​, and regularization coefficient set M=M−{αk}M=M-\\{\\alpha_k\\}M=M−{αk​} If www is not null, back to step 4. Else, all optimal subtrees with different α\\alphaα have been obtained in set www. Use cross-validation to test the prediction accuracy for all pruned tree in optimal subtree set www. And the tree with the best generalization prediction ability is returned as the final CART tree. 11. Summary of CART algorithm Algorithms Model Support Tree Structure Feature Selection Continuous Feature Missing Value Pruning ID3 Classification General Tree (Multitree) Information Gain Not Support Not Support Not Support C4.5 Classification General Tree (Multitree) Information Gain Ratio Support Support Support CART Classification &amp; Regression Binary Tree Gini Coefficient; Mean Square Error Support Support Support The CART algorithm still has some shortcomings: Whether it is ID3, C4.5 or CART, when making feature selection, they all choose the best one feature to build the decision tree node. However, in some case, the classification decision should not be determined by only one certain feature, but should be determined by a set of features. The decision tree obtained in this way is more accurate, which is called a multi-variate decision tree. When selecting the optimal feature, the multivariate decision tree does not select one certain optimal feature, but selects an optimal linear combination of features to make a decision. The representative of multi-variate decision tree algorithm is &quot;OC1&quot;. If the sample changes a little bit, it will cause a dramatic change in the tree structure. This can be solved by ensemble models, such as random forest in integrated learning. 12. Summary of Decision Tree Advantages of Decision Tree: The generated decision tree is very simple and intuitive. Basically no preprocessing is needed, such as normalization in advance, and missing values preprocessing. The cost in prediction of decision tree is O(log2m)O(log_2m)O(log2​m), m is the number of samples. Both discrete and continuous values can be processed. Many algorithms only focus on discrete values or continuous values. It can deal with multi categories classification problem. Compared with the black box classification model such as neural network, the decision tree can be well explained logically. Cross-validated pruning can be used to filter the model, thereby improving the generalization ability. High tolerance for some abnormal points. Disadvantages of Decision Tree: Decision tree algorithm is very easy to overfit, resulting in weak generalization ability. It can be improved by setting the minimum sample number of nodes and limiting the depth of decision tree. The decision tree structure will change dramatically when sample changes a little bit. This can be solved by ensemble models such as random forest. Finding the optimal decision tree is an NP-hard problem. We usually get into the local optimal by heuristic method. It can be improved by methods such as integrated learning. For some more complex relationships, decision trees are difficult to learn, such as XOR. There is no way for this. Generally, this relationship can be solved by using neural network classification methods. If the sample ratio of certain features is too large, generating decision trees tends to favor these features. This can be improved by adjusting the sample weights. ","link":"https://zl-wu.github.io/post/mla_decision_tree/"},{"title":"MLA -- Logistic Regression (sklearn instruction)","content":"This is a summary of using logistic regression libraries in sklearn. Focus on the matters that should be paid attention to in the tuning. Reference: sklearn's linear model Sklearn-LR code example In the previous article Logistic Regression Principle, it summarized the principle of logistic regression. Here is a summary of using logistic regression libraries in sklearn. Focus on the matters that should be paid attention to in the tuning. 1. Overview There are mainly 3 Class of logistic regression: LogisticRegression LogisticRegressionCV logistic_regression_path The main difference between LogisticRegression and LogisticRegressionCV is that LogisticRegressionCV uses cross-validation to select the regularization coefficient C. LogisticRegression needs to specify a regularization coefficient each time. Except this coefficient C, LogisticRegression and LogisticRegressionCV are basically used in the same way. The logistic_regression_path class is special. After fitting the data, it cannot directly make predictions, and can only select the appropriate logistic regression coefficient and regularization coefficient for the fitted data. It is mainly used in model selection. This class is generally not used. 2. Regularization Parameter: Penalty LogisticRegression and LogisticRegressionCV have regularization terms by default. Parameter penalty can be 'l1','l2','elasticnet','none'. Default value is 'l2'. l1l_1l1​: min⁡w,c∥w∥1+C∑i=1mlog(exp(−yi(XiTw+c))+1)\\min_{w,c}\\|w\\|_1 + C\\sum_{i=1}^mlog(exp(-y_i(X_i^Tw+c))+1) w,cmin​∥w∥1​+Ci=1∑m​log(exp(−yi​(XiT​w+c))+1) l2l_2l2​: min⁡w,c12wTw+C∑i=1mlog(exp(−yi(XiTw+c))+1)\\min_{w,c}\\frac{1}{2}w^Tw + C\\sum_{i=1}^mlog(exp(-y_i(X_i^Tw+c))+1) w,cmin​21​wTw+Ci=1∑m​log(exp(−yi​(XiT​w+c))+1) elasticnet: a combination of L1 and L2, and minimizes the following cost function: min⁡w,c1−ρ2wTw+ρ∥w∥1+C∑i=1mlog(exp(−yi(XiTw+c))+1)\\min_{w,c} \\frac{1-\\rho}{2}w^Tw + \\rho\\|w\\|_1 + C\\sum_{i=1}^mlog(exp(-y_i(X_i^Tw+c))+1) w,cmin​21−ρ​wTw+ρ∥w∥1​+Ci=1∑m​log(exp(−yi​(XiT​w+c))+1) When tuning the parameters, if the main purpose is to solve the overfitting, it is generally enough to choose L2 regularization for the penalty. However, if L2 regularization is still overfitting, that is, if the prediction performance is poor with L2, then L1 regularization can be considered. In addition, if the model has lots of features, we hope that some unimportant features' coefficients be zeroed, so that the model coefficients can be sparse, L1 regularization can also be used in this case. The choice of penalty parameter will affect the choice of loss function optimization algorithm (solver parameter): If penalty=&quot;l2l_2l2​&quot; (Ridge Regression), there are 4 optional algorithms (solver): &quot;newton-cg&quot; &quot;lbfgs&quot; &quot;liblinear&quot; &quot;sag&quot; If penalty=&quot;l1l_1l1​&quot; (Lasso Regression), there are only 1 optional algorithms (solver): solver=&quot;liblinear&quot;. Because the loss function with L1 regularization is not derivable, only &quot;liblinear&quot; can solve it. the other three methods [&quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;sag&quot;] requires the first or second continuous derivative of the loss function. 3. Optimization Algorithm Parameter: Solver There are 5 algorithms can be choosed to optimize Logistic Regression's Loss Function: &quot;newton-cg&quot;: A type of Newton's method family, iteratively optimizes the loss function by using the second derivative matrix of the loss function, the Hessian matrix. &quot;lbfgs&quot;: A kind of quasi-Newton method, iteratively optimizes the loss function by using the second derivative matrix of the loss function, the Hessian matrix. &quot;liblinear&quot;: Coordinate Axis Descent method &quot;sag&quot;: the random average gradient descent, a variant of the gradient descent method. The difference from the ordinary gradient descent method is that only a part of the samples are used to calculate the gradient in each iteration, which is suitable when there are many sample data. (like Mini-Batch Gradient Descent) &quot;saga&quot;: The “sag” solver uses Stochastic Average Gradient descent 6. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large. The “saga” solver 7 is a variant of “sag” that also supports the non-smooth penalty=&quot;l1&quot;. This is therefore the solver of choice for sparse multinomial logistic regression. It is also the only solver that supports penalty=&quot;elasticnet&quot;. &quot;sag&quot; uses only a part of the samples, so don't choose it when the sample size is small. And if the sample size is very large, such as greater than 100,000, &quot;sag&quot; is the first choice. But sag cannot be used for L1 regularization, so when you have a large number of samples and need L1 regularization, you have to make a trade-off. Either reduce the sample size by sampling the samples, or return to L2 regularization. Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing. Algorithm to use in the optimization problem. For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones. For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes. ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty ‘liblinear’ and ‘saga’ also handle L1 penalty ‘saga’ also supports ‘elasticnet’ penalty ‘liblinear’ does not support setting penalty='none' References L-BFGS-B – Software for Large-scale Bound-constrained Optimization Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales. http://users.iems.northwestern.edu/~nocedal/lbfgsb.html LIBLINEAR – A Library for Large Linear Classification https://www.csie.ntu.edu.tw/~cjlin/liblinear/ SAG – Mark Schmidt, Nicolas Le Roux, and Francis Bach Minimizing Finite Sums with the Stochastic Average Gradient https://hal.inria.fr/hal-00860051/document SAGA – Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014). SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives https://arxiv.org/abs/1407.0202 Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent methods for logistic regression and maximum entropy models. Machine Learning 85(1-2):41-75. https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf 4. Classification Type Parameter: Multi_class There are three values can be choose of the parameter multi_class: &quot;ovr&quot;: one-vs-rest(OvR) &quot;multinomial&quot;: many-vs-many(MvM) &quot;auto&quot; If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. ‘multinomial’ is unavailable when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’. The idea of OvR is very simple. No matter how many catogeries in logistic regression classification model, we can treat them as many binary logistic regression. For example, if there are K categories, we view each one of K categories as postive and the rest are negative. Finally we get K-1 classifiers. (like cross validation) The idea of MvM is relatively complicated. For example, if there are T categories, we pick two categories's data from the samples each time, and noted them as T1 and T2 classes (there are CT2C_T^2CT2​ ways). Use T1 as a positive class, and T2 as a negative class to perform binary logistic regression to obtain model parameters. Finally, We need $T(T-1) / 2 $ classifications in total. As we can see, OvR is relatively simple, but the classification performance is relatively poor.(here refers to the distribution of most samples, OvR may be better under certain sample distributions). The MvM classification is relatively accurate, but the classification speed is not as fast as OvR. If choose &quot;ovr&quot;, all 5 solvers can be chose: liblinear, newton-cg, lbfgs, sag, saga. If choose &quot;multinomial&quot;, except &quot;liblinear&quot;, we can use all other 4 solvers. 5. Type Weight Parameter: class_weight class_weight: dict or ‘balanced’, default=None The class_weight parameter is used to indicate the various categories' weights in the classification model. If None, all categories' weights are the same. Or we can input the weights of each category manually. For a binary model of 0 and 1 as an example, we can define the class_weight = {0: 0.9, 1: 0.1}, so that the weight of type 0 is 90%, and the weight of type 1 is 10%. If class_weight=&quot;balanced&quot;, then the library will calculate the weight based on the training sample size. The larger the sample size of a certain type, the lower the weight, and the smaller the sample size, the higher the weight. So what does class_weight do? In the classification model, we often encounter two types of problems: cost of misclassification. For example, it is very expensive to classify legal users and illegal users and classify illegal users as legal users. We prefer to classify legal users as illegal users. At this time, we can manually re-screen, but we do not want to classify illegal users as legal users . At this time, we can appropriately increase the weight of illegal users. the sample is highly unbalanced. For example, we have 10,000 binary sample data of legal users and illegal users. There are 9995 legal users and only 5 illegal users. If we do n’t consider the weight, we can divide all Of the test sets are predicted as legitimate users, so the prediction accuracy rate is 99.95% in theory, but it does not make any sense. At this time, we can choose balanced to let the class library automatically increase the weight of illegal user samples. And for the second type of sample imbalance, we can also consider using the sample weight parameter mentioned in the next section: sample_weight instead of class_weight. 6. sample_weight Due to the sample imbalance, the sample is not an unbiased estimate of the overall sample, which may lead to a decline in our model's predictive ability. In this case, we can try to solve this problem by adjusting the sample weight. There are two ways to adjust the sample weight. The first is to use balanced in class_weight. The second is to adjust the weight of each sample by sample_weight when calling the fit function. When doing logistic regression in scikit-learn, if the above two methods are used, then the real weight of the sample is class_weight * sample_weight. ","link":"https://zl-wu.github.io/post/mla-logistic-regression-sklearn-instruction/"},{"title":"MLA -- Regularization Lasso Regression","content":"&quot;What is the lasso regression? Add L2 norm into loss function.&quot; Norm is a commonly used concept in mathematics, which is also often encountered in machine learning. The first thing that needs to be clear is that Norm is a function. We usually use it to measure the value of the vector in machine learning. The norm is defined as: ∥x∥p=(∑i=1m∣xi∣p)1/p\\|x\\|_p = \\left(\\sum_{i=1}^m |x_i|^p\\right)^{1/p} ∥x∥p​=(i=1∑m​∣xi​∣p)1/p Common Norm: L2L^2L2 Norm: When p is 2, L2L^2L2 Norm is also called Euclidean norm. It represents the distance from the original to the points determined by the vector x. It is applied very frequently in machine learning. ∥x∥2=(∑i=1m∣xi∣2)1/2\\|x\\|_2 = \\left(\\sum_{i=1}^m |x_i|^2\\right)^{1/2} ∥x∥2​=(i=1∑m​∣xi​∣2)1/2 Square L2L^2L2 Norm: It is square of L2L^2L2 norm, $ |x|_2^2$. The advantage is that it is obviously easier to calculate, which can be simply calculated by the dot product of XTXX^TXXTX. L1L^1L1 Norm: In some case, L2L^2L2 Norm is not very popular, because it grows very slowly near the origin. Sometimes, it is important to distinguish elements that happen to be zero and non-zero but have very small value. In this case, we can use L1L^1L1 Norm: ∥x∥1=∑i=1m∣xi∣\\|x\\|_1 = \\sum_{i=1}^m |x_i| ∥x∥1​=i=1∑m​∣xi​∣ L∞L^\\inftyL∞ Norm: It is also called Maximum norm, which is the absolute value of the element with largest amplitude in the vector: ∣∣x∣∣∞=max∣xi∣||x||_\\infty = max|x_i| ∣∣x∣∣∞​=max∣xi​∣ Regularization in Regression 1. Linear Regression Review The norm form of linear regression: hθ(X)=Xθh_\\theta(X) = X\\theta hθ​(X)=Xθ The loss function that we need to minimize: J(θ)=12(Xθ−Y)T(Xθ−Y)J(\\theta) = \\frac{1}{2}(X\\theta - Y)^T(X\\theta - Y) J(θ)=21​(Xθ−Y)T(Xθ−Y) Gradient Descent:θ=θ−βXT(Xθ−Y)\\theta = \\theta - \\beta X^T(X\\theta - Y) θ=θ−βXT(Xθ−Y) Least Square:θ=(XTX)−1XTY\\theta = (X^TX)^{-1}X^TY θ=(XTX)−1XTY 2. Ridge Regression Review Since applying linear regression directly may produce verfitting problem, we need to add the regularization term. When L2L^2L2 Norm is added, it is called Ridge Regression. J(θ)=12(Xθ−Y)T(Xθ−Y)+12α∥θ∥22J(\\theta) = \\frac{1}{2}(X\\theta - Y)^T(X\\theta - Y) + \\frac{1}{2}\\alpha\\|\\theta\\|_2^2 J(θ)=21​(Xθ−Y)T(Xθ−Y)+21​α∥θ∥22​ α\\alphaα is the constant coeffient, which is used to adjust the weights of linear regression term and regularization term. ∥θ∥2\\|\\theta\\|_2∥θ∥2​ is the L2 norm of θ\\thetaθ vector. Ridge Regression is very simialr to normal linear regression. Gradient Descent: θ=θ−β(XT(Xθ−Y)+αθ)=θ(1−αβ)−β(XT(Xθ−Y))\\begin{aligned} \\theta &amp;= \\theta - \\beta(X^T(X\\theta-Y)+\\alpha\\theta) \\\\ &amp;=\\theta(1-\\alpha\\beta) - \\beta(X^T(X\\theta-Y)) \\end{aligned} θ​=θ−β(XT(Xθ−Y)+αθ)=θ(1−αβ)−β(XT(Xθ−Y))​ Least Square: θ=(XTX+αE)−1XTY\\theta = (X^TX + \\alpha E)^{-1}X^TY θ=(XTX+αE)−1XTY Ridge regression reduces the regression coefficients without abandoning any variables. When the coefficient is close to 0, it is equivalent to weakening the significance of its feature. But this model still has a lot of variables, and the model is poorly interpretable. Is there a compromise? That is, it can prevent overfitting and overcome the shortcomings of the Ridge regression model with many variables? Yes, this is the Lasso regression mentioned below. 2. Lasso Regression Similar as Ridge Regreesion, Lasso Regression also adds a Norm term in the loss function to try to avoid overfitting. When L1L^1L1 Norm is added, it is called Lasso Regression. J(θ)=12m(Xθ−Y)T(Xθ−Y)+α∥θ∥1J(\\theta) = \\frac{1}{2m}(X\\theta - Y)^T(X\\theta - Y) + \\alpha\\|\\theta\\|_1 J(θ)=2m1​(Xθ−Y)T(Xθ−Y)+α∥θ∥1​ m is the count of sample data. α\\alphaα is the constant coeffient, which is used to adjust the weights of linear regression term and regularization term. It needs to be tuned. ∥θ∥1\\|\\theta\\|_1∥θ∥1​ is the L1 norm of θ\\thetaθ vector. Lasso regression makes some coefficients smaller, and even some coefficients with smaller absolute values directly become 0, so it is especially suitable for the reduction of the number of parameters and the selection of parameters, so it is used to estimate the linear model of sparse parameters. However, there is a big problem with Lasso regression. Its loss function is not continuous and differentiable. Since the L1 norm uses the sum of absolute values, the loss function is not derivative. That is to say, Least Square method, Gradient Descent method, Newton method and Quasi-Newton method all failed in this case. So how can we find the minimum value of the loss function with this L1 norm? There are two new methods to get extreme value: Coordinate Descent Least Angle Regression, LARS 3. Coordinate Descent method for Lasso Regression As the name suggests, Coordinate Descent is the descent in the direction of the coordinate axis, different from the gradient direction in the gradient descent. But both are iterative methods in a heuristic way. Algorithm Process: Intialize θ\\thetaθ as θ(0)\\theta^{(0)}θ(0). 0 represents the current iteration is 0. In k-th iteration, we calculate θi(k)\\theta_i^{(k)}θi(k)​ started from θ1(k)\\theta_1^{(k)}θ1(k)​ to θn(k)\\theta_n^{(k)}θn(k)​: θi(k)=arg min⁡θiJ(θ1(k),θ2(k),...,θi−1(k),θi,θi+1(k−1),...,θn(k−1))\\theta_i^{(k)}=\\argmin_{\\theta_i} J(\\theta_1^{(k)},\\theta_2^{(k)},...,\\theta_{i-1}^{(k)},\\theta_{i},\\theta_{i+1}^{(k-1)},...,\\theta_{n}^{(k-1)} ) θi(k)​=θi​argmin​J(θ1(k)​,θ2(k)​,...,θi−1(k)​,θi​,θi+1(k−1)​,...,θn(k−1)​) &gt;In this case, $J(\\theta)$ only has one variable $\\theta_i$, and others are all constant. Hence, the minimum value of $J(\\theta)$ can be easily obtained by differentiation. Let's be more specific, in k-th iteration: θ1(k)∈arg min⁡θ1J(θ1,θ2(k−1),...,θn(k−1))θ2(k)∈arg min⁡θ2J(θ1(k),θ2,θ3(k−1)...,θn(k−1))...θn(k)∈arg min⁡θnJ(θ1(k),θ2(k),...,θn−1(k),θn)\\begin{aligned} \\theta_1^{(k)} &amp;\\in \\argmin_{\\theta_1}J(\\theta_1, \\theta_2^{(k-1)},...,\\theta_n^{(k-1)}) \\\\ \\theta_2^{(k)} &amp;\\in \\argmin_{\\theta_2}J(\\theta_1^{(k)}, \\theta_2, \\theta_3^{(k-1)}...,\\theta_n^{(k-1)}) \\\\ ... \\\\ \\theta_n^{(k)} &amp;\\in \\argmin_{\\theta_n}J(\\theta_1^{(k)}, \\theta_2^{(k)}, ...,\\theta_{n-1}^{(k)},\\theta_n) \\end{aligned} θ1(k)​θ2(k)​...θn(k)​​∈θ1​argmin​J(θ1​,θ2(k−1)​,...,θn(k−1)​)∈θ2​argmin​J(θ1(k)​,θ2​,θ3(k−1)​...,θn(k−1)​)∈θn​argmin​J(θ1(k)​,θ2(k)​,...,θn−1(k)​,θn​)​ Comparing the θ(k)\\theta^{(k)}θ(k) vector with θ(k−1)\\theta^{(k-1)}θ(k−1) vector, if the changes are small enough, then θ(k)\\theta^{(k)}θ(k) is the final return. Otherwise, jumping to Step 2 and continuing (k+1)-th iteration. 4. Least Angle Regression method for Lasso Regression Before introducing Least Angle Regression, let ’s look at two preliminary algorithms: (Unfinished) 4.1 Forward Selection Algorithm 4.2 Forward Stagewise Algorithm 4.3 Least Angle Regression Algorithm (LARS) 5. Conclusion Reference: LEAST ANGLE REGRESSION By BRADLEY EFRON [web]( ","link":"https://zl-wu.github.io/post/mla-regularization-lasso-regression/"},{"title":"MLA -- Logistic Regression","content":"&quot;Logistic Regression is a kind of very classical and basic classification algorithm.&quot; Logistic Regression is a kind of classification algorithm, it can implement binary classification and multiple classification. It is also a supervised learning algorithm, it implements a mapping from a given data set to 0 and 1 in binary case. Although it is a classification model, the principle of regression still remains in the model. 1. From Linear Regression to Logistic Regression In Linear Regression model, the linear relationship between output vector Y and input sample matrix X is: $ Y = X\\theta $. In this case, Y is continuous, hence, it is a regression model. If we want the output Y is the descrete value (for classification), one method is to do a second transformation on Y, g(Y). Through the function g(Y), all continous value can be mapped to descrete value. For example, Y belongs to category A when it is in a real number interval, and category B when it is in another real number interval. 2. Binary Logistic Regression model In Logistic Regression, we usually use a special function g(Y) to transform continous value Y: g(z)=11+e−zg(z) = \\frac{1}{1+e^{-z}} g(z)=1+e−z1​ This function has some very good properties, which are suitable for classification probability model: g(z)∈(0,1)g(z) \\in (0,1)g(z)∈(0,1) When z tends to positive infinity +∞+\\infty+∞, g(z) tends to 1. When z tends to negative infinity −∞-\\infty−∞, g(z) tends to 0. A good derivative propertiy:g′(z)=g(z)(1−g(z))g&#x27;(z) = g(z)(1-g(z)) g′(z)=g(z)(1−g(z)) If z=xθz=x\\thetaz=xθ, the general form of logistic regression model is: hθ(x)=11+e−xθh_\\theta(x) = \\frac{1}{1+e^{-x\\theta}} hθ​(x)=1+e−xθ1​ x is one 1xn data vector to be predicted. (n features) θ\\thetaθ is a nx1 parameters vector. xθx\\thetaxθ is a linear regression scalar. hθ(x)h_\\theta(x)hθ​(x) can be understood as the probability of one certain category. We have this correspondence with our binary sample output y (assuming 0 and 1): If hθ(x)h_\\theta(x)hθ​(x) &gt; 0.5 (xθx\\thetaxθ &gt; 0), predict y = 1. If hθ(x)h_\\theta(x)hθ​(x) &lt; 0.5 (xθx\\thetaxθ &lt; 0), predict y = 0. The smaller the value of hθ(x)h_\\theta(x)hθ​(x), the higher the probability of being classified as 0. Conversely, the larger the value of hθ(x)h_\\theta(x)hθ​(x), the higher the probability of being classified as 1. If it is close to the critical point (0.5), the classification accuracy will decrease. In matrix expression: hθ(X)=11+e−Xθh_\\theta(X) = \\frac{1}{1+e^{-X\\theta}} hθ​(X)=1+e−Xθ1​ hθ(X)h_\\theta(X)hθ​(X) is the output of logistic regression model. mx1 dimensions. X is the input sample data features matrix. mxn dimensions. θ\\thetaθ is the model coefficients for classification. nx1 dimensions. After understanding the Logistic Regression model of binary classification, we have to look at the loss function of the model, our goal is to minimize the loss function to get the corresponding model coefficients θ\\thetaθ. 3. Loss Function of Binary Logistic Regression Since the linear regression model is continuous, its loss function is Square Sum of Error. However, the logistic funtion is not continuous, SSE is not feasible in this case (will prove it later in the article). Now, we can use Maximum Likelihood Method to derive the loss function. Suppose the output sample is two value: 0 or 1, then it follows Bernoulli distribution. P(y=1∣x,θ)=hθ(x)P(y=0∣x,θ)=1−hθ(x)\\begin{aligned} &amp;P(y=1|x,\\theta) = h_\\theta(x) \\\\ &amp;P(y=0|x,\\theta) = 1- h_\\theta(x) \\end{aligned} ​P(y=1∣x,θ)=hθ​(x)P(y=0∣x,θ)=1−hθ​(x)​ Combine above two formula, the probability distribution function is: P(y∣x,θ)=hθ(x)y(1−hθ(x))1−yP(y|x,\\theta) = h_\\theta(x)^y(1-h_\\theta(x))^{1-y} P(y∣x,θ)=hθ​(x)y(1−hθ​(x))1−y y∈{0,1}y \\in \\{0,1\\}y∈{0,1} Through likelihood function maxmization, we can derive the model coefficient parameters θ\\thetaθ that we need. Assume that the samples are independent and identically distributed, Likelihood Function is: L(θ)=∏i=1m(hθ(x(i)))y(i)(1−hθ(x(i)))1−y(i) L(\\theta) = \\prod_{i=1}^m (h_\\theta(x^{(i)}))^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{1-y^{(i)}} L(θ)=i=1∏m​(hθ​(x(i)))y(i)(1−hθ​(x(i)))1−y(i) m is the counts of sample The function that inverts the logarithmization of the likelihood function, that is, the loss function is: J(θ)=−ln(L(θ))=−∑i=1m(y(i)ln(hθ(x(i)))+(1−y(i))ln(1−hθ(x(i))))\\begin{aligned} J(\\theta) &amp;= -ln(L(\\theta)) \\\\ &amp;=-\\sum_{i=1}^m (y^{(i)}ln(h_\\theta(x^{(i)})) + (1-y^{(i)})ln(1-h_\\theta(x^{(i)}))) \\end{aligned} J(θ)​=−ln(L(θ))=−i=1∑m​(y(i)ln(hθ​(x(i)))+(1−y(i))ln(1−hθ​(x(i))))​ In Matrix Expression: J(θ)=−YTln(hθ(X))−(E−Y)Tln(E−hθ(X))J(\\theta) = -Y^Tln(h_\\theta(X)) - (E-Y)^Tln(E-h_\\theta(X)) J(θ)=−YTln(hθ​(X))−(E−Y)Tln(E−hθ​(X)) E is an all 1 vector. 4. Optimization method of Loss Function There are lots of method to minimize the loss function of Logistic Regression model. Most common methods include gradient descent, coordinate descent, and Newton's method. Only each iteration's formula of gradient descent method is derived here. 4.1 Algebraic Expression J(θ)=−∑i=1m(y(i)ln(hθ(x(i)))+(1−y(i))ln(1−hθ(x(i))))J(\\theta) = -\\sum_{i=1}^m (y^{(i)}ln(h_\\theta(x^{(i)})) + (1-y^{(i)})ln(1-h_\\theta(x^{(i)}))) J(θ)=−i=1∑m​(y(i)ln(hθ​(x(i)))+(1−y(i))ln(1−hθ​(x(i)))) ∂∂θjJ(θ)=−∂∑i=1m(y(i)ln(hθ(x(i)))+(1−y(i))ln(1−hθ(x(i))))∂θj=−∑i=1my(i)hθ(x(i))∂hθ(x(i))∂θj−1−y(i)1−hθ(x(i))∂hθ(x(i))∂θj=−∑i=1m(y(i)hθ(x(i))−1−y(i)1−hθ(x(i)))∂hθ(x(i))∂θj=−∑i=1my(i)−hθ(x(i))hθ(x(i))(1−hθ(x(i)))hθ′(x(i))∂Xθ∂θj=−∑i=1my(i)−hθ(x(i))hθ(x(i))(1−hθ(x(i)))hθ(x(i))(1−hθ(x(i)))xj(i)=−∑i=1m(y(i)−hθ(x(i)))xj(i)=∑i=1m(hθ(x(i))−y(i))xj(i)\\begin{aligned} \\frac{\\partial}{\\partial\\theta_j}J(\\theta) &amp;= -\\frac{\\partial\\sum_{i=1}^m (y^{(i)}ln(h_\\theta(x^{(i)})) + (1-y^{(i)})ln(1-h_\\theta(x^{(i)})))}{\\partial\\theta_j} \\\\ &amp;= - \\sum_{i=1}^m \\frac{y^{(i)}}{h_\\theta(x^{(i)})} \\frac{\\partial h_\\theta(x^{(i)})}{\\partial\\theta_j} - \\frac{1-y^{(i)}}{1-h_\\theta(x^{(i)})} \\frac{\\partial h_\\theta(x^{(i)})}{\\partial\\theta_j} \\\\ &amp;= - \\sum_{i=1}^m (\\frac{y^{(i)}}{h_\\theta(x^{(i)})} - \\frac{1-y^{(i)}}{1-h_\\theta(x^{(i)})}) \\frac{\\partial h_\\theta(x^{(i)})}{\\partial\\theta_j} \\\\ &amp;= - \\sum_{i=1}^m \\frac{y^{(i)} - h_\\theta(x^{(i)})}{h_\\theta(x^{(i)})(1-h_\\theta(x^{(i)}))} h&#x27;_\\theta(x^{(i)})\\frac{\\partial X\\theta}{\\partial\\theta_j} \\\\ &amp;= - \\sum_{i=1}^m \\frac{y^{(i)} - h_\\theta(x^{(i)})}{h_\\theta(x^{(i)})(1-h_\\theta(x^{(i)}))} h_\\theta(x^{(i)})(1-h_\\theta(x^{(i)})) x_j^{(i)} \\\\ &amp;= - \\sum_{i=1}^m (y^{(i)} - h_\\theta(x^{(i)}))x_j^{(i)} \\\\ &amp;= \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\end{aligned} ∂θj​∂​J(θ)​=−∂θj​∂∑i=1m​(y(i)ln(hθ​(x(i)))+(1−y(i))ln(1−hθ​(x(i))))​=−i=1∑m​hθ​(x(i))y(i)​∂θj​∂hθ​(x(i))​−1−hθ​(x(i))1−y(i)​∂θj​∂hθ​(x(i))​=−i=1∑m​(hθ​(x(i))y(i)​−1−hθ​(x(i))1−y(i)​)∂θj​∂hθ​(x(i))​=−i=1∑m​hθ​(x(i))(1−hθ​(x(i)))y(i)−hθ​(x(i))​hθ′​(x(i))∂θj​∂Xθ​=−i=1∑m​hθ​(x(i))(1−hθ​(x(i)))y(i)−hθ​(x(i))​hθ​(x(i))(1−hθ​(x(i)))xj(i)​=−i=1∑m​(y(i)−hθ​(x(i)))xj(i)​=i=1∑m​(hθ​(x(i))−y(i))xj(i)​​ In gradient descent iteration: θj=θj−α∑i=1m(hθ(x(i))−y(i))xj(i)\\theta_j = \\theta_j - \\alpha\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} θj​=θj​−αi=1∑m​(hθ​(x(i))−y(i))xj(i)​ j = 0,1,2,...,n 4.2 Matrix Expression J(θ)=−YTln(hθ(X))−(E−Y)Tln(E−hθ(X))J(\\theta) = -Y^Tln(h_\\theta(X)) - (E-Y)^Tln(E-h_\\theta(X)) J(θ)=−YTln(hθ​(X))−(E−Y)Tln(E−hθ​(X)) ∂∂θJ(θ)=XT[1hθ(X)⊙hθ(X)⊙(E−hθ(X)⊙(−Y))]+XT[1E−hθ(X)⊙hθ(X)⊙(E−hθ(X)⊙(E−Y))]=XT(hθ(X)−Y)\\begin{aligned} \\frac{\\partial}{\\partial\\theta}J(\\theta) &amp;= X^T \\left[\\frac{1}{h_\\theta(X)} \\odot h_\\theta(X) \\odot (E-h_\\theta(X) \\odot (-Y)) \\right] + X^T \\left[\\frac{1}{E-h_\\theta(X)} \\odot h_\\theta(X) \\odot (E-h_\\theta(X) \\odot (E-Y)) \\right] \\\\ &amp;= X^T(h_\\theta(X) - Y) \\end{aligned} ∂θ∂​J(θ)​=XT[hθ​(X)1​⊙hθ​(X)⊙(E−hθ​(X)⊙(−Y))]+XT[E−hθ​(X)1​⊙hθ​(X)⊙(E−hθ​(X)⊙(E−Y))]=XT(hθ​(X)−Y)​ It used the chain rules of vector differentiation: ∂∂Xlog(X)=1/X\\frac{\\partial}{\\partial X}log(X)=1/X∂X∂​log(X)=1/X ∂∂zg(z)=g(z)(1−g(z))\\frac{\\partial}{\\partial z}g(z) = g(z)(1-g(z))∂z∂​g(z)=g(z)(1−g(z)), [g(z) is the sigmoid function] ∂Xθ∂θ=X\\frac{\\partial X\\theta}{\\partial \\theta}= X∂θ∂Xθ​=X The iteration formula of θ\\thetaθ is : θ=θ−αXT(hθ(X)−Y)\\theta = \\theta - \\alpha X^T(h_\\theta(X)-Y) θ=θ−αXT(hθ​(X)−Y) 5. Regularization of Logistic Regression In order to avoiding overfitting, we need to consider regularization. Most common methods are L1 and L2 regularization. L1L_1L1​ regularization J(θ)=−YTln(hθ(X))−(E−Y)Tln(E−hθ(X))+α∥θ∥1J(\\theta) = -Y^Tln(h_\\theta(X)) - (E-Y)^Tln(E-h_\\theta(X)) + \\alpha\\|\\theta\\|_1 J(θ)=−YTln(hθ​(X))−(E−Y)Tln(E−hθ​(X))+α∥θ∥1​ This L1 loss function is not derivable, we can find the parameters that minimize funtion based on two methods: Coordinate Descent or Least Angle Regression L2L_2L2​ regularization J(θ)=−YTln(hθ(X))−(E−Y)Tln(E−hθ(X))+12α∥θ∥22J(\\theta) = -Y^Tln(h_\\theta(X)) - (E-Y)^Tln(E-h_\\theta(X)) + \\frac{1}{2}\\alpha\\|\\theta\\|_2^2 J(θ)=−YTln(hθ​(X))−(E−Y)Tln(E−hθ​(X))+21​α∥θ∥22​ The optimization method of L2 loss function is similar to ordinary logistic regression. 6. Promotion of binary : Multiple Logistic Regression Binary Case P(y=1∣x,θ)=hθ(x)=11+e−xθ=exθ1+exθP(y=0∣x,θ)=1−hθ(x)=11+exθ\\begin{aligned} &amp;P(y=1|x,\\theta) = h_\\theta(x) = \\frac{1}{1+e^{-x\\theta}} = \\frac{e^{x\\theta}}{1+e^{x\\theta}} \\\\ &amp;P(y=0|x,\\theta) = 1-h_\\theta(x) = \\frac{1}{1+e^{x\\theta}} \\end{aligned} ​P(y=1∣x,θ)=hθ​(x)=1+e−xθ1​=1+exθexθ​P(y=0∣x,θ)=1−hθ​(x)=1+exθ1​​ y can only be 0 or 1, then: ln(P(y=1∣x,θ)P(y=0∣x,θ))=xθln\\left(\\frac{P(y=1|x,\\theta)}{P(y=0|x,\\theta)}\\right) = x\\theta ln(P(y=0∣x,θ)P(y=1∣x,θ)​)=xθ Suppose there is a K classification model, the value of the sample output y is 1,2,...,K. According to the experience of binary classification, we get: ln(P(y=1∣x,θ)P(y=K∣x,θ))=xθ1ln(P(y=2∣x,θ)P(y=K∣x,θ))=xθ2...ln(P(y=K−1∣x,θ)P(y=K∣x,θ))=xθK−1\\begin{aligned} &amp;ln\\left(\\frac{P(y=1|x,\\theta)}{P(y=K|x,\\theta)}\\right) = x\\theta_1 \\\\ &amp;ln\\left(\\frac{P(y=2|x,\\theta)}{P(y=K|x,\\theta)}\\right) = x\\theta_2 \\\\ &amp;... \\\\ &amp;ln\\left(\\frac{P(y=K-1|x,\\theta)}{P(y=K|x,\\theta)}\\right) = x\\theta_{K-1} \\\\ \\end{aligned} ​ln(P(y=K∣x,θ)P(y=1∣x,θ)​)=xθ1​ln(P(y=K∣x,θ)P(y=2∣x,θ)​)=xθ2​...ln(P(y=K∣x,θ)P(y=K−1∣x,θ)​)=xθK−1​​ There are K-1 equations above, and the sum of all probability is 1: ∑i=1KP(y=i∣x,θ)=1\\sum_{i=1}^KP(y=i|x,\\theta) = 1 i=1∑K​P(y=i∣x,θ)=1 Then there are K equations now. Solving this K linear equations, the probability distribution of K logistic regression is as follows: P(y=k∣x,θ)=exθk1+∑t=1K−1exθtk=1,2,...,K−1P(y=K∣x,θ)=11+∑t=1K−1exθt\\begin{aligned} &amp;P(y=k|x,\\theta) = \\frac{e^{x\\theta_k}}{1+\\sum_{t=1}^{K-1}e^{x\\theta_t}} &amp;k=1,2,...,K-1 \\\\ &amp;P(y=K|x,\\theta) = \\frac{1}{1+\\sum_{t=1}^{K-1}e^{x\\theta_t}} \\end{aligned} ​P(y=k∣x,θ)=1+∑t=1K−1​exθt​exθk​​P(y=K∣x,θ)=1+∑t=1K−1​exθt​1​​k=1,2,...,K−1 The loss function derivation and optimization of multiple logistic regression is similar to that of binary logistic regression. 7. Conclusion Logistic regression, especially binary logistic regression, is a very common model. The training speed is very fast. Although it is not as mainstream as the support vector machine (SVM), it is enough to solve normal classification problems. The training speed is also faster than SVM. Question: For logistic regression, why is the Square Sum of Error non-convex and not suitable as the loss function? Suppose we use SSE as logistic regression's loss function: J(θ)=12∑i=1m(y^i−yi)2J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(\\hat{y}_i - y_i)^2 J(θ)=21​i=1∑m​(y^​i​−yi​)2 y^i=11+e−xiθ\\hat{y}_i = \\frac{1}{1+e^{-x_i\\theta}} y^​i​=1+e−xi​θ1​ To determine whether J is a convex function, it depends on whether its second derivative is greater than 0. ∂∂θjJ(θ)=∑i=1m(y^i−yi)∂y^i∂θj=∑i=1m(y^i−yi)y^i(1−y^i)xj(i)=∑i=1m(−y^i3+(yi+1)y^i2−yiy^i)xj(i)\\begin{aligned} \\frac{\\partial}{\\partial\\theta_j}J(\\theta) &amp;= \\sum_{i=1}^m(\\hat{y}_i - y_i)\\frac{\\partial\\hat{y}_i}{\\partial\\theta_j} \\\\ &amp;= \\sum_{i=1}^m(\\hat{y}_i - y_i)\\hat{y}_i(1-\\hat{y}_i)x_j^{(i)} \\\\ &amp;= \\sum_{i=1}^m(-\\hat{y}_i^3 + (y_i+1)\\hat{y}_i^2 - y_i\\hat{y}_i)x_j^{(i)} \\end{aligned} ∂θj​∂​J(θ)​=i=1∑m​(y^​i​−yi​)∂θj​∂y^​i​​=i=1∑m​(y^​i​−yi​)y^​i​(1−y^​i​)xj(i)​=i=1∑m​(−y^​i3​+(yi​+1)y^​i2​−yi​y^​i​)xj(i)​​ ∂2∂θjJ(θ)=∑i=1m(−3y^i2+2(yi+1)y^i−yi)xj(i)∂y^∂θj=∑i=1m[−3y^i2+2(yi+1)y^i−yi]y^i(1−y^i)(xj(i))2\\begin{aligned} \\frac{\\partial^2}{\\partial\\theta_j}J(\\theta) &amp;= \\sum_{i=1}^m(-3\\hat{y}_i^2 + 2(y_i+1)\\hat{y}_i - y_i)x_j^{(i)}\\frac{\\partial\\hat{y}}{\\partial\\theta_j} \\\\ &amp;= \\sum_{i=1}^m\\left[-3\\hat{y}_i^2 + 2(y_i+1)\\hat{y}_i - y_i\\right]\\hat{y}_i(1-\\hat{y}_i)(x_j^{(i)})^2 \\end{aligned} ∂θj​∂2​J(θ)​=i=1∑m​(−3y^​i2​+2(yi​+1)y^​i​−yi​)xj(i)​∂θj​∂y^​​=i=1∑m​[−3y^​i2​+2(yi​+1)y^​i​−yi​]y^​i​(1−y^​i​)(xj(i)​)2​ y^∈(0,1)\\hat{y} \\in (0,1)y^​∈(0,1), hence, y^i(1−y^i)(xj(i))2\\hat{y}_i(1-\\hat{y}_i)(x_j^{(i)})^2y^​i​(1−y^​i​)(xj(i)​)2 &gt; 0. Therefore, the positive and negative property of the second derivative of J is determined by the term [−3y^i2+2(yi+1)y^i−yi][-3\\hat{y}_i^2 + 2(y_i+1)\\hat{y}_i - y_i][−3y^​i2​+2(yi​+1)y^​i​−yi​] And yi∈{0,1}y_i \\in \\{0,1\\}yi​∈{0,1} when yi=0y_i=0yi​=0, the term is [−3y^i2+2y^i][-3\\hat{y}_i^2 + 2\\hat{y}_i][−3y^​i2​+2y^​i​]. The condition of this term being greater than 0 is y^&lt;2/3\\hat{y}&lt;2/3y^​&lt;2/3 when yi=1y_i=1yi​=1, the term is [−3y^i2+4y^i−1]=−(3y^i−1)(y^i−1)[-3\\hat{y}_i^2 + 4\\hat{y}_i - 1]=-(3\\hat{y}_i-1)(\\hat{y}_i-1)[−3y^​i2​+4y^​i​−1]=−(3y^​i​−1)(y^​i​−1). The condition of this term being greater than 0 is y^&lt;1/3\\hat{y}&lt;1/3y^​&lt;1/3 As we can see, only when y^∈(0,13)\\hat{y} \\in (0,\\frac{1}{3})y^​∈(0,31​), we are sure the second derivative is greater than 0. The second derivative of J is not strictly greater than 0, so J is not a convex function. And J (SSE) is not suitable as Loss Function. ","link":"https://zl-wu.github.io/post/mla-logistic-regression/"},{"title":"MLA -- Linear Regression Principle","content":"&quot;Linear Regression is the beginning and the most basic algorithm.&quot; Linear Regression is the most basic question in Machine Learning. Here is a simple summary of Linear Regression Algorithm principle. 1. Linear Regression Question If we have m sample data, each sample has n features and one result value: (x1(1),x2(1),...,xn(1),y1),(x1(2),x2(2),...,xn(2),y2),...,(x1(m),x2(m),...,xn(m),ym)(x^{(1)}_1,x^{(1)}_2,...,x^{(1)}_n, y_1), (x^{(2)}_1,x^{(2)}_2,...,x^{(2)}_n, y_2), ..., (x^{(m)}_1,x^{(m)}_2,...,x^{(m)}_n, y_m)(x1(1)​,x2(1)​,...,xn(1)​,y1​),(x1(2)​,x2(2)​,...,xn(2)​,y2​),...,(x1(m)​,x2(m)​,...,xn(m)​,ym​) Now question is if we get a new data with only n features: $ (x{(a)}_1,x{(a)}_2,...,x^{(a)}_n) $, how can we predict its result value $ y_a $? And what is it? If $ y_a $ is the continuous value, it is a regression question. And if $ y_a $ is the discrete value, it is a classification question. 2. Linear Regression Model If we decide to solve this question with linear regression, then we assume the data model is in this form: $ h_\\theta(x_0,x_1,..,x_n) = \\theta_0 + \\theta_1x_1 + ... + \\theta_nx_n $ θi\\theta_iθi​ (i=0,1,...,n) is the coefficient of each feature, which is also the parameter of the model we need to estimate. If we assume x0=1x_0=1x0​=1, then model can be wrote in a simpler way: $ h_\\theta(x_0,x_1,..,x_n) = \\sum_{i=0}^n \\theta_ix_i $ If we use matrix representation, Model will become more simple and elegant: $ h_\\theta(X) = X\\theta $ X is a m×nm \\times nm×n matrix (m records and n features). θ\\thetaθ is a n×1n \\times 1n×1 vector (n coefficients of n features). hθ(X)h_\\theta(X)hθ​(X) is a m×1m\\times 1m×1 vector (m prediction y value of m sample records) Once we have determined the model prototype, we need to calculate the Loss Function. Generally, we use Mean Square Error as the loss function for linear regression model. The algebratic representation of the loss function is as follows: J(θ0,θ1,...,θn)=∑i=1m(hθ(x0(i),x1(i),..,xn(i))−yi)2J(\\theta_0,\\theta_1,...,\\theta_n) = \\sum_{i=1}^m (h_\\theta(x_0^{(i)},x_1^{(i)},..,x_n^{(i)}) - y_i)^2 J(θ0​,θ1​,...,θn​)=i=1∑m​(hθ​(x0(i)​,x1(i)​,..,xn(i)​)−yi​)2 In Matix Representation: J(θ)=12(Xθ−Y)T(Xθ−Y)J(\\theta) = \\frac{1}{2} (X\\theta - Y)^T(X\\theta - Y) J(θ)=21​(Xθ−Y)T(Xθ−Y) Y is a m×1m\\times 1m×1 vector (m actual y value of m sample records) 3. Linear Regression Algorithm Once the loss function is known, our goal is to find out the parameter θ\\thetaθ that could minimize the value of loss function. There are two methods: Gradient Descent method Least Square method If we choose Gradient Descent method, the iteration formula of θ\\thetaθ is: θ=θ−αXT(Xθ−Y)\\theta = \\theta - \\alpha X^T(X\\theta-Y) θ=θ−αXT(Xθ−Y) If we choose Least Square method, the formula of θ\\thetaθ is: θ=(XTX)−1XTY\\theta = (X^TX)^{-1}X^TY θ=(XTX)−1XTY 4.1. Generalization of linear regression: Polynomial Regression If the model prototype exists not only to the first power, but also to the second power, n-th power, the model becomes Polynomial Regression. For example, if the sample data has 2 features: (x1(1),x2(1),y1),(x1(2),x2(2),y2),...,(x1(m),x2(m),ym)(x_1^{(1)}, x_2^{(1)}, y_1), (x_1^{(2)}, x_2^{(2)}, y_2),...,(x_1^{(m)}, x_2^{(m)}, y_m) (x1(1)​,x2(1)​,y1​),(x1(2)​,x2(2)​,y2​),...,(x1(m)​,x2(m)​,ym​) Assume the model is in this form: hθ(x1,x2)=θ0+θ1x1+θ2x2+θ3x12+θ4x22+θ5x1x2h_\\theta(x_1,x_2) = \\theta_0+\\theta_1x_1+\\theta_2x_2 + \\theta_3x_1^2 + \\theta_4x_2^2 + \\theta_5x_1x_2 hθ​(x1​,x2​)=θ0​+θ1​x1​+θ2​x2​+θ3​x12​+θ4​x22​+θ5​x1​x2​ If we define: x0=1,x1=x1,x2=x2,x3=x12,x4=x22,x5=x1x2x_0=1, x_1=x_1, x_2=x_2, x_3=x_1^2, x_4=x_2^2, x_5=x_1x_2x0​=1,x1​=x1​,x2​=x2​,x3​=x12​,x4​=x22​,x5​=x1​x2​, then the Polynomial model is back to linear regression: hθ(x1,x2)=θ0+θ1x1+θ2x2+θ3x3+θ4x4+θ5x5h_\\theta(x_1,x_2) = \\theta_0+\\theta_1x_1+\\theta_2x_2 + \\theta_3x_3 + \\theta_4x_4 + \\theta_5x_5 hθ​(x1​,x2​)=θ0​+θ1​x1​+θ2​x2​+θ3​x3​+θ4​x4​+θ5​x5​ Therefore, the solution is to build a 5 features sample data (x1,x2,x12,x22,x1x2)(x_1, x_2, x_1^2, x_2^2, x_1x_2)(x1​,x2​,x12​,x22​,x1​x2​) for the 2 features sample data (x1,x2)(x_1,x_2)(x1​,x2​). Then we use this 5 features data to train the linear regression model. 4.2. Generalization of linear regression: Generalized Linear Regression In 4.1 section, we generized the feature side of the sample data. Now we try to generize the y value of the sample data. For example, if the Y does not have a linear relationship X, but ln(Y) does: ln(Y)=Xθln(Y) = X\\theta ln(Y)=Xθ In this case, use ln(y) instead of y, we can still deal with the problem with linear regression model. We generalize In(y), assuming this function is a monotonically differentiable function 𝐠 (.), Then the generalized generalized linear regression form is: g(Y)=Xθg(Y) = X\\theta g(Y)=Xθ 5. Regularization of linear regression In order to preventing overfitting of the model, we often add regularization terms when building the linear model. There are generally L1 regularization and L2 regularization. 5.1 L1 regularization L1 regularization of linear regression is usually called Lasso Regression. The difference between it and general linear regression is that an L1 regularization term is added to the loss function. The L1 regularized term has a constant coefficient α\\alphaα to adjust the weight of mean square error term and the regularized term of the loss function: J(θ)=12(Xθ−Y)T(Xθ−Y)+α∣∣θ∣∣1J(\\theta) = \\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y) + \\alpha||\\theta||_1 J(θ)=21​(Xθ−Y)T(Xθ−Y)+α∣∣θ∣∣1​ α\\alphaα is a constant coefficient and needs to be tuned. ∣∣θ∣∣1||\\theta||_1∣∣θ∣∣1​ is the L1 norm. Lasso regression can make the coefficients of some features smaller, or even some coefficients with smaller absolute values directly become 0. Enhance the generalization ability of the model. The solution methods of Lasso regression are generally: Coordinate Descent Least Angle Regression Please check Regularization of Regression-Summary of Lasso Regression 5.2 L2 regularization L2 regularization of linear regression is usually called Ridge Regression. It adds an L2 regularization term to the loss function: J(θ)=12(Xθ−Y)T(Xθ−Y)+12α∣∣θ∣∣22J(\\theta) = \\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y) + \\frac{1}{2}\\alpha||\\theta||_2^2 J(θ)=21​(Xθ−Y)T(Xθ−Y)+21​α∣∣θ∣∣22​ α\\alphaα is a constant coefficient and needs to be tuned. ∣∣θ∣∣2||\\theta||_2∣∣θ∣∣2​ is the L2 norm. (not equal to L1 norm) Ridge regression reduces the regression coefficient without abandoning any feature, making the model relatively stable, but compared with Lasso regression, this will leave the model with a lot of features and poor interpretability. The solution of Ridge regression is relatively simple, and the least square method is generally used. Here is a matrix derivation form using least squares, which is similar to ordinary linear regression. Let the derivative of J(θ)J(\\theta)J(θ) be 0 and get the following formula: XT(Xθ−Y)+αθ=0X^T(X\\theta - Y) + \\alpha\\theta = 0 XT(Xθ−Y)+αθ=0 Then: θ=(XTX+αE)−1XTY\\theta = (X^TX + \\alpha E)^{-1} X^TY θ=(XTX+αE)−1XTY E is the Identity matrix ","link":"https://zl-wu.github.io/post/mla-linear-regression-principle/"},{"title":"MLA -- Least Square method","content":"&quot;A standard approach in regression analysis to approximate the solution of overdetermined systems.&quot; The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals made in the results of every single equation. It is used to do model fitting and find the extreme value of the loss function. 1. Least Square principle The general form of least squares is simple: f=∑(y−y^)2f = \\sum (y - \\widehat{y})^2 f=∑(y−y​)2 f is the objective function y is the observation value (actual value) y^\\widehat{y}y​ is the theoretical value predicted from the model The objective function is also the loss function commonly used in the machine learning. Our goal is to get a fitting model when the objective function is minimized. Give a simple example of the simplest linear regression, we have m samples with only one feature: (x1,y1)(x2,y2),...,(xm,ym)(x_1,y_1)(x_2,y_2),...,(x_m,y_m)(x1​,y1​)(x2​,y2​),...,(xm​,ym​) Hypothesis model is: hθ(x)=θ0+θ1xh_\\theta(x) = \\theta_0 + \\theta_1x hθ​(x)=θ0​+θ1​x Loss function is: J(θ0,θ1)=∑i=1m(yi−hθ(xi))2=∑i=1m(yi−θ0−θ1xi)2\\begin{aligned} J(\\theta_0,\\theta_1) &amp;= \\sum_{i=1}^{m}(y_i - h_\\theta(x_i))^2 \\\\ &amp;= \\sum_{i=1}^{m}(y_i - \\theta_0 - \\theta_1x_i)^2 \\end{aligned} J(θ0​,θ1​)​=i=1∑m​(yi​−hθ​(xi​))2=i=1∑m​(yi​−θ0​−θ1​xi​)2​ Least Square method needs to get (θ0,θ1)(\\theta_0,\\theta_1)(θ0​,θ1​) that minimizes the value of J(θ0,θ1)J(\\theta_0,\\theta_1)J(θ0​,θ1​) 2. Least Squre solution 2.1 Algebraic way If we need to minimumize J(θ0,θ1)J(\\theta_0,\\theta_1)J(θ0​,θ1​), our method is to calculate the partial derivatives of θ0\\theta_0θ0​ and θ1\\theta_1θ1​ respectively, and make their partial derivatives all 0 to form a linear equation set for two variables. Partial derivation of J(θ0,θ1)J(\\theta_0,\\theta_1)J(θ0​,θ1​) to θ0\\theta_0θ0​: −2∑i=1m(yi−θ0−θ1xi)=0-2\\sum_{i=1}^{m}(y_i-\\theta_0-\\theta_1x_i) = 0 −2i=1∑m​(yi​−θ0​−θ1​xi​)=0 Partial derivation of J(θ0,θ1)J(\\theta_0,\\theta_1)J(θ0​,θ1​) to θ1\\theta_1θ1​: −2∑i=1m(yi−θ0−θ1xi)xi=0-2\\sum_{i=1}^{m}(y_i-\\theta_0-\\theta_1x_i)x_i = 0 −2i=1∑m​(yi​−θ0​−θ1​xi​)xi​=0 Now: {∑i=1m(yi−θ0−θ1xi)=0∑i=1m(yi−θ0−θ1xi)xi=0\\begin{cases} \\sum_{i=1}^{m}(y_i-\\theta_0-\\theta_1x_i) = 0\\\\ \\sum_{i=1}^{m}(y_i-\\theta_0-\\theta_1x_i)x_i = 0\\\\ \\end{cases} {∑i=1m​(yi​−θ0​−θ1​xi​)=0∑i=1m​(yi​−θ0​−θ1​xi​)xi​=0​ ={∑i=1myi−mθ0−θ1∑i=1mxi=0∑i=1myixi−θ0∑i=1mxi−θ1∑i=1mxi2=0=\\begin{cases} \\sum_{i=1}^{m}y_i - m\\theta_0-\\theta_1\\sum_{i=1}^{m}x_i = 0\\\\ \\sum_{i=1}^{m}y_ix_i -\\theta_0\\sum_{i=1}^{m}x_i-\\theta_1\\sum_{i=1}^{m}x_i^2 = 0\\\\ \\end{cases} ={∑i=1m​yi​−mθ0​−θ1​∑i=1m​xi​=0∑i=1m​yi​xi​−θ0​∑i=1m​xi​−θ1​∑i=1m​xi2​=0​ Then: θ0=∑xi2×∑yi−∑xi×∑xiyim∑xi2−(∑xi)2 \\theta_0 = \\frac {\\sum x_i^2 \\times \\sum y_i - \\sum x_i \\times \\sum x_iy_i} {m\\sum x_i^2-(\\sum x_i)^2} θ0​=m∑xi2​−(∑xi​)2∑xi2​×∑yi​−∑xi​×∑xi​yi​​ θ1=m∑xiyi−∑xi×∑yim∑xi2−(∑xi)2 \\theta_1 = \\frac { m\\sum x_iy_i - \\sum x_i \\times \\sum y_i} {m\\sum x_i^2-(\\sum x_i)^2} θ1​=m∑xi2​−(∑xi​)2m∑xi​yi​−∑xi​×∑yi​​ If the sample has more than 1 features, which is Multiple Linear Regression. We still use the rule that partial derivatives equal to 0 to form parametric equation set, then calculate these unknown parameters. The principle remains the same. 2.2 Matrix way The matrix expression is more concise than the algebra expression, and it can also replace loops (In essence, the nature of computing has not changed). So many books and machine learning libraries now use the matrix method to do the least square. Here we use the above multiple linear regression example to describe the matrix method solution. hθ(x1,x2,...,xn)=θ0+θ1x1+...+θnxnh_\\theta(x_1,x_2,...,x_n)=\\theta_0+\\theta_1x_1+...+\\theta_nx_nhθ​(x1​,x2​,...,xn​)=θ0​+θ1​x1​+...+θn​xn​. If in matrix expression: hθ(X)=Xθh_\\theta(X) = X\\theta hθ​(X)=Xθ X is a m×nm \\times nm×n matrix (m records and n features). θ\\thetaθ is a n×1n \\times 1n×1 vector (n coefficients of n features). hθ(X)h_\\theta(X)hθ​(X) is a m×1m\\times 1m×1 vector (m prediction y value of m sample records) The loss function is: J(θ)=12(Xθ−Y)T(Xθ−Y)J(\\theta) = \\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y) J(θ)=21​(Xθ−Y)T(Xθ−Y) Y is a m×1m\\times 1m×1 vector (m actual y value of m sample records) 1/2 is mainly to facilitate the calculation of later derivatives. Let the derivative to be 0: ∂∂θJ(θ)=XT(Xθ−Y)=0\\frac{\\partial}{\\partial\\theta}J(\\theta) = X^T(X\\theta-Y) = 0 ∂θ∂​J(θ)=XT(Xθ−Y)=0 It based on two Matrix derivative chain rule formula: Formula 1: ∂∂X(XTX)=2X\\frac{\\partial}{\\partial X}(X^TX) = 2X∂X∂​(XTX)=2X Formula 2: $ \\nabla_xf(AX+B) = A^T $ Then: XTXθ=XTYθ=(XTX)−1XTY\\begin{array}{cc} X^TX\\theta = X^TY\\\\\\\\ \\theta = (X^TX)^{-1}X^TY \\end{array} XTXθ=XTYθ=(XTX)−1XTY​ (XTX)−1XTY(X^TX)^{-1}X^TY(XTX)−1XTY is our answer. 3. Limitations and applicable scenarios of least square method Least Square answer is simple and efficient, it has no iterations camparing with gradient descent. But it still has some limitations when we use it: Least Square needs to calculate the inverse matrix of XTXX^TXXTX, however, it is possible that inverse matrix does not exist. In this case, there is no way to use Least Square directly, but Gradient Descent. Or, we can also try to remove redundant features by clean the sample data. Let the determinant of XTXX^TXXTX not be 0, and then continue to use least squares. When the features n of sample is very large, the calculating the inverse matrix of XTXX^TXXTX is a very time-consuming work, even not feasible. In this case, it's better to use Gradient Descent. (If n &gt; 1000, suggest not use Least Square). Or, we can also use Principal Component Analysis (PCA) to reduce the dimension of features and then use the least square method. If the fitting function (objective model) is not linear, Least Square cannot be used. But Gradient Descent can still be used. Or, we can convert the fitting function to linearity through some techniques before Least Square can be used. There are some special situation: When the sample size m is small and less than the feature number n, then the fitting equation is underdetermined, and the commonly used optimization methods cannot fit the data. When the sample size m is equal to the feature number n, it can be solved by the equation system. When the sample size m is greater than n, the fitting equation is overdetermined, which is the scenario where we usually use least squares. ","link":"https://zl-wu.github.io/post/mla-least-square-method/"},{"title":"MLA -- Gradient Descent method","content":"When improving the model parameters of machine learning algorithms, that is, unconstrained optimalization problem. Gradient Descent method is one of most common used methods. The other one is the least square method. Here is a complete summary of the gradient descent method. 1. What is gradient? In calculus, Gradient is to find the partial derivatives of the parameters of the multivariate function, and write all partial derivative of the obtained parameters in the form of one vector. For the function f(x, y) as an example, finding the partial derivatives of x and y, then the gradient vector is $ (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})^T $, AKA grad f(x, y) or $ \\nabla f(x,y)$ The specific gradient vector value at point (x0, y0) is $ (\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial y_0})^T $ or $ \\nabla f(x_0,y_0)$ If there are three parameters, then gradient vector is $ (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z})^T $, and so on. In geometrically speaking, the meaning of gradient is the direction where the function f(x,y) changes fastest. At point (x0, y0), the direction of gradient vector $(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial y_0})^T $ is where f(x,y) changes fastest. If we keep following the gradient vector direction, we will reach the maximum or minimum point of function faster and easier. 2. Gradient Descent Assume we are somewhere on a large mountain, because we don’t know how to go down the mountain, so we decided to take one step at a time. That is, we can take a step down from the current position's steepest direction, and then continue to calculate the gradient of the new position and take a step toward the position where the step is located at the steepest and easiest to descend. Going on like this step by step until we feel that we have reached the foot of the mountain. Of course, if we go on like this, we may not be able to reach the foot of the mountain, but to a certain foot in a local part of the mountain. As can be seen from the above explanation, gradient descent may not necessarily find the global optimal solution, but may be a local optimal solution. Of course, if the loss function is convex, the solution obtained by the gradient descent method must be the global optimal solution. 3. Concepts of gradient descent Learining Rate: The step size determines the length of each step in the negative direction of the gradient during the gradient descent iteration. Feature: Refers to the input part of the sample data. (x0(1),x1(1),...,xn(1),y1)(x_0^{(1)},x_1^{(1)},...,x_n^{(1)},y_1)(x0(1)​,x1(1)​,...,xn(1)​,y1​) Hypothesis Function: In supervised learning, the hypothesis function used to fit the input samples, denoted as hθ(X)h_\\theta(X)hθ​(X). For example, the linear regression hypothesis model is hθ(x1,x2,...,xn)=θ0+θ1x1+θ2x2+...+θnxnh_\\theta(x_1,x_2,...,x_n) = \\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_nhθ​(x1​,x2​,...,xn​)=θ0​+θ1​x1​+θ2​x2​+...+θn​xn​ Loss Function: In order to evaluate how well the model fits the data, a loss function is usually used to measure the degree of fit. The minimization of the loss function means that the degree of fitting is the best and the corresponding model parameters are the optimal parameters. In linear regression model, the loss function usually squares the difference between the sample output (yi) and the hypothesis function output (y hat). If there is a linear regression model for m samples: (x1i,x2i,...,xni,yi),i=(1,2,...,m)(x_1^{i},x_2^{i},...,x_n^{i},y_i),i=(1,2,...,m)(x1i​,x2i​,...,xni​,yi​),i=(1,2,...,m), the loss function is: J(θ0,θ1,...,θn)=∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)2J(\\theta_0,\\theta_1,...,\\theta_n) = \\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)^2 J(θ0​,θ1​,...,θn​)=i=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)2 x(i)x^{(i)}x(i) is the feature of the i-th sample data. yiy_iyi​ is the output of the i-th sample data. hθ(x1(i),x2(i),...,xn(i))h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})hθ​(x1(i)​,x2(i)​,...,xn(i)​) is the Hypothesis Function. (== y^\\widehat{y}y​) 4. Gradient Descent Algorithm The algorithm of gradient descent method can have two expressions: algebraic method and matrix method (also called vector method). 4.1 Algebraic Expression 1. Prerequite: Set Hypothesis Function and Loss Function of the optimization model. Assume Hypothesis Function is the linear regression model. Then the loss function is: J(θ0,θ1,...,θn)=12m∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)2J(\\theta_0,\\theta_1,...,\\theta_n) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)^2 J(θ0​,θ1​,...,θn​)=2m1​i=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)2 1/2m is a new factor we added. 1/m means average. 1/2 is mainly to facilitate the calculation of later partial derivatives. We know that m is a constant greater than zero, so it has no effect on the nature of the function gradient. It does not change the θ\\thetaθ when we get the minimum value of J(θ)J(\\theta)J(θ). After all, what we need is only the θ\\thetaθ, not the minimum value of J(θ)J(\\theta)J(θ). 2. Initialize some algorithm parameters: Initialize all (θ0,θ1,...,θn)(\\theta_0,\\theta_1,...,\\theta_n)(θ0​,θ1​,...,θn​). Learning Rate α\\alphaα. (Step Size) Algorithm termination distance ε\\varepsilonε. If the distance of gradient descent is less than ε\\varepsilonε, algorithm stoppes. For example, in the absence of any prior knowledge, we can simply intialize all θ\\thetaθ to 0 and α\\alphaα to 1. Then Optimize them again when tuning is needed. 3. Algorithm Process: Step 1: Calculate the gradient of current position. For θj\\theta_jθj​, its gradient expression is:∂∂θjJ(θ0,θ1,...,θn)\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n) ∂θj​∂​J(θ0​,θ1​,...,θn​) j is (0,1,2,...,n), represents the j-th parameter. Step 2: Multiply Gradient by Learning Rate α\\alphaα (step size) to get the distance of gradient descent at current position. That is, α∂∂θjJ(θ0,θ1,...,θn)\\alpha\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n)α∂θj​∂​J(θ0​,θ1​,...,θn​) Step 3: If all θj\\theta_jθj​'s distance of gradient descent is less than ε\\varepsilonε, algorithm stopped. And current (θ0,θ1,...,θn)(\\theta_0,\\theta_1,...,\\theta_n)(θ0​,θ1​,...,θn​) is returned as the final result. Step 4: Update all θj\\theta_jθj​ and jump to Step 1 for a new loop. The update expression is as follows: θj=θj−α∂∂θjJ(θ0,θ1,...,θn)\\theta_j = \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n) θj​=θj​−α∂θj​∂​J(θ0​,θ1​,...,θn​) The algorithm is very simple and elegant, the key point is how to calculate ∂∂θjJ(θ0,θ1,...,θn)\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n)∂θj​∂​J(θ0​,θ1​,...,θn​): J(θ0,θ1,...,θn)=12m∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)2J(\\theta_0,\\theta_1,...,\\theta_n) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)^2 J(θ0​,θ1​,...,θn​)=2m1​i=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)2 ∂∂θjJ(θ0,θ1,...,θn)=1m∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n) = \\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} ∂θj​∂​J(θ0​,θ1​,...,θn​)=m1​i=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ θj=θj−α∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\theta_j = \\theta_j - \\alpha\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} θj​=θj​−αi=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ 4.2 Matrix Expression Algebraic Expression and Matrix Expression are essentially the same, but the representation of Matrix will be more concise. Suppose we have m sample data: (x1(1),x2(1),...,xn(1),y1),(x1(2),x2(2),...,xn(2),y2),...,(x1(m),x2(m),...,xn(m),ym)(x_1^{(1)},x_2^{(1)},...,x_n^{(1)},y_1),(x_1^{(2)},x_2^{(2)},...,x_n^{(2)},y_2),...,(x_1^{(m)},x_2^{(m)},...,x_n^{(m)},y_m)(x1(1)​,x2(1)​,...,xn(1)​,y1​),(x1(2)​,x2(2)​,...,xn(2)​,y2​),...,(x1(m)​,x2(m)​,...,xn(m)​,ym​) 1. Prerequite: Set Hypothesis Function and Loss Function of the optimization model. Assume Hypothesis Function is the linear regression model: hθ(X)=Xθh_\\theta(X) = X\\theta hθ​(X)=Xθ J(θ)=12(Xθ−Y)T(Xθ−Y)J(\\theta) = \\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y) J(θ)=21​(Xθ−Y)T(Xθ−Y) X is a m×nm \\times nm×n matrix (m records and n features). θ\\thetaθ is a n×1n \\times 1n×1 vector (n coefficients of n features). hθ(X)h_\\theta(X)hθ​(X) is a m×1m\\times 1m×1 vector (m prediction y value of m sample records) Y is a m×1m\\times 1m×1 vector (m actual y value of m sample records) 2. Initialize some algorithm parameters: Same as 4.1 section. 3. Algorithm Process: Same as 4.1 section. ∂∂θJ(θ)=XT(Xθ−Y)\\frac{\\partial}{\\partial\\theta}J(\\theta) = X^T(X\\theta-Y) ∂θ∂​J(θ)=XT(Xθ−Y) θ=θ−α∂∂θJ(θ)=θ−αXT(Xθ−Y)\\begin{aligned} \\theta &amp;= \\theta - \\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta)\\\\ &amp;= \\theta - \\alpha X^T(X\\theta-Y) \\end{aligned} θ​=θ−α∂θ∂​J(θ)=θ−αXT(Xθ−Y)​ Matrix derivative chain rule: Formula 1: ∂∂X(XTX)=2X\\frac{\\partial}{\\partial X}(X^TX) = 2X∂X∂​(XTX)=2X Formula 2: $ \\nabla_xf(AX+B) = A^T $ 5. Tuning of gradient descent algorithm Learning Rate: If the step size is too large, the iteration will be too fast, and it may even miss the optimal solution. The step size is too small, the iteration speed is too slow, and the algorithm cannot end for a long time. So the step size of the algorithm needs to be run multiple times to get a better value. θ\\thetaθ initialization: different intial parameter value may lead to different minimum value. Hence, the gradient descent only finds the local minimum. if the loss function is a convex function, it must be the global optimal solution. It is necessary to run the algorithm with different initial values multiple times, which can increase the possibility that we skip the local minimum to reach the global minimum. Normalized: The data range of different features is different, if some features' data value are very high, and some others' are very low, it may lead to a very slow iteration. In order to reduce the influence of the feature value, the feature data can be normalized. For each xj feature's data, we transform it to:xj=xj−xj‾std(xj)x_j = \\frac{x_j - \\overline{x_j}}{std(x_j)} xj​=std(xj​)xj​−xj​​​ xjx_jxj​ becomes a new feature with new expectation 0 and new variance 1, the iteration speed can be greatly accelerated. 6. Gradient Descent Family (BGD, SGD, MBGD) 6.1 BGD (Batch Gradient Descent) Batch Gradient Descent is the most commonly used form. It always uses all m sample data when we update parameter θ\\thetaθ once. As discuss above: θj=θj−α∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\theta_j = \\theta_j - \\alpha\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} θj​=θj​−αi=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ For each iteration of one of θ\\thetaθ, we use all m samples to caculate gradient. 6.2 SGD (Stochastic Gradient Descent) Stochastic Gradient Descent is similar to the BGD, the difference is that SGD use only one random sample to calculate gradient: θj=θj−α(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\theta_j = \\theta_j - \\alpha(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} θj​=θj​−α(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ BGD and SGD are two extremes. One uses all data for gradient descent, and one uses only one sample data for gradient descent. Advantages and disadvantages are very prominent. Traning Speed: SGD is very fast, since it uses only one sample to iterate each time. While the BGD cannot satisfy the training speed when the sample size is large. Accuracy: SGD uses only one sample, most likely resulting in a not optimal solution. convergence Speed: SGD uses one sample at a interation, the iteration direction changes greatly, and it cannot quickly converge to the local optimal solution. Is there a moderate method to neutralize the advantages and disadvantages of BGD and SGD? Yes, MBGD 6.3 MBGD (Mini-batch Gradient Descent) The Mini-batch Gradient Descent is a compromise between the BGD and the SGD, that is, for all m samples, we use k samples to iterate, 0&lt;k&lt;m0&lt;k&lt;m0&lt;k&lt;m. Of course, the value of k can be adjusted according to the sample data. The iteration formula is: θj=θj−α∑i=tt+k−1(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\theta_j = \\theta_j - \\alpha\\sum_{i=t}^{t+k-1}(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} θj​=θj​−αi=t∑t+k−1​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ 7. Comparison of gradient descent method and other optimization algorithms In machine learning, there are generally these optimization algorithms: Gradient Descent Least Square Newton's method and Quasi-Newton's method Gradient Descent and Lease Square: Gradient Descent needs step size, Lease Square doesn't. Gradient Descent is an iterative solution, and the Least Square is an analytical solution. If the sample size is not very large, and there is an analytical solution, the Least Square has an advantage over the Gradient Descent, and the LS calculation speed is fast. However, if the sample size is large, Least Square requires a super large inverse matrix, which is difficult or slow to solve the analytical solution. Then iterative Gradient Descent is better. Gradient Descent and Newton's method/Quasi-Newton's method: Both of them are iterative solution. Gradient Descent is gradient solution. Newton's method is solved by using the inverse or pseudo-inverse matrix of the second-order Hessian matrix. Newton method / Quasi-Newton's method converges faster. But each iteration takes longer than gradient descent Python implementation Example f(x)=x2+1f(x)=x^2+1f(x)=x2+1 &quot;&quot;&quot; One Dimensional Gradient Descent Example -- One feature &quot;&quot;&quot; def func_1d(x): &quot;&quot;&quot; Objective Function :param x: independent variable, scalar :return: dependent variable, scalar &quot;&quot;&quot; return x ** 2 + 1 def grad_1d(x): &quot;&quot;&quot; Gradient of Objective Function :param x: independent variable, scalar :return: dependent variable, scalar &quot;&quot;&quot; return x * 2 def gradient_descent_1d(grad, cur_x=0.1, learning_rate=0.01, precision=0.0001, max_iters=10000): &quot;&quot;&quot; One-dimensional gradient descent algorithm :param grad: Gradient Fuction of Objective Function :param cur_x: Current x value :param learning_rate: step size :param precision: Set convergence accuracy (stop condition) :param max_iters: max number of iterations :return: local minumum x* &quot;&quot;&quot; for i in range(max_iters): grad_cur = grad(cur_x) if abs(grad_cur) &lt; precision: # when the gradient approaches 0, it is regarded as convergence break cur_x = cur_x - grad_cur * learning_rate print(&quot;The&quot;, i, &quot;-th iteration's x value: &quot;, cur_x) print(&quot;Local Minumum x =&quot;, cur_x) return cur_x if __name__ == '__main__': gradient_descent_1d(grad_1d, cur_x=10, learning_rate=0.2, precision=0.000001, max_iters=10000) f(x,y)=−e−(x2+y2)f(x,y) = -e^{-(x^2+y^2)}f(x,y)=−e−(x2+y2) &quot;&quot;&quot; 2-Dimensional Gradient Descent example -- 2 features &quot;&quot;&quot; import math import numpy as np def func_2d(x): &quot;&quot;&quot; Objective Function :param x: independent variable, 2d vector :return: dependent variable, scalar &quot;&quot;&quot; return - math.exp(-(x[0] ** 2 + x[1] ** 2)) def grad_2d(x): &quot;&quot;&quot; Gradient of the Objective Function :param x: independent variable, 2d vector :return: dependent variable, 2d vector &quot;&quot;&quot; deriv0 = 2 * x[0] * math.exp(-(x[0] ** 2 + x[1] ** 2)) deriv1 = 2 * x[1] * math.exp(-(x[0] ** 2 + x[1] ** 2)) return np.array([deriv0, deriv1]) def gradient_descent_2d(grad, cur_x=np.array([0.1, 0.1]), learning_rate=0.01, precision=0.0001, max_iters=10000): &quot;&quot;&quot; 2D gradient descent algorithm :param grad: Gradient Fuction of Objective Function :param cur_x: Current x value (Parameter Initialization) :param learning_rate: step size :param precision: Set convergence accuracy (stop condition) :param max_iters: max number of iterations :return: Local minimum x* &quot;&quot;&quot; print(f&quot;{cur_x} as the Initialization...&quot;) for i in range(max_iters): grad_cur = grad(cur_x) if np.linalg.norm(grad_cur, ord=2) &lt; precision: # when the gradient approaches 0, it is regarded as convergence break cur_x = cur_x - grad_cur * learning_rate print(&quot;The&quot;, i, &quot;-th iteration's x value: &quot;, cur_x) print(&quot;Local minimum x =&quot;, cur_x) return cur_x if __name__ == '__main__': gradient_descent_2d(grad_2d, cur_x=np.array([1, -1]), learning_rate=0.2, precision=0.000001, max_iters=10000) f(x1,x2,x3)=3x1+4x2+5x3+6f(x_1,x_2,x_3) = 3x_1+4x_2+5x_3+6f(x1​,x2​,x3​)=3x1​+4x2​+5x3​+6 &quot;&quot;&quot; Parameter Estimation Multiple linear regression gradient descent example (Matrix Method) Dataset(4 points on the function): x1, x2, x3, b, y 1, 1, 2, 1, 23 2, 4, 3, 1, 43 3, 2, 4, 1, 43 4, 3, 1, 1, 35 &quot;&quot;&quot; import numpy as np def gradient_descent(theta, X, Y, lr=0.01, precision=0.0001, max_iters=10000): &quot;&quot;&quot; matrix gradient descent algorithm :param theta: parameter estimation of the objective function :param X: sample data, independent data, mxn matrix :param Y: sample data, dependent data, mx1 matrix :param lr: step size :param precision: Set convergence accuracy (stop condition) :param max_iters: max number of iterations :return: Local minimum theta* &quot;&quot;&quot; print(f&quot;{theta} as the Initialization...&quot;) for i in range(max_iters): distance = lr*X.T*(X*theta-Y) if np.linalg.norm(distance, ord=2) &lt; precision: # when the gradient approaches 0, it is regarded as convergence print(&quot;total iterations:&quot;, i) break theta = theta-distance print(&quot;Local minimum theta =\\n&quot;, theta) return theta x = np.mat([[1,1,2,1],[2,4,3,1],[3,2,4,1],[4,3,1,1]]) y = np.mat([[23],[43],[43],[35]]) theta = np.mat([[0],[0],[0],[0]]) gradient_descent(theta, x, y) ##### [[0] [0] [0] [0]] as the Initialization... total iterations: 1852 Local minimum theta = [[3.00521152] [4.00294095] [5.00623515] [5.96159285]] &quot;&quot;&quot; Additional code -- np.linalg.norm linalg = linear algebra &quot;&quot;&quot; x_norm=np.linalg.norm(x, ord=None, axis=None, keepdims=False) import numpy as np x = np.array([ [0, 3, 4], [1, 6, 4]]) # np.sqrt(0+9+16+1+36+16) print(&quot;Default(sqrt of square sum of all elements, does not return matrix), keepdims=False:\\n&quot;,np.linalg.norm(x)) print(&quot;Sqrt of square sum of all elements, return matrix, keepdims=True:\\n&quot;,np.linalg.norm(x,keepdims=True)) print(&quot;***********************&quot;) # np.sqrt(0+9+16), np.sqrt(1+36+16) print(&quot;Matrix's row vector, l2 norm, axis=1:\\n&quot;,np.linalg.norm(x,axis=1,keepdims=True)) # np.sqrt(0+1), np.sqrt(9+36), np.sqrt(16+16) print(&quot;Matrix's column vector, l2 norm, axis=0:\\n&quot;,np.linalg.norm(x,axis=0,keepdims=True)) print(&quot;***********************&quot;) # max(0+1, 3+6, 4+4) print(&quot;Matrix's l1 norm, ord=1:&quot;,np.linalg.norm(x,ord=1,keepdims=True)) print(&quot;Matrix's l2 norm, ord=2:&quot;,np.linalg.norm(x,ord=2,keepdims=True)) # max(0+3+4, 1+6+4) print(&quot;Matrix's ∞ norm, ord=np.inf:&quot;,np.linalg.norm(x,ord=np.inf,keepdims=True)) print(&quot;***********************&quot;) print(&quot;Matrix's row vector, l1 norm, ord=1, axis=1:\\n&quot;,np.linalg.norm(x,ord=1,axis=1,keepdims=True)) print(&quot;Matrix's row vector, l1 norm, ord=1, axis=1, keepdims=False:\\n&quot;,np.linalg.norm(x,ord=1,axis=1,keepdims=False)) ######### Output ######### &quot;&quot;&quot; Default(sqrt of square sum of all elements, does not return matrix), keepdims=False: 8.831760866327848 Sqrt of square sum of all elements, return matrix, keepdims=True: [[8.83176087]] *********************** Matrix's row vector, l2 norm, axis=1: [[5. ] [7.28010989]] Matrix's column vector, l2 norm, axis=0: [[1. 6.70820393 5.65685425]] *********************** Matrix's l1 norm, ord=1: [[9.]] Matrix's l2 norm, ord=2: [[8.70457079]] Matrix's ∞ norm, ord=np.inf: [[11.]] *********************** Matrix's row vector, l1 norm, ord=1, axis=1: [[ 7.] [11.]] Matrix's row vector, l1 norm, ord=1, axis=1, keepdims=False: [ 7. 11.] &quot;&quot;&quot; parameter description computation ord=1 l1l_1l1​ norm |x1x_1x1​|+|x2x_2x2​|+...+|xnx_nxn​| Default l2l_2l2​ norm x12+x22+...+xn2\\sqrt{x_1^2+x_2^2+...+x_n^2}x12​+x22​+...+xn2​​ ord=∞ l∞l_∞l∞​ norm max( |xix_ixi​| ) ord=1: Maximum of each column's sum ord=∞: Maximum of each row's sum ord=2: Find the eigenvalue, then find the maximum square of the eigenvalue. ans = np.mat(x).T*np.mat(x) [x,y] = np.linalg.eig(ans)) max(np.sqrt(x)) axis: process rule axis=0: processing by column vectors and finds the norm of multiple column vectors axis=1: processing by row vectors and finds the norm of multiple row vectors axis=None: Matrix's norm keepdims: whether to maintain the two-dimensional characteristics of the matrix ","link":"https://zl-wu.github.io/post/mla-gradient-descent-method/"},{"title":"MLA -- Naive Bayes theory","content":"&quot;Priori Probability + data = Posterior probability&quot; 条件概率： P(X|Y) or P(Y|X) 联合概率： P(X,Y), P(XY) or P(XnY) 边缘概率： P(X) or P(Y) Let's look at the conditional independence formula first. If X and Y are independent of each other, there are: P(X,Y)=P(X)×P(Y)P(X, Y) = P(X) \\times P(Y) P(X,Y)=P(X)×P(Y) P(X∣Y)=P(X);P(Y∣X)=P(Y);P(X|Y) = P(X) ; P(Y|X) = P(Y) ; P(X∣Y)=P(X);P(Y∣X)=P(Y); Usually P(X|Y) != P(X) unless X and Y are independent of each other. And standard conditional probability formula: P(X∣Y)=P(X,Y)/P(Y)P(X|Y) = P(X, Y) / P(Y) P(X∣Y)=P(X,Y)/P(Y) P(Y∣X)=P(X,Y)/P(X)P(Y|X) = P(X, Y) / P(X) P(Y∣X)=P(X,Y)/P(X) or =&gt; P(Y∣X)=P(X∣Y)×P(Y)P(X)P(Y|X) = \\frac {P(X|Y) \\times P(Y)}{P(X)} P(Y∣X)=P(X)P(X∣Y)×P(Y)​ Then the total probability formula: P(X)=∑kP(X∣Y=Yk)P(Yk)P(X) = \\sum_kP(X|Y = Y_k)P(Y_k) P(X)=k∑​P(X∣Y=Yk​)P(Yk​) ∑kP(Yk)=1\\sum_kP(Y_k) = 1 k∑​P(Yk​)=1 Therefore, The Bayesian formula is easily derived from the above formulas: P(Yk∣X)=P(X∣Yk)×P(Yk)∑kP(X∣Y=Yk)P(Yk)P(Y_k|X) = \\frac {P(X|Y_k) \\times P(Y_k)} {\\sum_k P(X|Y=Y_k)P(Y_k)} P(Yk​∣X)=∑k​P(X∣Y=Yk​)P(Yk​)P(X∣Yk​)×P(Yk​)​ Question: Suppose there is a latent disease among the people. P(disease) = 0.01 P(well) = 0.99 Doctor can detect this disease by some kinds of kit. The test result is &gt;positive(not healthy) and negtive(healthy). However, detection result &gt;may also be wrong. The conditional probability is : P(positive | well) = 0.01 (healthy people detected with positive) P(negtive | well) = 0.99 P(positive | disease) = 0.99 (unhealthy people detected with positive) P(negtive | disease) = 0.01 So, What is probability of P(disease | postive)? that is, if the detection is positive(bad), what is the probabiliy of true disease？ (Answer: P(disease | postive) = 0.5 ) (We can conclude P(X|Y) and P(Y|X) are totally different. P(X|Y) = P(Y|X)&gt;*P(X)/P(Y)) Bayesian interpretation In the Bayesian (or epistemological) interpretation, probability measures a “degree of belief.” Bayes’ theorem then links the degree of belief in a proposition before and after accounting for evidence. For example, suppose it is believed with 50% certainty that a coin is twice as likely to land heads than tails. If the coin is flipped a number of times and the outcomes observed, that degree of belief may rise, fall or remain the same depending on the results. P(A∣B)=P(B∣A)×P(A)P(B)P(A|B) = \\frac {P(B|A) \\times P(A)}{P(B)} P(A∣B)=P(B)P(B∣A)×P(A)​ For proposition A and evidence B, P (A), the prior, is the initial degree of belief in A. P (A | B), the posterior is the degree of belief having accounted for B. the quotient P(B | A)/P(B) represents the support B provides for A. In theory, the probabilistic model classifier is a conditional probability model: P( C|F1,...,Fn ) Based on the Bayes Formula: P(C∣F1,...,Fn)=P(F1,...,Fn∣C)∗P(C)P(F1,...,Fn)P(C|F1,...,Fn) = \\frac {P(F1,...,Fn|C) * P(C)} {P(F1,...,Fn)} P(C∣F1,...,Fn)=P(F1,...,Fn)P(F1,...,Fn∣C)∗P(C)​ P(类别∣特征)=P(特征∣类别)∗P(类别)P(特征)P(类别|特征) = \\frac {P(特征|类别) * P(类别)} {P(特征)} P(类别∣特征)=P(特征)P(特征∣类别)∗P(类别)​ P(C) is the prior probability P(C|F1,...,Fn) is the posterior probability C represents any Group of the classifier. (F1,...,Fn) represents features of data that we need to predict. We can understand it this way, when we don’t know any features of the sample (predict database) that we need to predict, first determine the probability that the sample is a certain category is P(C), then, after knowing the features of the sample, multiply $ \\frac {P(F1,...,Fn|C)} {P(F1,...,Fn)} $ to get the sample's classification prediction. Denominator is P(F1,...,Fn), we can view it as a constant, since it does not depend on C, and value is same for each group probability prediction. Therefore, what we need to concern is the numerator P(F1,...,Fn|C) * P(C), which is equivalent to the joint distribution model P(C,F1,...,Fn) The Naive Bayes model makes a bold assumption here that ALL FEATURES (F1,...,Fn) are independent of each other, so that we can draw: P(F1,...,Fn∣C)=P(F1∣C)∗P(F2∣C)∗...∗P(Fn∣C)P(F1,...,Fn|C) = P(F1|C)*P(F2|C)*...*P(Fn|C) P(F1,...,Fn∣C)=P(F1∣C)∗P(F2∣C)∗...∗P(Fn∣C) Then the numerator P(F1,...,Fn|C) * P(C) is equivalent to: P(C)∗∏i=1nP(Fi∣C)P(C)* \\prod_{i=1}^n P(F_i|C) P(C)∗i=1∏n​P(Fi​∣C) Suppose there are k groups for classification: C1, C2, ... Ck. Then the prediction result formula of Naive Bayes Classifier is: Cresult=arg max⁡C1,C2,...CkP(Ck)∗∏i=1nP(Fi∣Ck)C_{result} = \\argmax_{C1, C2, ... Ck} P(C_k)* \\prod_{i=1}^n P(F_i|C_k) Cresult​=C1,C2,...Ckargmax​P(Ck​)∗i=1∏n​P(Fi​∣Ck​) classify(f1,...fn)=arg max⁡C1,C2,...CkP(C=c)∗∏i=1nP(Fi=fi∣C=c)classify(f1,...fn) = \\argmax_{C1, C2, ... Ck} P(C = c)* \\prod_{i=1}^n P(F_i=f_i|C=c) classify(f1,...fn)=C1,C2,...Ckargmax​P(C=c)∗i=1∏n​P(Fi​=fi​∣C=c) We calculate all the probabilities that the predicted data belong to each group, and then, by comparison, classify the data into the group with the highest probability (the most probable group). Naive Bayes parameter estimation In the previous section, we knew the formula, as long as we compare which group has largest $ P(C = c)* \\prod_{i=1}^n P(F_i=f_i|C=c) $. In this section, we discuss how to calculate this two kinds of probability. P(C=Ck) is easy, by maximum likelihood estimation we can easily get the frequency of occurrence of Ck. If the total number of sample is M, number of occurrences of Ck is Mk. Then P(C=Ck) = Mk/M. Therefore, the prior probability P(C=Ck) is very dependent on the selection of the overall sample. If the size of dataset is not big enough, or the propotion of count of each category is uneven, it will greatly affect the prediction of Naive Bayes Classification. P(Fi=fi | C=Ck) is a bit complicated, it depends on the data type of Fi. If the Fi is the Discrete Value: Polynomial distributionP(Fi=fi∣C=Ck)=MkfiMkP(Fi=fi | C=Ck) = \\frac{M_{kfi}}{M_k} P(Fi=fi∣C=Ck)=Mk​Mkfi​​ Mkfi is the number of occurrences of fi in Mk. Somtime, Mkfi may be 0, which will affect he estimation of posterior probability. The solution is to import Laplace smoothing:P(Fi=fi∣C=Ck)=Mkfi+λMk+MfiλP(Fi=fi | C=Ck) = \\frac{M_{kfi} + \\lambda}{M_k + M_{fi}\\lambda} P(Fi=fi∣C=Ck)=Mk​+Mfi​λMkfi​+λ​ λ is a constant larger than 0, usually 1. Mfi is number of fi value. If the Fi is the Very Sparse discrete values: Bernoulli distributionP(Fi=fi∣C=Ck)=P(Fi=1∣C=Ck)∗fi+(1−P(Fi=1∣C=Ck))∗(1−fi)P(Fi=fi | C=Ck) = P(Fi=1|C=Ck)*fi + (1-P(Fi=1|C=Ck))*(1-fi) P(Fi=fi∣C=Ck)=P(Fi=1∣C=Ck)∗fi+(1−P(Fi=1∣C=Ck))∗(1−fi) fi is 0 or 1. if fi appears in the subset Mk, fi is 1, otherwise 0. If the Fi is a very sparse discrete value, that is, the occurrence probability of each feature is very low, then we can assume Fi in line with Bernoulli distribution. Feature fi appears as 1, does not appear as 0. We only pay attention to whether fi appears, not the number of occurrences. If the Fi is the Continuous Value, we usually take Fi's prior probability as normally distributed: P(Fi=fi∣C=Ck)=12πσk2∗e−(fi−μk)22σk2P(Fi=fi | C=C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} * e^{-\\frac{(fi - \\mu_k)^2}{2\\sigma_k^2}} P(Fi=fi∣C=Ck​)=2πσk2​​1​∗e−2σk2​(fi−μk​)2​ # Continuous Data Code import math pi = math.pi e = math.e def probabilityDensity(mu, sig2, v): res = 1/math.sqrt(2*pi*sig2) * e ** (-(v-mu)**2/(2*sig2)) return format(res,'.4e') Naive Bayes algorithm process Step one: Input the prior probability P(C=Ck), if the P(C=Ck) is Null, P(C=Ck) = (Mk + λ) / (M + Kλ) Step two: Calculate the conditional probability of each feature of the k-th category. Condition 1: Feature i is the Discrete Value:P(Fi=fi∣C=Ck)=Mkfi+λMk+MfiλP(Fi=fi | C=Ck) = \\frac{M_{kfi} + \\lambda}{M_k + M_{fi}\\lambda} P(Fi=fi∣C=Ck)=Mk​+Mfi​λMkfi​+λ​ Condition 2: Feature i is the Sparse Discrete Value:P(Fi=fi∣C=Ck)=P(Fi=1∣C=Ck)∗fi+(1−P(Fi=1∣C=Ck))∗(1−fi)P(Fi=fi | C=Ck) = P(Fi=1|C=Ck)*fi + (1-P(Fi=1|C=Ck))*(1-fi) P(Fi=fi∣C=Ck)=P(Fi=1∣C=Ck)∗fi+(1−P(Fi=1∣C=Ck))∗(1−fi) Condition 1: Feature i is the Continuous Value:P(Fi=fi∣C=Ck)=12πσk2∗e−(fi−μk)22σk2P(Fi=fi | C=C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} * e^{-\\frac{(fi - \\mu_k)^2}{2\\sigma_k^2}} P(Fi=fi∣C=Ck​)=2πσk2​​1​∗e−2σk2​(fi−μk​)2​ Step three: For the instance prediction features (pf1,...pfn), Calculate posterior probability separately: P(C=Ck)∗∏i=1nP(Fi=pfi∣Ck)P(C=C_k) * \\prod_{i=1}^n P(F_i = pf_i | C_k) P(C=Ck​)∗i=1∏n​P(Fi​=pfi​∣Ck​) Determine the classification of prediction data (pf1,...pfn): Cresult=arg max⁡C1,C2,...CkP(Ck)∏i=1nP(Fi=pfi∣Ck)C_{result} = \\argmax_{C_1, C_2,...C_k} P(C_k) \\prod_{i=1}^n P(F_i = pf_i | C_k) Cresult​=C1​,C2​,...Ck​argmax​P(Ck​)i=1∏n​P(Fi​=pfi​∣Ck​) Pros and cons Conclusion Advantages of Naive Bayes: The naive Bayesian model originates from classical mathematical theory and has a stable classification efficiency. Performs well on small-scale data, can handle multi-classification tasks, suitable for incremental training, especially when the amount of data exceeds the memory, we can batch-by-batch incremental training. Less sensitive to missing data, and the algorithm is relatively simple, often used for text classification. Disadvantages of Naive Bayes: In theory, the Naive Bayes model has the smallest error rate compared with other classification methods. However, this is not always the case, because the naive Bayes model gives an output category, and assumes that the attributes are mutually independent. This assumption is often not true in practical applications. When the correlation between them is large, the classification effect is not good. When the attribute correlation is small, Naive Bayes has the best performance. For this, there are algorithms such as semi-Naive Bayes that are moderately improved by considering partial relevance. Need to know the a priori probability, and the priori probability often depends on the hypothesis, there can be many models of hypothesis, so in some cases, due to the hypothesis of the prior model, the prediction effect is not good. Since we decide the classification by prior and data to determine the posterior probability, the classification decision has a certain error rate. Very sensitive to the type of input data: discrete, sparse discrete or contiuous data. ","link":"https://zl-wu.github.io/post/mla-naive-bayes-theory/"},{"title":"Python code of binomial distribution","content":"&quot;python code to play with binomial distribution&quot; Binomial Distribution import numpy as np import matplotlib.pyplot as plt def C_nk (n, k): # get C(n,k) res = 1 for i in range(n, n-k,-1): res *= i for i in range(k,0,-1): res /= i return res def binomProb(n, k, p): return C_nk(n,k) * p**k * (1-p)**(n-k) # n = 10, P(yes) = 0.3; P(no) = 0.7 n = 10 p = 0.3 x = np.arange(n+1) y = [] for k in x: y.append(binomProb(n,k,p)) # Draw the Binomial probability distribution plt.bar(x,y) plt.show() import scipy.stats as sps # all k&gt;2 probability n = 10 p = 0.3 k = np.arange(n + 1) PX = sps.binom.pmf(k, n, p) # 二项分布累计分布 print(sum(PX[2:])) Bernoulli Distribution X~B(n,p), when n=1, Binomial distribution becomes Bernoulli Distribution. 伯努利分布、两点分布、0-1分布这三种分布是同一个分布的不同名称，又都是二项分布在n=1时的特例。 概率质量函数(probability mass function，简称PMF) 累积分布函数(cumulative distribution function, 简称CDF) 离散概率分布(Discrete probability distribution) ","link":"https://zl-wu.github.io/post/python-code-of-binomial-distribution/"},{"title":"Markdown syntax note (Continuously updating)","content":"LaTex Greek Alphabet correspondence table: Alphabet LaTex Code α\\alphaα $\\alpha$ β\\betaβ $\\beta$ γ\\gammaγ $\\gamma$ Γ\\GammaΓ $\\Gamma$ δ\\deltaδ $\\delta$ Δ\\DeltaΔ $\\Delta$ ϵ\\epsilonϵ $\\epsilon$ ε\\varepsilonε $\\varepsilon$ ζ\\zetaζ $\\zeta$ η\\etaη $\\eta$ θ\\thetaθ $\\theta$ Θ\\ThetaΘ $\\Theta$ ϑ\\varthetaϑ $\\vartheta$ ι\\iotaι $\\iota$ κ\\kappaκ $\\kappa$ λ\\lambdaλ $\\lambda$ Λ\\LambdaΛ $\\Lambda$ μ\\muμ $\\mu$ ν\\nuν $\\nu$ ξ\\xiξ $\\xi$ Ξ\\XiΞ $\\Xi$ π\\piπ $\\pi$ Π\\PiΠ $\\Pi$ ϖ\\varpiϖ $\\varpi$ ρ\\rhoρ $\\rho$ ϱ\\varrhoϱ $\\varrho$ σ\\sigmaσ $\\sigma$ Σ\\SigmaΣ $\\Sigma$ ς\\varsigmaς $\\varsigma$ τ\\tauτ $\\tau$ υ\\upsilonυ $\\upsilon$ Υ\\UpsilonΥ $\\Upsilon$ ϕ\\phiϕ $\\phi$ Φ\\PhiΦ $\\Phi$ φ\\varphiφ $\\varphi$ χ\\chiχ $\\chi$ ψ\\psiψ $\\psi$ Ψ\\PsiΨ $\\Psi$ Ω\\OmegaΩ $\\Omega$ ω\\omegaω $\\omega$ ≡\\equiv≡ $\\equiv$ ","link":"https://zl-wu.github.io/post/markdown-syntax-note-continuously-updating/"},{"title":"Topic modeling in Gensim","content":"&quot;Research the source code of Topic Modeling in gensim&quot; A Simple Example Code from nltk.tokenize import regexp_tokenize from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() tokens = [] for doc in my_docs: words = regexp_tokenize(doc.lower(), r'[A-Za-z]+') words = [w for w in words if w not in stopwords.words('english')] words = [lemmatizer.lemmatize(w) for w in words] tokens.append(words) # Gensim tf-idf from gensim.corpora import Dictionary from gensim.models import TfidfModel my_dict = Dictionary(tokens) my_dict.filter_extremes(no_below=5, no_above=0.90) dtm = [my_dict.doc2bow(doc) for doc in tokens] tfidf = TfidfModel(dtm) for doc in tfidf[dtm]: print([[my_dict[i], np.around(freq, decimals=2)] for i, freq in doc]) ''' output: [['advertising', 0.16], ['bbc', 0.04], ['bill', 0.05], ['book', 0.05], ....] [['book', 0.19], ['company', 0.05], ['firm', 0.11], ['month', 0.04], ['telecom', 0.26], ....] [['bbc', 0.09], ['moment', 0.11], ['month', 0.05], ['bos', 0.12], ['cross', 0.15], ....] [['home', 0.06], ['play', 0.07], ['face', 0.31], ['game', 0.11], ['league', 0.1], ....] ... ''' # Gensim: LSI from gensim.models import LsiModel, CoherenceModel lsi_model = LsiModel(corpus=dtm, id2word=my_dict, num_topics=5) # lsi_model.print_topics(-1) lsi_model.print_topics(num_topics=5, num_words=5) # Determining optimum number of topics using coherence values coherence_values = [] lsi_model_list = [] min_topics, max_topics, step = 1, 5, 1 for i in range(min_topics, max_topics, step): lsi_model = LsiModel(dtm, id2word=my_dict, num_topics=i) lsi_model_list.append(lsi_model) coherencemodel = CoherenceModel(model=lsi_model, texts=tokens, \\ dictionary=my_dict, coherence='c_v') coherence_values.append(coherencemodel.get_coherence()) # Gensim: LDA from gensim.models import LdaModel, LdaMulticore lda_model = LdaModel(dtm, num_topics=3, id2word=my_dict, passes=10) # lda_model.print_topics(-1) lda_model.print_topics(num_topics=3, num_words=3) lda_model_mc = LdaMulticore(dtm, num_topics=3, id2word=my_dict, passes=10, workers=4) lda_model_mc.print_topics(-1) lda_model.save('my_lda_model.lda') # Save LDA model # Evaluating LDA models: Topic coherence goodLdaModel = LdaModel(corpus=dtm, id2word=my_dict, iterations=50, num_topics=2) badLdaModel = LdaModel(corpus=dtm, id2word=my_dict, iterations=1, num_topics=2) goodcm = CoherenceModel(model=goodLdaModel, corpus=dtm, dictionary=my_dict, coherence='u_mass') badcm = CoherenceModel(model=badLdaModel, corpus=dtm, dictionary=my_dict, coherence='u_mass') goodcm.get_coherence() badcm.get_coherence() goodcm = CoherenceModel(model=goodLdaModel, texts=dtm, dictionary=my_dict, coherence='c_v') badcm = CoherenceModel(model=badLdaModel, texts=dtm, dictionary=my_dict, coherence='c_v') # Display LDA outputs (runs only on HTML platforms like Jupyter) import pyLDAvis.gensim pyLDAvis.enable_notebook() vis = pyLDAvis.gensim.prepare(lda_model, dtm, my_dict) pyLDAvis.display(vis) 1. Dictionary (source) &gt;&gt;&gt; from gensim.corpora import Dictionary &gt;&gt;&gt; &gt;&gt;&gt; texts = [['human', 'interface', 'computer']] &gt;&gt;&gt; dct = Dictionary(texts) # initialize a Dictionary &gt;&gt;&gt; dct.add_documents([[&quot;cat&quot;, &quot;say&quot;, &quot;meow&quot;], [&quot;dog&quot;]]) # add more document (extend the vocabulary) &gt;&gt;&gt; dct.doc2bow([&quot;dog&quot;, &quot;computer&quot;, &quot;non_existent_word&quot;]) [(0, 1), (6, 1)] The most important methods is doc2bow(['..','..',...]) def doc2bow(self, document, allow_update=False, return_missing=False): &quot;&quot;&quot;Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.&quot;&quot;&quot; ... # Construct (word, frequency) mapping. counter = defaultdict(int) for w in document: counter[w if isinstance(w, unicode) else unicode(w, 'utf-8')] += 1 token2id = self.token2id result = {token2id[w]: freq for w, freq in iteritems(counter) if w in token2id} ... result = sorted(iteritems(result)) ... return result doc2bow() is very similar with nltk.FreqDist(). The result returned is [(worid_id, freq),(worid_id, freq),...]. 2. TfidfModel (source) from gensim.models import TfidfModel dtm = [my_dict.doc2bow(doc) for doc in tokens] tfidf_vectorizer = TfidfModel(dtm) tfidf = tfidf_vectorizer[dtm] # idf: self.idfs = precompute_idfs(self.wglobal, self.dfs, self.num_docs) print(self.wglobal) # &lt;function gensim.models.tfidfmodel.df2idf(docfreq, totaldocs, log_base=2.0, add=0.0)&gt; def precompute_idfs(wglobal, dfs, total_docs): return {termid: wglobal(df, total_docs) for termid, df in iteritems(dfs)} def df2idf(docfreq, totaldocs, log_base=2.0, add=0.0): return add + np.log(float(totaldocs) / docfreq) / np.log(log_base) idf=0+log⁡(totalDocsdocFreq)log⁡2=log⁡2(totalDocsdocFreq)=log⁡2(Nnk)idf = 0 + \\frac{\\log(\\frac{totalDocs}{docFreq})} {\\log2} = \\log_2(\\frac{totalDocs}{docFreq}) = \\log_2(\\frac{N}{n_k}) idf=0+log2log(docFreqtotalDocs​)​=log2​(docFreqtotalDocs​)=log2​(nk​N​) # tf: dtm # tf-idf: as example of dtm[0] from gensim.models import TfidfModel termid_array, tf_array = [], [] for termid, tf in dtm[0]: termid_array.append(termid) tf_array.append(tf) model = TfidfModel(dtm) print(model.idfs) # it is the idf of each token in dictionary print(model.eps) # 1e-12 # if a word almost appears in all documents, which makes idf of this word is very close to 0, then we remove it in tfidf. vector = [(termid, tf * model.idfs.get(termid)) for termid, tf in zip(termid_array, tf_array) if abs(model.idfs.get(termid, 0.0)) &gt; model.eps] # next we use l2 normalize this vector (default normalize) length = 1.0 * math.sqrt(sum(val ** 2 for _, val in vector)) tfidf = [(termid, val/length) for termid, val in vector] # This is what happend behind the code &quot;model[dtm[0]]&quot; # l1 : length = float(sum(abs(val) for _, val in vector)) # l2 : length = 1.0 * math.sqrt(sum(val ** 2 for _, val in vector)) # unique: length = 1.0 * len(vector) After obtaining the product of tf and idf of the current word, Cosine normalization is performed. length=1.0×∑val2∈vector=∑k=1twik2length = 1.0 \\times \\sqrt{\\sum{val^2 \\in vector}} = \\sqrt{\\sum^{t}_{k=1}w^2_{i_k}} length=1.0×∑val2∈vector​=k=1∑t​wik​2​​ tfidf=val∈vectorlengthtfidf = \\frac{val \\in vector} {length} tfidf=lengthval∈vector​ In fact, the above tfidf calculation methods are all default, that is, we can specify the calculation method according to requirements. # We can modify this attribute when we create an instance of TfidfModel class TfidfModel(interfaces.TransformationABC): def __init__(self, corpus=None, id2word=None, dictionary=None, wlocal=utils.identity, wglobal=df2idf, normalize=True, smartirs=None, pivot=None, slope=0.25): .... &quot;&quot;&quot; smartirs : str, optional SMART (System for the Mechanical Analysis and Retrieval of Text) Information Retrieval System, a mnemonic scheme for denoting tf-idf weighting variants in the vector space model. The mnemonic for representing a combination of weights takes the form XYZ, for example 'ntc', 'bpn' and so on, where the letters represents the term weighting of the document vector. Term frequency weighing: * `b` - binary, * `t` or `n` - raw, * `a` - augmented, * `l` - logarithm, * `d` - double logarithm, * `L` - log average. Document frequency weighting: * `x` or `n` - none, * `f` - idf, * `t` - zero-corrected idf, * `p` - probabilistic idf. Document normalization: * `x` or `n` - none, * `c` - cosine, * `u` - pivoted unique, * `b` - pivoted character length. Default is 'nfc'. For more information visit `SMART Information Retrieval System &lt;https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System&gt;`_. &quot;&quot;&quot; SMART Information Retrieval System 3. LsiModel (source) Model for Latent Semantic Indexing https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing # Gensim: LSI from gensim.models import LsiModel, CoherenceModel lsi_model = LsiModel(corpus=dtm, id2word=my_dict, num_topics=5) # lsi_model.print_topics(-1) lsi_model.print_topics(num_topics=5, num_words=5) def __init__(self, m, k, docs=None, use_svdlibc=False, power_iters=P2_EXTRA_ITERS, extra_dims=P2_EXTRA_DIMS, dtype=np.float64): ## __init__() self.projection = Projection( self.num_terms, self.num_topics, power_iters=self.power_iters, extra_dims=self.extra_samples, dtype=dtype ) ## add_document() update = Projection( self.num_terms, self.num_topics, job, extra_dims=self.extra_samples, power_iters=self.power_iters, dtype=self.dtype ) def stochastic_svd(corpus, rank, num_terms, chunksize=20000, extra_dims=None, power_iters=0, dtype=np.float64, eps=1e-6): pass u, s = stochastic_svd( docs, k, chunksize=sys.maxsize, num_terms=m, power_iters=self.power_iters, extra_dims=self.extra_dims, dtype=dtype) self.show_topics(num_topics=num_topics, num_words=num_words, log=True) def show_topics(self, num_topics=-1, num_words=10, log=False, formatted=True): &quot;&quot;&quot;Get the most significant topics. log : bool, optional If True - log topics with logger. formatted : bool, optional If True - each topic represented as string, otherwise - in BoW format. Returns ------- list of (int, str) If `formatted=True`, return sequence with (topic_id, string representation of topics) **OR** list of (int, list of (str, float)) Otherwise, return sequence with (topic_id, [(word, value), ... ]). &quot;&quot;&quot; shown = [] if num_topics &lt; 0: num_topics = self.num_topics for i in range(min(num_topics, self.num_topics)): if i &lt; len(self.projection.s): if formatted: topic = self.print_topic(i, topn=num_words) else: topic = self.show_topic(i, topn=num_words) shown.append((i, topic)) if log: logger.info(&quot;topic #%i(%.3f): %s&quot;, i, self.projection.s[i], topic) return shown 4. LdaModel 5. LdaMulticore 6. CoherenceModel Example of sklearn topic modeling https://cloud.tencent.com/developer/article/1530432 docs = [&quot;In the middle of the night&quot;, &quot;When our hopes and fears collide&quot;, &quot;In the midst of all goodbyes&quot;, &quot;Where all human beings lie&quot;, &quot;Against another lie&quot;] vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(docs) terms = vectorizer.get_feature_names() print(terms) n_pick_topics = 3 # 设定主题数为3 lsa = TruncatedSVD(n_pick_topics) X2 = lsa.fit_transform(X) X2 n_pick_docs= 2 topic_docs_id = [X2[:,t].argsort()[:-(n_pick_docs+1):-1] for t in range(n_pick_topics)] topic_docs_id n_pick_keywords = 4 topic_keywords_id = [lsa.components_[t].argsort()[:-(n_pick_keywords+1):-1] for t in range(n_pick_topics)] topic_keywords_id for t in range(n_pick_topics): print(&quot;topic %d:&quot; % t) print(&quot; keywords: %s&quot; % &quot;, &quot;.join(terms[topic_keywords_id[t][j]] for j in range(n_pick_keywords))) for i in range(n_pick_docs): print(&quot; doc %d&quot; % i) print(&quot;\\t&quot;+docs[topic_docs_id[t][i]]) ","link":"https://zl-wu.github.io/post/topic-modeling-in-gensim/"},{"title":"Compare nltk and textblob''''''''''''''''s sentiment analysis","content":"&quot;This is a comparison report between nltk and textblob in sentiment analysis.&quot; Overview lexicon 1. NLTK's vader_lexicon.txt : NLTK's default lexicon file (online source) # Part of vader_lexicon.txt .... # emoticon (-* 1.3 1.26886 [4, 1, 2, 0, 2, -1, 1, 2, 1, 1] (-: 1.6 0.8 [2, 2, 1, 3, 1, 1, 1, 3, 1, 1] (-:0 2.8 0.87178 [3, 2, 3, 4, 3, 2, 3, 1, 4, 3] (-:&lt; -0.4 2.15407 [-3, 3, -1, -1, 2, -1, -2, 3, -3, -1] (-:o 1.5 0.67082 [3, 1, 1, 2, 2, 2, 1, 1, 1, 1] (-:O 1.5 0.67082 [3, 1, 1, 2, 2, 2, 1, 1, 1, 1] (-:{ -0.1 1.57797 [-2, -3, 1, -2, 1, 1, 0, 0, 2, 1] (-:|&gt;* 1.9 0.83066 [3, 2, 2, 1, 0, 2, 3, 2, 2, 2] (-; 1.3 1.18743 [3, 2, 3, 0, 1, -1, 1, 2, 1, 1] (-;| 2.1 1.13578 [3, 2, 2, 4, 1, 1, 1, 4, 2, 1] (8 2.6 1.0198 [4, 2, 1, 3, 3, 3, 3, 1, 2, 4] .... # word cheer 2.3 0.64031 [2, 1, 2, 2, 2, 3, 3, 3, 2, 3] cheered 2.3 0.78102 [2, 3, 3, 4, 2, 1, 2, 2, 2, 2] cheerer 1.7 0.45826 [1, 2, 2, 2, 1, 1, 2, 2, 2, 2] cheerers 1.8 0.87178 [2, 2, 3, 2, 1, 2, 0, 1, 3, 2] cheerful 2.5 0.67082 [3, 2, 3, 2, 2, 2, 4, 2, 3, 2] cheerfuller 1.9 0.83066 [3, 3, 2, 3, 2, 1, 1, 2, 1, 1] cheerfullest 3.2 0.87178 [4, 4, 4, 4, 3, 2, 2, 3, 2, 4] cheerfully 2.1 0.83066 [3, 2, 2, 2, 1, 3, 1, 3, 1, 3] cheerfulness 2.1 0.9434 [3, 2, 1, 2, 3, 4, 1, 2, 1, 2] cheerier 2.6 0.4899 [2, 2, 3, 3, 2, 3, 3, 2, 3, 3] cheeriest 2.2 0.6 [3, 2, 3, 1, 2, 2, 3, 2, 2, 2] .... # load lexicon in Python from nltk.sentiment.vader import SentimentIntensityAnalyzer analyzer = SentimentIntensityAnalyzer() type(analyzer.lexicon) # dict len(analyzer.lexicon) # 7502 analyzer.lexicon ''' output: { ... '(-*': 1.3, '(-:': 1.6, '(-:0': 2.8, '(-:&lt;': -0.4, '(-:o': 1.5, '(-:O': 1.5, '(-:{': -0.1, '(-:|&gt;*': 1.9, '(-;': 1.3, '(-;|': 2.1, '(8': 2.6, ... } ''' 2. TextBlob's en-sentiment.xml : TextBlob's default lexicon file (online source) &lt;sentiment language=&quot;en&quot; version=&quot;1.3&quot; author=&quot;Tom De Smedt, Walter Daelemans&quot; license=&quot;PDDL&quot;&gt; &lt;word form=&quot;13th&quot; wordnet_id=&quot;a-02203763&quot; pos=&quot;JJ&quot; sense=&quot;coming next after the twelfth in position&quot; polarity=&quot;0.0&quot; subjectivity=&quot;0.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;20th&quot; cornetto_synset_id=&quot;n_a-531612&quot; wordnet_id=&quot;a-02204716&quot; pos=&quot;JJ&quot; sense=&quot;coming next after the nineteenth in position&quot; polarity=&quot;0.0&quot; subjectivity=&quot;0.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;21st&quot; wordnet_id=&quot;a-02204823&quot; pos=&quot;JJ&quot; sense=&quot;coming next after the twentieth in position&quot; polarity=&quot;0.0&quot; subjectivity=&quot;0.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;2nd&quot; wordnet_id=&quot;a-02202146&quot; pos=&quot;JJ&quot; sense=&quot;coming next after the first in position in space or time or degree or magnitude&quot; polarity=&quot;0.0&quot; subjectivity=&quot;0.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;3rd&quot; cornetto_synset_id=&quot;n_a-530634&quot; wordnet_id=&quot;a-02202307&quot; pos=&quot;JJ&quot; sense=&quot;coming next after the second and just before the fourth in position&quot; polarity=&quot;0.0&quot; subjectivity=&quot;0.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;abhorrent&quot; wordnet_id=&quot;a-1625063&quot; pos=&quot;JJ&quot; sense=&quot;offensive to the mind&quot; polarity=&quot;-0.7&quot; subjectivity=&quot;0.8&quot; intensity=&quot;1.0&quot; reliability=&quot;0.9&quot; /&gt; &lt;word form=&quot;able&quot; cornetto_synset_id=&quot;n_a-534450&quot; wordnet_id=&quot;a-01017439&quot; pos=&quot;JJ&quot; sense=&quot;having a strong healthy body&quot; polarity=&quot;0.5&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;able&quot; wordnet_id=&quot;a-00001740&quot; pos=&quot;JJ&quot; sense=&quot;(usually followed by 'to') having the necessary means or skill or know-how or authority to do something&quot; polarity=&quot;0.5&quot; subjectivity=&quot;0.5&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; .... &lt;word form=&quot;implicit in&quot; cornetto_synset_id=&quot;n_a-520863&quot; wordnet_id=&quot;a-00941940&quot; pos=&quot;JJ&quot; sense=&quot;in the nature of something though not readily apparent&quot; polarity=&quot;0.0&quot; subjectivity=&quot;0.1&quot; intensity=&quot;1.0&quot; confidence=&quot;0.8&quot; /&gt; &lt;word form=&quot;important&quot; cornetto_synset_id=&quot;d_a-9178&quot; wordnet_id=&quot;a-01830403&quot; pos=&quot;JJ&quot; sense=&quot;having authority or ascendancy or influence&quot; polarity=&quot;0.5&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;important&quot; cornetto_synset_id=&quot;n_a-527688&quot; wordnet_id=&quot;a-02161432&quot; pos=&quot;JJ&quot; sense=&quot;important in effect or meaning&quot; polarity=&quot;0.5&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;important&quot; wordnet_id=&quot;a-00655779&quot; pos=&quot;JJ&quot; sense=&quot;of extreme importance&quot; polarity=&quot;0.0&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;important&quot; wordnet_id=&quot;a-01275562&quot; pos=&quot;JJ&quot; sense=&quot;of great significance or value&quot; polarity=&quot;0.5&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;important&quot; wordnet_id=&quot;a-01539887&quot; pos=&quot;JJ&quot; sense=&quot;having or suggesting a consciousness of high position&quot; polarity=&quot;0.5&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;impossible&quot; cornetto_synset_id=&quot;n_a-511166&quot; wordnet_id=&quot;a-02418692&quot; pos=&quot;JJ&quot; sense=&quot;totally unlikely&quot; polarity=&quot;-0.5&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;impossible&quot; cornetto_synset_id=&quot;n_a-521243&quot; wordnet_id=&quot;a-02436025&quot; pos=&quot;JJ&quot; sense=&quot;used of persons or their behavior&quot; polarity=&quot;-1.0&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;impossible&quot; wordnet_id=&quot;a-01823092&quot; pos=&quot;JJ&quot; sense=&quot;not capable of occurring or being accomplished or dealt with&quot; polarity=&quot;-0.5&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;impressed&quot; cornetto_synset_id=&quot;n_a-509653&quot; wordnet_id=&quot;a-00071142&quot; pos=&quot;JJ&quot; sense=&quot;deeply or markedly affected or influenced&quot; polarity=&quot;1.0&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;impressive&quot; cornetto_synset_id=&quot;n_a-524894&quot; wordnet_id=&quot;a-00835292&quot; pos=&quot;JJ&quot; sense=&quot;producing a strong effect&quot; polarity=&quot;1.0&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;impressive&quot; wordnet_id=&quot;a-01282014&quot; pos=&quot;JJ&quot; sense=&quot;making a strong or vivid impression&quot; polarity=&quot;1.0&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.9&quot; /&gt; &lt;word form=&quot;in good taste&quot; cornetto_synset_id=&quot;n_a-528044&quot; wordnet_id=&quot;a-00689215&quot; pos=&quot;JJ&quot; sense=&quot;satisfying generally accepted social or esthetic standards&quot; polarity=&quot;0.9&quot; subjectivity=&quot;1.0&quot; intensity=&quot;1.0&quot; confidence=&quot;0.8&quot; /&gt; &lt;word form=&quot;in stock&quot; cornetto_synset_id=&quot;n_a-533683&quot; wordnet_id=&quot;a-00184543&quot; pos=&quot;JJ&quot; sense=&quot;available for use or sale&quot; polarity=&quot;0.1&quot; subjectivity=&quot;0.4&quot; intensity=&quot;1.0&quot; confidence=&quot;0.8&quot; /&gt; .... &lt;/sentiment&gt; # load lexicon in Python from textblob.en import sentiment as pattern_sentiment type(pattern_sentiment) # textblob.en.Sentiment len(pattern_sentiment) # 2860 pattern_sentiment['great'] # {'JJ': [0.8, 0.75, 1.0], None: [0.8, 0.75, 1.0]} pattern_sentiment('great') # (0.8, 0.75) for k,v in pattern_sentiment.items(): print(k,v) ''' output: 13th {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} 20th {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} 21st {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} 2nd {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} 3rd {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} abhorrent {'JJ': [-0.7, 0.8, 1.0], None: [-0.7, 0.8, 1.0]} able {'JJ': [0.5, 0.625, 1.0], None: [0.5, 0.625, 1.0]} above {'JJ': [0.0, 0.1, 1.0], None: [0.0, 0.1, 1.0]} .... 13th {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} 20th {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} 21st {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} 2nd {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} 3rd {'JJ': [0.0, 0.0, 1.0], None: [0.0, 0.0, 1.0]} abhorrent {'JJ': [-0.7, 0.8, 1.0], None: [-0.7, 0.8, 1.0]} able {'JJ': [0.5, 0.625, 1.0], None: [0.5, 0.625, 1.0]} above {'JJ': [0.0, 0.1, 1.0], None: [0.0, 0.1, 1.0]} .... ''' Summary Main Algorithm 1. NLTK's methods score_valence(self, sentiments, text) def score_valence(self, sentiments, text): if sentiments: ''' calculate compound value ''' sum_s = float(sum(sentiments)) # compute and add emphasis from punctuation in text punct_emph_amplifier = self._punctuation_emphasis(sum_s, text) if sum_s &gt; 0: sum_s += punct_emph_amplifier elif sum_s &lt; 0: sum_s -= punct_emph_amplifier compound = self.constants.normalize(sum_s) ''' calculate positive, negative and neutral values respectively ''' pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments) if pos_sum &gt; math.fabs(neg_sum): pos_sum += punct_emph_amplifier elif pos_sum &lt; math.fabs(neg_sum): neg_sum -= punct_emph_amplifier total = pos_sum + math.fabs(neg_sum) + neu_count pos = math.fabs(pos_sum / total) neg = math.fabs(neg_sum / total) neu = math.fabs(neu_count / total) else: compound = 0.0 pos = 0.0 neg = 0.0 neu = 0.0 sentiment_dict = { &quot;neg&quot;: round(neg, 3), &quot;neu&quot;: round(neu, 3), &quot;pos&quot;: round(pos, 3), &quot;compound&quot;: round(compound, 4), } return sentiment_dict Parameter sentiments is a list of sentiment values corresponding to each token in the text. In brief, NLTK first sums up all individual words' sentiment values (positive real numbers and negative real numbers), then performs a text puctuation statistical emphasis operation, and finally normalizes the final result to obtain the &quot;Compound&quot; value. And NLTK uses the _sift_sentiment_scores method to get three values of pos, neg, and neu. We will look at this method later. Pos, neg, and neu have also undergone puctuation statistical emphasis. sentiment_valence(self, valence, sentitext, item, i, sentiments) def sentiment_valence(self, valence, sentitext, item, i, sentiments): is_cap_diff = sentitext.is_cap_diff words_and_emoticons = sentitext.words_and_emoticons item_lowercase = item.lower() if item_lowercase in self.lexicon: # get the sentiment valence valence = self.lexicon[item_lowercase] # Rule 1 : check if sentiment laden word is in ALL CAPS (while others aren't) if item.isupper() and is_cap_diff: if valence &gt; 0: valence += self.constants.C_INCR else: valence -= self.constants.C_INCR # Context Rules for start_i in range(0, 3): if ( i &gt; start_i and words_and_emoticons[i - (start_i + 1)].lower() not in self.lexicon ): # dampen the scalar modifier of preceding words and emoticons # (excluding the ones that immediately preceed the item) based # on their distance from the current item. # Rule 2 : preceding booster word s = self.constants.scalar_inc_dec( words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff ) if start_i == 1 and s != 0: s = s * 0.95 if start_i == 2 and s != 0: s = s * 0.9 valence = valence + s # Rule 3 : preceding &quot;never&quot; word valence = self._never_check( valence, words_and_emoticons, start_i, i ) # Rule 4 : idioms check if start_i == 2: valence = self._idioms_check(valence, words_and_emoticons, i) # Rule 5 : check for negation case using &quot;least&quot; valence = self._least_check(valence, words_and_emoticons, i) sentiments.append(valence) return sentiments sentiment_valence is a method of calculating and adjusting the sentiment value of a single word. It is based on multiple rules. First the word has to be in the lexicon databse. After obtaining the basic sentiment value of the word, adjust the sentiment value of the current word again according to the ALL CAPS emphasis, the preceding booster word, &quot;never&quot;, idioms, and &quot;least&quot;. polarity_scores(self, text) def polarity_scores(self, text): &quot;&quot;&quot; Return a float for sentiment strength based on the input text. Positive values are positive valence, negative value are negative valence. &quot;&quot;&quot; # text, words_and_emoticons, is_cap_diff = self.preprocess(text) sentitext = SentiText(text, self.constants.PUNC_LIST, self.constants.REGEX_REMOVE_PUNCTUATION) sentiments = [] words_and_emoticons = sentitext.words_and_emoticons for item in words_and_emoticons: valence = 0 i = words_and_emoticons.index(item) if ( i &lt; len(words_and_emoticons) - 1 and item.lower() == &quot;kind&quot; and words_and_emoticons[i + 1].lower() == &quot;of&quot; ) or item.lower() in self.constants.BOOSTER_DICT: sentiments.append(valence) continue sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments) sentiments = self._but_check(words_and_emoticons, sentiments) return self.score_valence(sentiments, text) After obtaining the list of sentiment values, &quot;but check&quot; rule is also performed. 2. TextBlob's methods ","link":"https://zl-wu.github.io/post/compare-nltk-and-textblobs-sentiment-analysis/"},{"title":"Text Analysis -- NLTK''''''''''''''''s vader library","content":"&quot;Topic: How does NLTK's vader library work to calculate sentiment value for text?&quot; Topic: How does NLTK's vader library work to calculate sentiment value for text? source code -- vader.py Overview NLTK's vader tool is mainly used to judge the sentiment polarity of a text: positive, negative or neutral. From its code design perspective, the vader is more suitable for analyzing short social media text sentiment analysis. The most important part of vader is class SentimentIntensityAnalyzer, and the core methods of this class are polarity_scores(), sentiment_valence() and score_valence(). One simple example of using vader from nltk.sentiment.vader import SentimentIntensityAnalyzer analyzer = SentimentIntensityAnalyzer() sentiment = analyzer.polarity_scores(text) As we all know, the first &quot;import&quot; statement imported “SentimentIntensityAnalyzer” class, and then we can use the methods of this class to calculate the text sentiment value. Next, we initiate a new instance &quot;analyzer&quot; of class &quot;SentimentIntensityAnalyzer&quot;. Finally, we call the methods &quot;polarity_scores()&quot; in this instance &quot;analyzer&quot;. Therefore, we can easily find out that the most critical question is what algorithm is included in the last method &quot;polarity_scores()&quot;? At first what is the content of this class? Class &quot;SentimentIntensityAnalyzer&quot; class SentimentIntensityAnalyzer: &quot;&quot;&quot; Give a sentiment intensity score to sentences. &quot;&quot;&quot; def __init__( self, lexicon_file=&quot;sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt&quot;, ): self.lexicon_file = nltk.data.load(lexicon_file) self.lexicon = self.make_lex_dict() self.constants = VaderConstants() def make_lex_dict(self): &quot;&quot;&quot; Convert lexicon file to a dictionary &quot;&quot;&quot; lex_dict = {} for line in lexicon.split(&quot;\\n&quot;): # Only get &quot;word&quot; and &quot;sentiment value&quot; two data # (word, measure) = line.strip().split(&quot;\\t&quot;)[0:2] lex_dict[word] = float(measure) return lex_dict def polarity_scores(self, text): &quot;&quot;&quot; Return a float for sentiment strength based on the input text. Positive values are positive valence, negative value are negative valence. &quot;&quot;&quot; ... return self.score_valence(sentiments, text) def sentiment_valence(self, valence, sentitext, item, i, sentiments): ... return sentiments def _least_check(self, valence, words_and_emoticons, i): # check for negation case using &quot;least&quot; ... return valence def _but_check(self, words_and_emoticons, sentiments): ... return sentiments def _idioms_check(self, valence, words_and_emoticons, i): ... return valence def _never_check(self, valence, words_and_emoticons, start_i, i): ... return valence def _punctuation_emphasis(self, sum_s, text): # add emphasis from exclamation points and question marks ... return qm_amplifier def _sift_sentiment_scores(self, sentiments): # want separate positive versus negative sentiment scores ... return pos_sum, neg_sum, neu_count def score_valence(self, sentiments, text): ... return sentiment_dict This class needs one argument -- lexicon_file, which is a database of sentiment values for each word and emoticon. It uses nltk's own database by default, but we can also replace our database as needed. The database has at least two columns, the first column is the word and the second column is the sentiment value. We can see what the database -- vader_lexicon.txt looks like at the end of this article. This class also has 10 methods inside.The method starting with underscore _ is generally a method used internally by this class, and generally does not need to be called separately by the user. We can also roughly guess from their names &quot;_xxx_check&quot; that their meaning is the detection and corresponding scheme of the special grammar of text, such as but, never or adjective-est, etc. So we can currently focus our main attention on this four methods: polarity_scores() # main program, which is called mostly by users make_lex_dict() sentiment_valence() score_valence() Before starting to study the algorithm of this method, we must figure out what the two properties of this class generated in the constructor are. self.lexicon &amp; self.constants self.lexicon is generated by loading the word sentiment database and processing with the method make_lex_dict(). And self.lexicon is a dictionary, key and value pair is &quot;word or emoticon&quot; and &quot;sentiment value (a float number)&quot; self.constants is a instance of the class VaderConstants(), which defines some special values, and syntax, such as booster words, emphasize words and idioms. They are mainly used in the check method. The content of class VaderConstants() will be released later. 1. Method &quot;polarity_scores()&quot;: def polarity_scores(self, text): &quot;&quot;&quot; Return a float for sentiment strength based on the input text. Positive values are positive valence, negative value are negative valence. &quot;&quot;&quot; # text, words_and_emoticons, is_cap_diff = self.preprocess(text) sentitext = SentiText(text, self.constants.PUNC_LIST, self.constants.REGEX_REMOVE_PUNCTUATION) sentiments = [] words_and_emoticons = sentitext.words_and_emoticons for item in words_and_emoticons: # Traverse each takens as 'item' valence = 0 i = words_and_emoticons.index(item) # if we need index, why not &quot;for i, item in enumerate(words_and_emoticons):&quot; if ( i &lt; len(words_and_emoticons) - 1 and item.lower() == &quot;kind&quot; and words_and_emoticons[i + 1].lower() == &quot;of&quot; ) or item.lower() in self.constants.BOOSTER_DICT: # if exists &quot;kind of&quot; or current item is a &quot;booster word&quot; # current item (token) sentiment is 0 sentiments.append(valence) continue sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments) sentiments = self._but_check(words_and_emoticons, sentiments) return self.score_valence(sentiments, text) sentitext is a new instance of the class &quot;SentiText&quot;, which is a small class used to Identify sentiment-relevant string-level properties of input text and Clean the text. words_and_emoticons is a clean list of tokens without symbols, but emojis tokens are still retained. (This is also the significance of the existence of the _words_plus_punc() method when you call the _words_and_emoticons() in the SentiText class) Then we start to traverse the clean words_and_emoticons list, and then judge the sentiment value of each token in turn. If current word is &quot;kind&quot; and next word is &quot;of&quot; or the word is a booster word, the sentiment of the current word is 0. If the above special conditions do not exist, we continue to call another method sentiment_valence(valence, sentitext, item, i, sentiments). So far, the valence is 0; item is the current word; i is the index of current word in the text; sentiments is a list of sentiment values up to the previous word. When we get the list of sentiment values for all words, we call a but check method and update a new list of the sentiments. Finally, we return an answer generated by method score_valence(). So now we need to know what method sentiment_valence() and method score_valence() are Remember: we don't remove all stop words in this situation. And before calculating sentiment value, this method has already remove all puctuations in the text and all words with 1 letter. 2. Method &quot;sentiment_valence()&quot;: def sentiment_valence(self, valence, sentitext, item, i, sentiments): is_cap_diff = sentitext.is_cap_diff words_and_emoticons = sentitext.words_and_emoticons item_lowercase = item.lower() if item_lowercase in self.lexicon: # get the sentiment valence valence = self.lexicon[item_lowercase] # check if sentiment laden word is in ALL CAPS (while others aren't) if item.isupper() and is_cap_diff: if valence &gt; 0: valence += self.constants.C_INCR else: valence -= self.constants.C_INCR for start_i in range(0, 3): if ( i &gt; start_i and words_and_emoticons[i - (start_i + 1)].lower() not in self.lexicon ): # dampen the scalar modifier of preceding words and emoticons # (excluding the ones that immediately preceed the item) based # on their distance from the current item. s = self.constants.scalar_inc_dec( words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff ) if start_i == 1 and s != 0: s = s * 0.95 if start_i == 2 and s != 0: s = s * 0.9 valence = valence + s valence = self._never_check( valence, words_and_emoticons, start_i, i ) if start_i == 2: valence = self._idioms_check(valence, words_and_emoticons, i) # future work: consider other sentiment-laden idioms # other_idioms = # {&quot;back handed&quot;: -2, &quot;blow smoke&quot;: -2, &quot;blowing smoke&quot;: -2, # &quot;upper hand&quot;: 1, &quot;break a leg&quot;: 2, # &quot;cooking with gas&quot;: 2, &quot;in the black&quot;: 2, &quot;in the red&quot;: -2, # &quot;on the ball&quot;: 2,&quot;under the weather&quot;: -2} valence = self._least_check(valence, words_and_emoticons, i) sentiments.append(valence) return sentiments Let's review the code when we are calling this method in the polarity_scores(). sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments) &quot;valence&quot; is 0; &quot;item&quot; is the current word; &quot;i&quot; is the index of current word in the text; &quot;sentiments&quot; is a list of sentiment values up to the previous word. Keep going on... is_cap_diff is a booling value. If it is True, the word list is a mixture of all uppercase words and ordinary words, such as ['HELLO','world']. However, if there is &quot;no total-uppercase words in the list&quot; or &quot;all words are total-uppercase, which leads to no emphasis on some individual words&quot;, it'll be False, such as ['Hello','World] or ['HELLO','WORLD']. words_and_emoticons is the clean token list of the text without puctuations. item_lowercase is the lowercase form of the current word. If the item_lowercase is not in lexicon (database), the valence of the current word is 0. If it is in our lexicon, then we can get the valence from our lexicon database. Then depending on the context, we will also slightly adjust its value. If the current word is total-uppercase and is_cap_diff is True, current word is a emphasis in the text, we need to add or substract a constant value C_INCR based on its valence. Next &quot;For&quot; loop is interesting, it updates the valence of the current word again based on the previous three words! Loop three times to see if the three words in front of the current word are in the lexicon database. Because all booster words are not in lexicon database, but in BOOSTER_DICT variable. scalar_inc_dec() method is to check if the preceding words increase, decrease, or negate/nullify the valence. (Check booster word, uppercase emphasis and currenr word's valence) _never_check(): Detection and corresponding schema, if &quot;never&quot; exists on previous three words. _idioms_check(): Detection and corresponding schema, if context 3 words make up an idiom. _least_check() Summary Step: Get basic valence of the current word from the lexicon database 1st update on valence (uppercase emphsis): if the current word is uppercased compared with other words, it is an emphsis word. Add or substract an scalar value C_INCR based on positive or negtive current valence. 2nd update on valence (context booster): if the previous three words exists booster words, which are included in BOOSTER_DICT, it is also an emphsis to current word. Calculate a scalar value and add or substract to current valence. Rule 1: The booster word also applies to uppercase emphsis rule. Rule 2: Neighboring word has 100% influence. Booster word that is two words distance away from the current has 95% influence. Similarly, three words distance has 90% influence. 3rd update on valence (context never) 4th update on valence (context idioms) 5th update on valence (context least) Append lastest current word's valence to the sentiments list. Remember: There are only 7502 token in the lexicon database.If the word in the text is not included in the lexicon database, it doesn't have valence (or sentiment / it is a nerual word). But these words may affect subsequent words' valence. 3. Method &quot;score_valence()&quot;: def score_valence(self, sentiments, text): if sentiments: sum_s = float(sum(sentiments)) # compute and add emphasis from punctuation in text punct_emph_amplifier = self._punctuation_emphasis(sum_s, text) if sum_s &gt; 0: sum_s += punct_emph_amplifier elif sum_s &lt; 0: sum_s -= punct_emph_amplifier compound = self.constants.normalize(sum_s) # discriminate between positive, negative and neutral sentiment scores pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments) if pos_sum &gt; math.fabs(neg_sum): pos_sum += punct_emph_amplifier elif pos_sum &lt; math.fabs(neg_sum): neg_sum -= punct_emph_amplifier total = pos_sum + math.fabs(neg_sum) + neu_count pos = math.fabs(pos_sum / total) neg = math.fabs(neg_sum / total) neu = math.fabs(neu_count / total) else: compound = 0.0 pos = 0.0 neg = 0.0 neu = 0.0 sentiment_dict = { &quot;neg&quot;: round(neg, 3), &quot;neu&quot;: round(neu, 3), &quot;pos&quot;: round(pos, 3), &quot;compound&quot;: round(compound, 4), } return sentiment_dict If sentiments is not None: sum_s is the sum of all word valence in the sentiments list. punct_emph_amplifier is also a scalar that used to update sum_s value. It is based on counts of &quot;!&quot; and &quot;?&quot;. According to the overall sentiment value, add or substract punct_emph_amplifier value to the sum_s. compound is the normalize of sum_s. compound = sum_s/sqrt(sum_s^2+15) def normalize(self, score, alpha=15): &quot;&quot;&quot; Normalize the score to be between -1 and 1 using an alpha that approximates the max expected value &quot;&quot;&quot; norm_score = score / math.sqrt((score * score) + alpha) return norm_score _sift_sentiment_scores() def _sift_sentiment_scores(self, sentiments): # want separate positive versus negative sentiment scores pos_sum = 0.0 neg_sum = 0.0 neu_count = 0 for sentiment_score in sentiments: if sentiment_score &gt; 0: pos_sum += ( float(sentiment_score) + 1 ) # compensates for neutral words that are counted as 1 if sentiment_score &lt; 0: neg_sum += ( float(sentiment_score) - 1 ) # when used with math.fabs(), compensates for neutrals if sentiment_score == 0: neu_count += 1 return pos_sum, neg_sum, neu_count _sift_sentiment_scores() counts the number of pos, neg and neu words. And for pos words and neg words, their latest valences are also added into pos_sum and neg_sum. pos_sum, neg_sum and neu_count also need to be update based on punct_emph_amplifier. The sum of pos, neg and neu is one. Class &quot;SentiText&quot; class SentiText: &quot;&quot;&quot; Identify sentiment-relevant string-level properties of input text. &quot;&quot;&quot; def __init__(self, text, punc_list, regex_remove_punctuation): if not isinstance(text, str): text = str(text.encode(&quot;utf-8&quot;)) self.text = text self.PUNC_LIST = punc_list self.REGEX_REMOVE_PUNCTUATION = regex_remove_punctuation self.words_and_emoticons = self._words_and_emoticons() # doesn't separate words from # adjacent punctuation (keeps emoticons &amp; contractions) self.is_cap_diff = self.allcap_differential(self.words_and_emoticons) def _words_plus_punc(self): &quot;&quot;&quot; Returns mapping of form: { 'cat,': 'cat', ',cat': 'cat', } &quot;&quot;&quot; no_punc_text = self.REGEX_REMOVE_PUNCTUATION.sub(&quot;&quot;, self.text) # removes punctuation (but loses emoticons &amp; contractions) words_only = no_punc_text.split() # remove singletons words_only = set(w for w in words_only if len(w) &gt; 1) # the product gives ('cat', ',') and (',', 'cat') punc_before = {&quot;&quot;.join(p): p[1] for p in product(self.PUNC_LIST, words_only)} punc_after = {&quot;&quot;.join(p): p[0] for p in product(words_only, self.PUNC_LIST)} words_punc_dict = punc_before words_punc_dict.update(punc_after) return words_punc_dict def _words_and_emoticons(self): &quot;&quot;&quot; Removes leading and trailing puncutation Leaves contractions and most emoticons Does not preserve punc-plus-letter emoticons (e.g. :D) &quot;&quot;&quot; wes = self.text.split() words_punc_dict = self._words_plus_punc() wes = [we for we in wes if len(we) &gt; 1] for i, we in enumerate(wes): if we in words_punc_dict: wes[i] = words_punc_dict[we] return wes def allcap_differential(self, words): &quot;&quot;&quot; Check whether just some words in the input are ALL CAPS :param list words: The words to inspect :returns: `True` if some but not all items in `words` are ALL CAPS &quot;&quot;&quot; is_different = False allcap_words = 0 for word in words: if word.isupper(): allcap_words += 1 cap_differential = len(words) - allcap_words if 0 &lt; cap_differential &lt; len(words): is_different = True return is_different Class &quot;VaderConstants&quot; class VaderConstants: &quot;&quot;&quot; A class to keep the Vader lists and constants. &quot;&quot;&quot; ##Constants## # (empirically derived mean sentiment intensity rating increase for booster words) B_INCR = 0.293 B_DECR = -0.293 # (empirically derived mean sentiment intensity rating increase for using # ALLCAPs to emphasize a word) C_INCR = 0.733 N_SCALAR = -0.74 NEGATE = { &quot;aint&quot;, &quot;arent&quot;, &quot;cannot&quot;, &quot;cant&quot;, &quot;couldnt&quot;, &quot;darent&quot;, ..., } # booster/dampener 'intensifiers' or 'degree adverbs' # http://en.wiktionary.org/wiki/Category:English_degree_adverbs BOOSTER_DICT = { &quot;absolutely&quot;: B_INCR, &quot;amazingly&quot;: B_INCR, &quot;awfully&quot;: B_INCR, &quot;completely&quot;: B_INCR, &quot;considerably&quot;: B_INCR, &quot;decidedly&quot;: B_INCR, ... &quot;very&quot;: B_INCR, &quot;almost&quot;: B_DECR, &quot;barely&quot;: B_DECR, &quot;kind of&quot;: B_DECR, ... &quot;sort of&quot;: B_DECR, &quot;sorta&quot;: B_DECR, &quot;sortof&quot;: B_DECR, &quot;sort-of&quot;: B_DECR, } # check for special case idioms using a sentiment-laden keyword known to SAGE SPECIAL_CASE_IDIOMS = { &quot;the shit&quot;: 3, &quot;the bomb&quot;: 3, &quot;bad ass&quot;: 1.5, &quot;yeah right&quot;: -2, &quot;cut the mustard&quot;: 2, &quot;kiss of death&quot;: -1.5, &quot;hand to mouth&quot;: -2, } # for removing punctuation REGEX_REMOVE_PUNCTUATION = re.compile(&quot;[{0}]&quot;.format(re.escape(string.punctuation))) PUNC_LIST = [ &quot;.&quot;, &quot;!&quot;, &quot;?&quot;, &quot;,&quot;, &quot;;&quot;, &quot;:&quot;, &quot;-&quot;, &quot;'&quot;, '&quot;', &quot;!!&quot;, &quot;!!!&quot;, &quot;??&quot;, &quot;???&quot;, &quot;?!?&quot;, &quot;!?!&quot;, &quot;?!?!&quot;, &quot;!?!?&quot;, ] def __init__(self): pass def negated(self, input_words, include_nt=True): &quot;&quot;&quot; Determine if input contains negation words &quot;&quot;&quot; neg_words = self.NEGATE if any(word.lower() in neg_words for word in input_words): return True if include_nt: if any(&quot;n't&quot; in word.lower() for word in input_words): return True for first, second in pairwise(input_words): if second.lower() == &quot;least&quot; and first.lower() != &quot;at&quot;: return True return False def normalize(self, score, alpha=15): &quot;&quot;&quot; Normalize the score to be between -1 and 1 using an alpha that approximates the max expected value &quot;&quot;&quot; norm_score = score / math.sqrt((score * score) + alpha) return norm_score def scalar_inc_dec(self, word, valence, is_cap_diff): &quot;&quot;&quot; Check if the preceding words increase, decrease, or negate/nullify the valence &quot;&quot;&quot; scalar = 0.0 word_lower = word.lower() if word_lower in self.BOOSTER_DICT: scalar = self.BOOSTER_DICT[word_lower] if valence &lt; 0: scalar *= -1 # check if booster/dampener word is in ALLCAPS (while others aren't) if word.isupper() and is_cap_diff: if valence &gt; 0: scalar += self.C_INCR else: scalar -= self.C_INCR return scalar Database &quot;vader_lexicon.txt&quot; We can take a quick look at the contents of the file &quot;vader_lexicon.txt&quot;: $: -1.5 0.80623 [-1, -1, -1, -1, -3, -1, -3, -1, -2, -1] %) -0.4 1.0198 [-1, 0, -1, 0, 0, -2, -1, 2, -1, 0] %-) -1.5 1.43178 [-2, 0, -2, -2, -1, 2, -2, -3, -2, -3] &amp;-: -0.4 1.42829 [-3, -1, 0, 0, -1, -1, -1, 2, -1, 2] ... adorableness 2.5 0.67082 [2, 3, 3, 2, 3, 2, 1, 3, 3, 3] adorably 2.1 0.7 [3, 1, 2, 3, 2, 2, 1, 3, 2, 2] adoration 2.9 0.7 [3, 3, 3, 2, 3, 3, 4, 2, 4, 2] adorations 2.2 0.87178 [2, 2, 3, 1, 3, 1, 3, 3, 1, 3] ... The first column is words and emoticons, the second column is sentiment value. In class SentimentIntensityAnalyzer, method &quot;make_lex_dict()&quot; only uses the first two columns: word name and sentiment value. What does 3rd column and 4th column represent? How does these valence value come out? ","link":"https://zl-wu.github.io/post/text-analysis-nltks-vader-library/"},{"title":"Text Analysis -- TextBlob sentiment","content":"&quot;Topic: How to calculate sentiment value in TextBlob library?&quot; Topic: How to calculate sentiment value in TextBlob library? A Simple Example Code from textblob import TextBlob TextBlob(text).sentiment[0] Class TextBlob(BaseBlob) source code # textblob/blob.py class TextBlob(BaseBlob): &quot;&quot;&quot;A general text block, meant for larger bodies of text (esp. those containing sentences). Inherits from :class:`BaseBlob &lt;BaseBlob&gt;`. :param str text: A string. :param tokenizer: (optional) A tokenizer instance. If ``None``, defaults to :class:`WordTokenizer() &lt;textblob.tokenizers.WordTokenizer&gt;`. :param np_extractor: (optional) An NPExtractor instance. If ``None``, defaults to :class:`FastNPExtractor() &lt;textblob.en.np_extractors.FastNPExtractor&gt;`. :param pos_tagger: (optional) A Tagger instance. If ``None``, defaults to :class:`NLTKTagger &lt;textblob.en.taggers.NLTKTagger&gt;`. :param analyzer: (optional) A sentiment analyzer. If ``None``, defaults to :class:`PatternAnalyzer &lt;textblob.en.sentiments.PatternAnalyzer&gt;`. :param classifier: (optional) A classifier. &quot;&quot;&quot; @cached_property def sentences(self): &quot;&quot;&quot;Return list of :class:`Sentence &lt;Sentence&gt;` objects.&quot;&quot;&quot; return self._create_sentence_objects() @cached_property def words(self): &quot;&quot;&quot;Return a list of word tokens. This excludes punctuation characters. If you want to include punctuation characters, access the ``tokens`` property. :returns: A :class:`WordList &lt;WordList&gt;` of word tokens. &quot;&quot;&quot; return WordList(word_tokenize(self.raw, include_punc=False)) @property def raw_sentences(self): &quot;&quot;&quot;List of strings, the raw sentences in the blob.&quot;&quot;&quot; return [sentence.raw for sentence in self.sentences] @property def serialized(self): &quot;&quot;&quot;Returns a list of each sentence's dict representation.&quot;&quot;&quot; return [sentence.dict for sentence in self.sentences] def to_json(self, *args, **kwargs): '''Return a json representation (str) of this blob. Takes the same arguments as json.dumps. .. versionadded:: 0.5.1 ''' return json.dumps(self.serialized, *args, **kwargs) @property def json(self): '''The json representation of this blob. .. versionchanged:: 0.5.1 Made ``json`` a property instead of a method to restore backwards compatibility that was broken after version 0.4.0. ''' return self.to_json() def _create_sentence_objects(self): '''Returns a list of Sentence objects from the raw text. ''' sentence_objects = [] sentences = sent_tokenize(self.raw) char_index = 0 # Keeps track of character index within the blob for sent in sentences: # Compute the start and end indices of the sentence # within the blob start_index = self.raw.index(sent, char_index) char_index += len(sent) end_index = start_index + len(sent) # Sentences share the same models as their parent blob s = Sentence(sent, start_index=start_index, end_index=end_index, tokenizer=self.tokenizer, np_extractor=self.np_extractor, pos_tagger=self.pos_tagger, analyzer=self.analyzer, parser=self.parser, classifier=self.classifier) sentence_objects.append(s) return sentence_objects There is not attribute or method named &quot;sentiment&quot; in the class TextBlob, but we can see that it inherits class &quot;BaseBlob&quot; and has all the properties and methods of class BaseBlob. Therefore, we need to continue to search for BaseBlob content. Fortunately, we quickly found traces of sentiment. Class BaseBlob(StringlikeMixin, BlobComparableMixin) source code # textblob/blob.py class BaseBlob(StringlikeMixin, BlobComparableMixin): &quot;&quot;&quot;An abstract base class that all textblob classes will inherit from. Includes words, POS tag, NP, and word count properties. Also includes basic dunder and string methods for making objects like Python strings. .... &quot;&quot;&quot; np_extractor = FastNPExtractor() pos_tagger = NLTKTagger() tokenizer = WordTokenizer() translator = Translator() analyzer = PatternAnalyzer() parser = PatternParser() def __init__(self, text, tokenizer=None, ... ... @cached_property def sentiment(self): &quot;&quot;&quot;Return a tuple of form (polarity, subjectivity ) where polarity is a float within the range [-1.0, 1.0] and subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity)`` &quot;&quot;&quot; return self.analyzer.analyze(self.raw) ... It declares the method sentiment as a property, which is convenient for users to call. And this method returns self.analyzer.analyze(self.raw), which is PatternAnalyzer().analyze(self.raw). Now what is PatternAnalyzer class? Look at the statement at the beginning of the document &quot;from textblob.sentiments import PatternAnalyzer&quot;, let's keep track of this class Class PatternAnalyzer(BaseSentimentAnalyzer) source code The path of this file: textblob/en/sentiments.py # textblob/en/sentiments.py class PatternAnalyzer(BaseSentimentAnalyzer): &quot;&quot;&quot;Sentiment analyzer that uses the same implementation as the pattern library. Returns results as a named tuple of the form: ``Sentiment(polarity, subjectivity, [assessments])`` where [assessments] is a list of the assessed tokens and their polarity and subjectivity scores &quot;&quot;&quot; kind = CONTINUOUS # This is only here for backwards-compatibility. # The return type is actually determined upon calling analyze() RETURN_TYPE = namedtuple('Sentiment', ['polarity', 'subjectivity']) def analyze(self, text, keep_assessments=False): &quot;&quot;&quot;Return the sentiment as a named tuple of the form: ``Sentiment(polarity, subjectivity, [assessments])``. &quot;&quot;&quot; #: Return type declaration if keep_assessments: Sentiment = namedtuple('Sentiment', ['polarity', 'subjectivity', 'assessments']) assessments = pattern_sentiment(text).assessments polarity, subjectivity = pattern_sentiment(text) return Sentiment(polarity, subjectivity, assessments) else: Sentiment = namedtuple('Sentiment', ['polarity', 'subjectivity']) return Sentiment(*pattern_sentiment(text)) Therefore, what is algorithm behind this two final commands? from collections import namedtuple from textblob.en import sentiment as pattern_sentiment Sentiment = namedtuple('Sentiment', ['polarity', 'subjectivity']) return Sentiment(*pattern_sentiment(text)) Google what is namedtuple if you don't know about it. So far, Sentiment is a empty tuple, and it is this statement &quot;Sentiment(*pattern_sentiment(text))&quot; that gives this tuple the correct value. pattern_sentiment is the alias of sentiment, which is in textblob.en.__init__.py type(pattern_sentiment) # textblob.en.Sentiment len(pattern_sentiment) # 2860 for k,v in pattern_sentiment.items(): print(k, v) ''' Output: ... absolute {'JJ': [0.2, 0.9, 1.0], None: [0.2, 0.9, 1.0]} absorbed {'JJ': [0.3, 0.9, 1.0], None: [0.3, 0.9, 1.0]} absorbing {'JJ': [0.2, 0.95, 1.0], None: [0.2, 0.95, 1.0]} absurd {'JJ': [-0.5, 1.0, 1.0], None: [-0.5, 1.0, 1.0]} ... ''' # textblob.en.__init__.py sentiment = Sentiment( path = os.path.join(MODULE, &quot;en-sentiment.xml&quot;), synset = &quot;wordnet_id&quot;, negations = (&quot;no&quot;, &quot;not&quot;, &quot;n't&quot;, &quot;never&quot;), modifiers = (&quot;RB&quot;,), modifier = lambda w: w.endswith(&quot;ly&quot;), tokenizer = parser.find_tokens, language = &quot;en&quot; ) Class Sentiment(lazydict) source code This class is in textblob/_text.py # textblob/_text.py ### SENTIMENT POLARITY LEXICON ##################################################################### # A sentiment lexicon can be used to discern objective facts from subjective opinions in text. # Each word in the lexicon has scores for: # 1) polarity: negative vs. positive (-1.0 =&gt; +1.0) # 2) subjectivity: objective vs. subjective (+0.0 =&gt; +1.0) # 3) intensity: modifies next word? (x0.5 =&gt; x2.0) # For English, adverbs are used as modifiers (e.g., &quot;very good&quot;). # For Dutch, adverbial adjectives are used as modifiers # (&quot;hopeloos voorspelbaar&quot;, &quot;ontzettend spannend&quot;, &quot;verschrikkelijk goed&quot;). # Negation words (e.g., &quot;not&quot;) reverse the polarity of the following word. # Sentiment()(txt) returns an averaged (polarity, subjectivity)-tuple. # Sentiment().assessments(txt) returns a list of (chunk, polarity, subjectivity, label)-tuples. # Semantic labels are useful for fine-grained analysis, e.g., # negative words + positive emoticons could indicate cynicism. class Sentiment(lazydict): def __init__(self, path=&quot;&quot;, language=None, synset=None, confidence=None, **kwargs): &quot;&quot;&quot; A dictionary of words (adjectives) and polarity scores (positive/negative). The value for each word is a dictionary of part-of-speech tags. The value for each word POS-tag is a tuple with values for polarity (-1.0-1.0), subjectivity (0.0-1.0) and intensity (0.5-2.0). &quot;&quot;&quot; .... ... def __call__(self, s, negation=True, **kwargs): &quot;&quot;&quot; Returns a (polarity, subjectivity)-tuple for the given sentence, with polarity between -1.0 and 1.0 and subjectivity between 0.0 and 1.0. The sentence can be a string, Synset, Text, Sentence, Chunk, Word, Document, Vector. An optional weight parameter can be given, as a function that takes a list of words and returns a weight. &quot;&quot;&quot; def avg(assessments, weighted=lambda w: 1): s, n = 0, 0 for words, score in assessments: w = weighted(words) s += w * score n += w return s / float(n or 1) # A pattern.en.wordnet.Synset. # Sentiment(synsets(&quot;horrible&quot;, &quot;JJ&quot;)[0]) =&gt; (-0.6, 1.0) if hasattr(s, &quot;gloss&quot;): a = [(s.synonyms[0],) + self.synset(s.id, pos=s.pos) + (None,)] # A synset id. # Sentiment(&quot;a-00193480&quot;) =&gt; horrible =&gt; (-0.6, 1.0) (English WordNet) # Sentiment(&quot;c_267&quot;) =&gt; verschrikkelijk =&gt; (-0.9, 1.0) (Dutch Cornetto) elif isinstance(s, basestring) and RE_SYNSET.match(s) and hasattr(s, &quot;synonyms&quot;): a = [(s.synonyms[0],) + self.synset(s.id, pos=s.pos) + (None,)] # A string of words. # Sentiment(&quot;a horrible movie&quot;) =&gt; (-0.6, 1.0) elif isinstance(s, basestring): a = self.assessments(((w.lower(), None) for w in &quot; &quot;.join(self.tokenizer(s)).split()), negation) # A pattern.en.Text. elif hasattr(s, &quot;sentences&quot;): a = self.assessments(((w.lemma or w.string.lower(), w.pos[:2]) for w in chain(*s)), negation) # A pattern.en.Sentence or pattern.en.Chunk. elif hasattr(s, &quot;lemmata&quot;): a = self.assessments(((w.lemma or w.string.lower(), w.pos[:2]) for w in s.words), negation) # A pattern.en.Word. elif hasattr(s, &quot;lemma&quot;): a = self.assessments(((s.lemma or s.string.lower(), s.pos[:2]),), negation) # A pattern.vector.Document. # Average score = weighted average using feature weights. # Bag-of words is unordered: inject None between each two words # to stop assessments() from scanning for preceding negation &amp; modifiers. elif hasattr(s, &quot;terms&quot;): a = self.assessments(chain(*(((w, None), (None, None)) for w in s)), negation) kwargs.setdefault(&quot;weight&quot;, lambda w: s.terms[w[0]]) # A dict of (word, weight)-items. elif isinstance(s, dict): a = self.assessments(chain(*(((w, None), (None, None)) for w in s)), negation) kwargs.setdefault(&quot;weight&quot;, lambda w: s[w[0]]) # A list of words. elif isinstance(s, list): a = self.assessments(((w, None) for w in s), negation) else: a = [] weight = kwargs.get(&quot;weight&quot;, lambda w: 1) # [(w, p) for w, p, s, x in a] return Score(polarity = avg( [(w, p) for w, p, s, x in a], weight ), subjectivity = avg([(w, s) for w, p, s, x in a], weight), assessments = a) def assessments(self, words=[], negation=True): &quot;&quot;&quot; Returns a list of (chunk, polarity, subjectivity, label)-tuples for the given list of words: where chunk is a list of successive words: a known word optionally preceded by a modifier (&quot;very good&quot;) or a negation (&quot;not good&quot;). &quot;&quot;&quot; a = [] m = None # Preceding modifier (i.e., adverb or adjective). n = None # Preceding negation (e.g., &quot;not beautiful&quot;). for w, pos in words: # Only assess known words, preferably by part-of-speech tag. # Including unknown words (polarity 0.0 and subjectivity 0.0) lowers the average. if w is None: continue if w in self and pos in self[w]: p, s, i = self[w][pos] # Known word not preceded by a modifier (&quot;good&quot;). if m is None: a.append(dict(w=[w], p=p, s=s, i=i, n=1, x=self.labeler.get(w))) # Known word preceded by a modifier (&quot;really good&quot;). if m is not None: a[-1][&quot;w&quot;].append(w) a[-1][&quot;p&quot;] = max(-1.0, min(p * a[-1][&quot;i&quot;], +1.0)) a[-1][&quot;s&quot;] = max(-1.0, min(s * a[-1][&quot;i&quot;], +1.0)) a[-1][&quot;i&quot;] = i a[-1][&quot;x&quot;] = self.labeler.get(w) # Known word preceded by a negation (&quot;not really good&quot;). if n is not None: a[-1][&quot;w&quot;].insert(0, n) a[-1][&quot;i&quot;] = 1.0 / a[-1][&quot;i&quot;] a[-1][&quot;n&quot;] = -1 # Known word may be a negation. # Known word may be modifying the next word (i.e., it is a known adverb). m = None n = None if pos and pos in self.modifiers or any(map(self[w].__contains__, self.modifiers)): m = (w, pos) if negation and w in self.negations: n = w else: # Unknown word may be a negation (&quot;not good&quot;). if negation and w in self.negations: n = w # Unknown word. Retain negation across small words (&quot;not a good&quot;). elif n and len(w.strip(&quot;'&quot;)) &gt; 1: n = None # Unknown word may be a negation preceded by a modifier (&quot;really not good&quot;). if n is not None and m is not None and (pos in self.modifiers or self.modifier(m[0])): a[-1][&quot;w&quot;].append(n) a[-1][&quot;n&quot;] = -1 n = None # Unknown word. Retain modifier across small words (&quot;really is a good&quot;). elif m and len(w) &gt; 2: m = None # Exclamation marks boost previous word. if w == &quot;!&quot; and len(a) &gt; 0: a[-1][&quot;w&quot;].append(&quot;!&quot;) a[-1][&quot;p&quot;] = max(-1.0, min(a[-1][&quot;p&quot;] * 1.25, +1.0)) # Exclamation marks in parentheses indicate sarcasm. if w == &quot;(!)&quot;: a.append(dict(w=[w], p=0.0, s=1.0, i=1.0, n=1, x=IRONY)) # EMOTICONS: {(&quot;grin&quot;, +1.0): set((&quot;:-D&quot;, &quot;:D&quot;))} if w.isalpha() is False and len(w) &lt;= 5 and w not in PUNCTUATION: # speedup for (type, p), e in EMOTICONS.items(): if w in imap(lambda e: e.lower(), e): a.append(dict(w=[w], p=p, s=1.0, i=1.0, n=1, x=MOOD)) break for i in range(len(a)): w = a[i][&quot;w&quot;] p = a[i][&quot;p&quot;] s = a[i][&quot;s&quot;] n = a[i][&quot;n&quot;] x = a[i][&quot;x&quot;] # &quot;not good&quot; = slightly bad, &quot;not bad&quot; = slightly good. a[i] = (w, p * -0.5 if n &lt; 0 else p, s, x) return a def annotate(self, word, pos=None, polarity=0.0, subjectivity=0.0, intensity=1.0, label=None): ... To be simply, the results returned from assessments method looks like: from textblob.en import sentiment as pattern_sentiment pattern_sentiment(text).assessments ''' Output: [(['accomplished'], 0.2, 0.5, None), (['internal'], 0.0, 0.0, None), (['internal'], 0.0, 0.0, None), (['particular'], 0.16666666666666666, 0.3333333333333333, None), (['useful'], 0.3, 0.0, None), (['other'], -0.125, 0.375, None), (['not', 'great'], -0.4, 0.75, None)] # The fist column is chunk word. (w) # The second column is polarity. (p) # The third column is subjectivity. (s) # The fourth column is label of the word. (x) ''' Then we know how __call__ methods works: def __call__(self, s, negation=True, **kwargs): def avg(assessments, weighted=lambda w: 1): s, n = 0, 0 for words, score in assessments: w = weighted(words) s += w * score n += w return s / float(n or 1) .... a = self.assessments(...) # the format of &quot;a&quot; is looks like the output above weight = kwargs.get(&quot;weight&quot;, lambda w: 1) # [(w, p) for w, p, s, x in a] return Score(polarity = avg( [(w, p) for w, p, s, x in a], weight ), subjectivity = avg([(w, s) for w, p, s, x in a], weight), assessments = a) We can easily find out that the whole text polarity and subjectivity are the weight average of polarity and subjectivity for each word. In fact, there are two main contributors that can make textblob's sentiment analysis accurate: Various rules in method assessments() of Sentiment class, which defines how to update the Polarity and Subjectivity values of the current word according to context. lexicon corpus building: en-sentiment.xml (textblob/en/en-sentiment.xml) online source Let's check out some interesting rules of textblob sentiment analysis First, the lexicon corpus contains 2860 words, and they are almost all adjective. But en-sentiment.xml has more records than 2860. Because some words have different polarities and subjectivities in different contexts. For example, there are 12 records belong to &quot;rich&quot;, and when textblob load this corpus, textblob average 12 polarities and 12 subjectivities to be the final polarity and subjectivity value of &quot;rich&quot;. The rules of textblob analysis are mainly in two aspects: modifier (i.e., adverb or adjective). negation (e.g., &quot;not beautiful&quot;) self.negations = kwargs.get(&quot;negations&quot;, (&quot;no&quot;, &quot;not&quot;, &quot;n't&quot;, &quot;never&quot;)) self.modifiers = kwargs.get(&quot;modifiers&quot;, (&quot;RB&quot;,)) self.modifier = kwargs.get(&quot;modifier&quot; , lambda w: w.endswith(&quot;ly&quot;)) If current word is a negation (&quot;no&quot;, &quot;not&quot;, &quot;n't&quot;, &quot;never&quot;): a = [] # result list m = None # Preceding modifier (i.e., adverb or adjective). n = None # Preceding negation (e.g., &quot;not beautiful&quot;). negation = True # default argument ########### Current Word is Known ########### ########### Rule 1 ########### # current Known word not preceded by a modifier (&quot;good&quot;). if m is None: a.append(dict(w=[w], p=p, s=s, i=i, n=1, x=self.labeler.get(w))) # normal append a dictionary ########### Rule 2 ########### # current Known word preceded by a modifier (&quot;really good&quot;). if m is not None: a[-1][&quot;w&quot;].append(w) # append modifier word to the latest word to become one new chunck token a[-1][&quot;p&quot;] = max(-1.0, min(p * a[-1][&quot;i&quot;], +1.0)) # new chunk token's original intensity times current word's polarity (control it between(-1.0, 1.0)) a[-1][&quot;s&quot;] = max(-1.0, min(s * a[-1][&quot;i&quot;], +1.0)) # same operation on the subjectivity a[-1][&quot;i&quot;] = i # i update to the current known word's intensity a[-1][&quot;x&quot;] = self.labeler.get(w) # x update to the current known word's label ########### Rule 3 ############ # current Known word preceded by a negation (&quot;not really good&quot;). if n is not None: a[-1][&quot;w&quot;].insert(0, n) # insert negation word to the latest word to be one chunck token a[-1][&quot;i&quot;] = 1.0 / a[-1][&quot;i&quot;] # the latest word's intensity becomes the original countdown a[-1][&quot;n&quot;] = -1 # set n is -1 ########### Rule 4 ########### # current Known word may be a negation. # current Known word may be modifying the next word (i.e., it is a known adverb). m = None # remove the influence from previous modifier (known or unknown word) n = None # remove the influence from previous negation (known or unknown word) if pos and pos in self.modifiers or any(map(self[w].__contains__, self.modifiers)): # if current known word is a modifier m = (w, pos) # assign (w, pos) tuple to m if negation and w in self.negations: # if current known word is a negation n = w # assign current word to n ########### Current Word is Unknown ########### ########### Rule 5 ############ # current Unknown word may be a negation (&quot;not good&quot;). if negation and w in self.negations: # if current unknown word is a negation n = w # assign current word to n # current Unknown word. Retain negation across small words (&quot;not a good&quot;). elif n and len(w.strip(&quot;'&quot;)) &gt; 1: # if previous word is a negation, and current word is more than on letter n = None # Reset n ########### Rule 6 ############ # Unknown word may be a negation preceded by a modifier (&quot;really not good&quot;). if n is not None and m is not None and (pos in self.modifiers or self.modifier(m[0])): # if modifier + negation a[-1][&quot;w&quot;].append(n) # append negation word to the latest word to become one new chunck token a[-1][&quot;n&quot;] = -1 n = None # reset n # Unknown word. Retain modifier across small words (&quot;really is a good&quot;). elif m and len(w) &gt; 2: m = None # Reset m ########### Rule 7 ############ # Exclamation marks boost previous word. if w == &quot;!&quot; and len(a) &gt; 0: a[-1][&quot;w&quot;].append(&quot;!&quot;) a[-1][&quot;p&quot;] = max(-1.0, min(a[-1][&quot;p&quot;] * 1.25, +1.0)) ########### Rule 8 ############ # Exclamation marks in parentheses indicate sarcasm. if w == &quot;(!)&quot;: a.append(dict(w=[w], p=0.0, s=1.0, i=1.0, n=1, x=IRONY)) ########### Rule 9 ############ # EMOTICONS: {(&quot;grin&quot;, +1.0): set((&quot;:-D&quot;, &quot;:D&quot;))} if w.isalpha() is False and len(w) &lt;= 5 and w not in PUNCTUATION: # speedup for (type, p), e in EMOTICONS.items(): if w in imap(lambda e: e.lower(), e): a.append(dict(w=[w], p=p, s=1.0, i=1.0, n=1, x=MOOD)) break st=&gt;start: Assessment:&gt;https://github.com/sloria/TextBlob/blob/e6cd9791ae42e37b5a2132676f9ca69340e8d8c0/textblob/_text.py#L854 e=&gt;end: Return a :&gt;http://www.google.com io1=&gt;inputoutput: Initialize a=[ ], m=None, n=None cond=&gt;condition: word in lexicon? &lt;!-- op1=&gt;operation: Initialize a, m, n --&gt; &lt;!-- sub1=&gt;subroutine: My Subroutine --&gt; op1=&gt;operation: Get word's p,s,i rule1=&gt;condition: m == None? r1yop=&gt;operation: append word dict to &quot;a&quot; r1nop=&gt;operation: a[-1][&quot;w&quot;].append(word), update p,s,i rule2=&gt;condition: n == None? r2nop=&gt;operation: a[-1][&quot;w&quot;].insert(0,n), update i, n op4=&gt;operation: reset m,n = None,None rule3=&gt;condition: word is modifier? r3yop=&gt;operation: m = w rule4=&gt;condition: word is negation? r4yop=&gt;operation: n = w io=&gt;inputoutput: catch something... para=&gt;parallel: parallel tasks st-&gt;io1-&gt;cond cond(yes)-&gt;op1-&gt;rule1 rule1(yes)-&gt;r1yop-&gt;rule2 rule1(no)-&gt;r1nop-&gt;rule2 rule2(yes)-&gt;op4 rule2(no)-&gt;r2nop-&gt;op4 op4-&gt;rule3 rule3(yes)-&gt;r3yop-&gt;rule4 rule3(no)-&gt;rule4 rule4(yes)-&gt;r4yop-&gt;e rule4(no)-&gt;e &lt;!-- cond(no)-&gt;para --&gt; &lt;!-- para(path1, bottom)-&gt;sub1(right)-&gt;op1 --&gt; &lt;!-- para(path2, top)-&gt;op1 --&gt; 'abc'.isalpha() ==&gt; True 'abc1'.isalpha() ==&gt; False 'abc:'.isalpha() ==&gt; False Sentiment.call() support lots of format ","link":"https://zl-wu.github.io/post/text-analysis-textblob-sentiment/"},{"title":"Git 幼儿园入门","content":"&quot;这只是一个 Git 新手基础入门（幼儿园级别）&quot; Git Mind map: VCS（版本管理系统）发展 服务器文档式（VCS出现之前） 集中式VCS 分布式VCS （Git [Linux支付创造] ） Git 的特点 最优的储存能力 非凡的性能 开源 很容易做备份 支持离线操作 很容易定制工作流程 DevOps 工具[全流程生命周期] DevOps:持续交付实践 Git GitHub GitLab [社区版本] 安装Git https://git-scm.com/download/mac https://git-scm.com/book/zh/v2 [书] 配置USER 配置 user.name 和 user.email $ git config --global user.name 'your_name' $ git config --global user.email 'your_email@domain.com' Why git need these? 代码的每一次变更时，时间点与变更人 都是与变更信息捆绑在一起的。在Code Review时，每一次变更信息都带上了用户的Email地址，评审人员指出该用户的哪个文件有问题时，Git管控的Web系统会取出变更者的Email并发送邮件。必须配置！！ --global有什么用呢？ 是什么意思呢？ config的三个作用域 config 可以配置许多仓库属性 $ git config --local $ git config --global $ git config --system --local : 只对某个仓库有效 [登录到某一个仓库 再设置该仓库] --global: 对当前用户的所有仓库有效 [如登录Mac系统的用户Wu的所有10个仓库，因此变更人信息设置常用global] --system: 对系统的所有登录用户有效 [不常用] 显示config的配置，加list $ git config --list --local fatal: --local can only be used inside a git repository $ git config --list --global filter.lfs.clean=git-lfs clean -- %f filter.lfs.smudge=git-lfs smudge -- %f filter.lfs.process=git-lfs filter-process filter.lfs.required=true user.email=wzhenglong@yahoo.com user.name=zhenglong $ git config --list --system fatal: unable to read config file '/etc/gitconfig': No such file or directory 注意：--global 和 --list的位置可以对调，无影响。 建立Git仓库 两种场景： 把已有项目代码纳入Git管理 $ cd 项目代码所在的文件夹 $ git init 新建的项目直接用Git管理 $ cd 某个文件夹 $ git init your_project #在当前路劲下创建和项目名称同名的文件夹 # 该文件夹内会建立一个裸仓库，以.git后缀 $ cd your_project 演示 $ git init git_learning Initialized empty Git repository in /Users/zhenglongwu/Desktop/Programming/Git/git_learning/.git/ $ cd git_learning $ ls -al total 0 drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 16:02 . drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 16:02 .. drwxr-xr-x 9 zhenglongwu staff 288 Sep 12 16:02 .git $ cd .git $ ls -al total 24 drwxr-xr-x 9 zhenglongwu staff 288 Sep 12 16:02 . drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 16:02 .. -rw-r--r-- 1 zhenglongwu staff 23 Sep 12 16:02 HEAD -rw-r--r-- 1 zhenglongwu staff 137 Sep 12 16:02 config -rw-r--r-- 1 zhenglongwu staff 73 Sep 12 16:02 description drwxr-xr-x 13 zhenglongwu staff 416 Sep 12 16:02 hooks drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 16:02 info drwxr-xr-x 4 zhenglongwu staff 128 Sep 12 16:02 objects drwxr-xr-x 4 zhenglongwu staff 128 Sep 12 16:02 refs 比较局部配置与全局配置 $ git config --list --global user.email=wzhenglong@yahoo.com user.name=zhenglong # 局部配置 $ git config --local user.name 'user1' $ git config --local user.email '670362192@qq.com' $ git config --local --list user.name=user1 user.email=670362192@qq.com # 基本Git操作 # cp cp [-R [-H | -L | -P]] [-fi | -n] [-apvXc] source_file ... target_directory # cp [-R [-H | -L | -P]] [-fi | -n] [-apvXc] source_file target_file $ cp ../readme.txt . $ git commit -m'Add readme' On branch master Initial commit Untracked files: readme.txt nothing added to commit but untracked files present ## git commit报错 因为没有跟踪该文件。我们需要先跟踪该文件 $ git add readme.txt $ git status On branch master No commits yet Changes to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: readme.txt $ git commit -m'Add readme' [master (root-commit) 61841ea] Add readme 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 readme.txt $ git log commit 61841eab81f0c26cfb9a884f4617169c0741c1fd (HEAD -&gt; master) Author: user1 &lt;670362192@qq.com&gt; Date: Thu Sep 12 19:14:42 2019 -0400 Add readme git add filename ==&gt; 表示文件添加到Git的暂存区中，Git已经可以开始管理这个文件了（在stage的状态）。同理， (use &quot;git rm --cached ...&quot; to unstage) git commit git log ==&gt; 打印git的日志，看commit是否被创建出来了（包括完成提交的作者，邮箱[都是local配置优先] 和时间） Git的暂存区概念 暂存区应用场景举例： 代码写好后先存入暂存区内，此时有一种重构的想法并在工作目录源代码上重构，若重构失败，可从暂存区中再调回之前的版本。[暂存：暂时存放，不是正式提交。可以让我们实验代码的多种可能性，修正到最佳后再提交] 暂存：git add files 正式提交：git commit $ cp ../css3-windy-switch/index.html index.html $ git status On branch master Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) index.html nothing added to commit but untracked files present (use &quot;git add&quot; to track) $ cp -r ../css3-windy-switch/css . $ cp -r ../css3-windy-switch/css javascript $ rm -r javascript $ cp -r ../css3-windy-switch/js javascript $ git status On branch master Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) css/ index.html javascript/ git status 是最常用的一个命令，我们常常都需要去运行看看当前目录的情况，上例子中出现了untracked files: index.html，是刚复制粘贴到文件夹（Git的当前仓库）。若要将这个index.html 纳入Git管理，先要git add 注意： git add 可以添加多个文件（包括文件夹） 中间用空格隔开 git add 要写文件名或者文件夹名 git commit 只要写正式提交的备注，方便后期读懂提交仓库改动的意义 git add -u : 在git仓库中所有被Git管理了的新修改文件（tracked）一起再提交到暂存区中 Git 文件重命名 # Method1: 手动重命名方法 $ mv readme.txt readme.md $ git status $ git add readme.md $ git rm readme.txt $ git status # Method2: 用Git的快速重命名命令（更高效） $ git mv readme.txt readme.md $ git status # 正式提交 $ git commit -m'Move readme.txt to readme.md' $ git log Git log查看版本演变历史 # 便捷查看（将省略提交人信息） $ git log --oneline # 只看最近的4次或最近的2次 $ git log -n4 --oneline $ git log -n2 --oneline # 查看当前的分支 $ git branch -v # 创建一个分支叫temp，后面的数字是随意复制某一个commit的地址，在这个分支中，包含了包括所复制commit的所有之前的commit情况。 $ git checkout -b temp d4e1729be # 其中head变成了temp分支。 $ git log --oneline f9192c6 (HEAD -&gt; temp) Add test d4e1729 Add javasrcipt 7c13cfe Add index and css 61841ea Add readme # 查看master的分支 $ git log master --oneline # --all 查看所有分支的演变信息(父子关系) $ git log --all --oneline # --graph 图形化查看版本演变信息，从哪一步的代码更新开始产生分支？ $ git log --all --oneline --graph Author &amp; Committer Author 代表作者 Committer 代表提交人 一般情况作者就是提交人。但是当某人非常喜欢项目中master分支的一个commit，他想基于这个commit创建一个temp分支研究，此时为了尊重版权。此时在temp分支中当前Author就是原作者，Commiitter就是研究人。 # 利用gltk图形化界面查看git版本历史 $ gitk # 可以查看每一个commit的： # 1. Author # 2. Committer # 3. Parent ： 最早的commit没有parent # 4. Child ： 最后的commit没有child # 5. Branches ： 拥有这条commit的所有分支 # 6. Follows # 7. Precedes # 设置View: view ==&gt; New view ==&gt; All refs # ==&gt; Ok后可以查看所有分支版本演进历史 探究 .git 目录 $ cd .git $ ls -al total 56 drwxr-xr-x 14 zhenglongwu staff 448 Sep 14 20:57 . drwxr-xr-x 9 zhenglongwu staff 288 Sep 12 21:29 .. -rw-r--r-- 1 zhenglongwu staff 9 Sep 12 21:09 COMMIT_EDITMSG -rw-r--r-- 1 zhenglongwu staff 21 Sep 12 21:06 HEAD -rw-r--r-- 1 zhenglongwu staff 41 Sep 12 20:49 ORIG_HEAD -rw-r--r-- 1 zhenglongwu staff 184 Sep 12 16:10 config -rw-r--r-- 1 zhenglongwu staff 73 Sep 12 16:02 description -rw-r--r-- 1 zhenglongwu staff 461 Sep 14 20:57 gitk.cache drwxr-xr-x 13 zhenglongwu staff 416 Sep 12 16:02 hooks -rw-r--r-- 1 zhenglongwu staff 792 Sep 12 21:09 index drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 16:02 info drwxr-xr-x 4 zhenglongwu staff 128 Sep 12 19:14 logs drwxr-xr-x 28 zhenglongwu staff 896 Sep 12 21:09 objects drwxr-xr-x 4 zhenglongwu staff 128 Sep 12 16:02 refs # .git/HEAD文件： $ cat head ref: refs/heads/temp # ref代表一个引用，引用到temp分支,继续检查 $ git branch -av master 4ba1df1 Move readme.txt to readme.md * temp f9192c6 Add test ## *号代表当前的工作区在哪一个分支上 ## 因此HEAD的内容含义代表 我们当前正在工作的Git分支是temp ## 实验： 切换到master分支再查看HEAD文件 $ git checkout master fatal: this operation must be run in a work tree $ cd .. $ git checkout master Switched to branch 'master' $ cd - /Users/zhenglongwu/Desktop/Programming/Git/git_learning/.git $ cat head ref: refs/heads/master ## 备注： git checkout 命令就是切换分支 ## 结论： HEAD的文件内容会根据切换分支而改变，它告诉我们当前正在工作的是哪个分支。 # .git/config 文件 $ cat config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true [user] name = user1 email = 670362192@qq.com ## Git的local用户名设置就记录在仓库的config文件中。 ## 实验，修改username $ vi config $ cat config ... [user] name = userlong email = 670362192@qq.com $ git config --local --list core.repositoryformatversion=0 core.filemode=true core.bare=false core.logallrefupdates=true core.ignorecase=true core.precomposeunicode=true user.name=userlong user.email=670362192@qq.com $ git config --local user.name userlong # 反向更改 $ git config --local user.name 'long' $ git config --local user.name long $ cat config ... [user] name = long email = 670362192@qq.com ## 结论：config文件就是储存所有与本地仓库相关的配置信息。 # .git/refs/ 文件夹探究 $ cd refs $ ls -al total 0 drwxr-xr-x 4 zhenglongwu staff 128 Sep 12 16:02 . drwxr-xr-x 14 zhenglongwu staff 448 Sep 14 21:35 .. drwxr-xr-x 4 zhenglongwu staff 128 Sep 12 21:09 heads drwxr-xr-x 2 zhenglongwu staff 64 Sep 12 16:02 tags ## tags也被称作里程碑，比如当前项目开发到V1.0了，可以加一个标签打一个标识 ## heads对应于所有分支，分支代表的是一个独立的开发空间。比如开发软件时有前端开发和后端开发，此时前端可以创建一个分支，后端建一个分支。 彼此在自己的空间里工作是互不影响的。集成时又可以集成到一个公共的分支上。 ### 再挖 .git/refs/heads/文件夹 $ cd heads $ ls -al total 16 drwxr-xr-x 4 zhenglongwu staff 128 Sep 12 21:09 . drwxr-xr-x 4 zhenglongwu staff 128 Sep 12 16:02 .. -rw-r--r-- 1 zhenglongwu staff 41 Sep 12 20:52 master -rw-r--r-- 1 zhenglongwu staff 41 Sep 12 21:09 temp $ cat master 4ba1df1b3b40dae8540c01b81f947a0b4b3d8c1c ## heads文件夹里存放所有的分支，master文件存的是一个HASH值代表这个master最后一次commit的指针 $ git cat-file -t 4ba1df1b3b40dae commit ## cat-file -t 查看当前哈希值的object是什么类型，是一个commit ## 查看当前所有分支最新commit情况 $ git branch -av * master 4ba1df1 Move readme.txt to readme.md temp f9192c6 Add test ## 4ba1df1是短hash值，一般够用，当短的不够用时（产生重复时）再用更长一点的。 $ cat temp f9192c679da9f6ff44f20c81d1d8ba0fb381193a ## 同理可看出temp文件里存的也是temp分支中最后一次commit的哈希值 ### 继续挖 .git/refs/tags/文件夹 $ cd tags $ ls -al total 8 drwxr-xr-x 3 zhenglongwu staff 96 Sep 14 23:41 . drwxr-xr-x 4 zhenglongwu staff 128 Sep 12 16:02 .. -rw-r--r-- 1 zhenglongwu staff 41 Sep 14 23:41 js01 ## 有一个tag：js01， 读它 $ cat js01 b7e1bfaf5787edb14f14981df2d32530792d83fa $ git cat-file -t b7e1 tag ## 备注：可以少输入几位hash值，只要能识别出唯一的值。 ## 加-p 深挖这个hash值代表的东西 $ git cat-file -p b7e1 object d4e1729be6ec1d96e1068171e5d355b09ae5e3e7 type commit tag js01 tagger long &lt;670362192@qq.com&gt; 1568518864 -0400 js demo ## 继续挖这个object d4e1729be6ec1d96e1068171e5d355b09ae5e3e7是什么? 是一个commit $ git cat-file -t d4e1 commit $ git cat-file -p d4e1 tree 9c640279d65a23da23454864b9f96adb59cb3bec parent 7c13cfe19f0c14585b0629c1095b58be53cdc7cc author user1 &lt;670362192@qq.com&gt; 1568334269 -0400 committer user1 &lt;670362192@qq.com&gt; 1568334269 -0400 Add javasrcipt ###因此js01文件里存放的hash值代表一个tag，这个tag的hash值里存放了一个对象，这个对象的hash值代表一个commit git cat-file -t HASH值： 读取当前hash值代表的object类型 git cat-file -p HASH值： 读取当前hash值代表的object里面存放的东西（应该有各种属性和代表其它object的hash值） .git/refs/heads： 存放所有分支 .git/refs/heads/master：master分支最后一次commit的hash值 .git/refs/heads/temp：temp分支最后一次commit的hash值 .git/refs/tags： 里程碑，存放对某一次特殊意义commit 的标记 .git/refs/tags/js01： 名叫js01的标记，该文件记录的hash值里存放了另一个commit对象的hash值，也就是标记的commit。 ## !!重点: .git/objects/ 文件夹的探究 $ cd objects $ ls -al total 0 drwxr-xr-x 29 zhenglongwu staff 928 Sep 14 23:41 . drwxr-xr-x 14 zhenglongwu staff 448 Sep 14 21:35 .. .... drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 19:14 77 drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 20:17 7c drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 20:24 7f drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 21:09 8c .... drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 20:29 fb drwxr-xr-x 2 zhenglongwu staff 64 Sep 12 16:02 info drwxr-xr-x 2 zhenglongwu staff 64 Sep 12 16:02 pack ## 可以看出有两种类型的文件夹， ## 一种是只有2个字母的大部分文件夹 ## 另一种是pack文件夹：Git会做自我梳理过程，当松散文件过多时会打包到这里 ### 1.先深挖两字符的文件夹 .git/objects/77/ $ cd 77 $ ls -al total 8 drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 19:14 . drwxr-xr-x 29 zhenglongwu staff 928 Sep 14 23:41 .. -r--r--r-- 1 zhenglongwu staff 55 Sep 12 19:14 37016481fd9dbdf0ec0d9145d56358fd71feb2 ##解读：文件夹里有一堆类似hash值得东西：37016481fd9dbdf0ec0d9145d56358fd71feb2。但是它不是完整的hash值，它还缺少开头两个字符--文件夹的名字，因此这串完整的hash值要在开头加上77： 7737016481fd9dbdf0ec0d9145d56358fd71feb2 $ git cat-file -t 7737016481 tree $ git cat-file -p 773701 100644 blob e69de29bb2d1d6434b8b29ae775ad8c2e48c5391 readme.txt ## 解读：这串hash值代表一个tree类型的object，里面存放了一个文件 readme.txt。文件的hash值是e69de29bb2d1d6....， 这个文件在git里表示的类型是blob--文件对象。 ## 我们再以文件夹7f为例： $ ls -al 7f total 8 drwxr-xr-x 3 zhenglongwu staff 96 Sep 12 20:24 . drwxr-xr-x 29 zhenglongwu staff 928 Sep 14 23:41 .. -r--r--r-- 1 zhenglongwu staff 122 Sep 12 20:24 0153aaaa90651214e53926593e375803742ad5 $ git cat-file -t 7f0153aaaa tree $ git cat-file -p 7f0153aaaa 100644 blob c8984ae212db84b4a95e983e0af8e33215c41ec7 .DS_Store 100644 blob 230bfa8af0829fdffd7f78bf0dda6b851e482bd8 index.js 100644 blob 4d9b3a258759c53e7bc66b6fc554c51e2434437c jquery.min.js $ git cat-file -t 230bfa8a blob $ git cat-file -p 230bfa8a $(&quot;#switch&quot;).on('click', function () { if ($(&quot;body&quot;).hasClass(&quot;slow-wind&quot;)) { $(&quot;body&quot;).removeClass(&quot;slow-wind&quot;); $(&quot;#switch&quot;).removeClass(&quot;switched&quot;); } ## 结论：blob就是git的文件类型，并且使用git cat-file -p HASH值 可以直接打开文件内的记录内容。 到现在为止接触到的所有Git的数据类型：(也是接触最多的核心4类型) commit tag tree blob Git目录总结：（常用的） ./git/HEAD 文件： 当前工作的git仓库分支. 直接修改其内容与 git checkout 命令同样效果。 ./git/config 文件： 当前的local仓库所有分支的配置，直接修改文件内容与 git config --local user.name '' 一样效果 ./git/refs 文件夹： 存放各个分支最新commit 和 tag里程碑信息 ./git/objects 文件夹：存放git中所有的数据对应的hash值。（只要仓库中两文件的内容一模一样，在git眼里它就是一个文件，然后只有一个hash值代表这个文件？） commit、tree、blob三个对象之间的关系 存储是git的核心技术点。版本管理系统中，文件的变更是非常频繁的，所以设计一个良好文件储存机制对于版本管理系统是非常关键的，否则版本管理会越来越混乱。 一个commit对应且仅对应一颗树， 这颗树代表当前commit对应的一个视图快照，就是在当时那个时间点时，项目所有的文件夹和文件的样子。 一个commit里包含了一棵树（项目文件夹整体快照），parent（上一个commit），author（作者），committer（提交人）四者的对应Hash值，然后加上commit时的message信息。 一棵树tree里包含了其它树（文件夹）和 blob（文件）的对应hash值 其它树tree里依然包含了其它树（子文件夹） 和 blob（文件）的对应hash值 blob（文件）里仅包含文件的内容 因此： commit仅储存快照tree，parent，author，committer的hash值 和 commit时附加的解释信息 tree中仅储存 tree 和 blob 的 hash值 blob中就储存 文件的内容信息。（通过blob的hash值读文件）（注意，blob和文件名一点关系都没有，只要文件的内容相同，不管文件名叫什么，都只是一个东西） # 代码实验 $ git branch -av * master 4ba1df1 Move readme.txt to readme.md temp f9192c6 Add test $ git log ... commit d4e1729be6ec1d96e1068171e5d355b09ae5e3e7 (tag: js01) Author: user1 &lt;670362192@qq.com&gt; Date: Thu Sep 12 20:24:29 2019 -0400 Add javasrcipt ... # 读取commit的hash值 $ git cat-file -p d4e1729be6ec tree 9c640279d65a23da23454864b9f96adb59cb3bec parent 7c13cfe19f0c14585b0629c1095b58be53cdc7cc author user1 &lt;670362192@qq.com&gt; 1568334269 -0400 committer user1 &lt;670362192@qq.com&gt; 1568334269 -0400 Add javasrcipt $ ls .git/objects/d4 e1729be6ec1d96e1068171e5d355b09ae5e3e7 # 读取tree的hash值 $ git cat-file -p 9c640279d6 040000 tree e4addc26befad478e734cb7f6919cde05fe70aa6 css 100644 blob 726ecbd247e7b0156bf6a5e39dc9ce0b1048818c index.html 040000 tree 7f0153aaaa90651214e53926593e375803742ad5 javascript 100644 blob e69de29bb2d1d6434b8b29ae775ad8c2e48c5391 readme.txt $ ls .git/objects/9c 640279d65a23da23454864b9f96adb59cb3bec # 读取blob的hash值 $ git cat-file -p 726ecbd247 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot; &gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; ... &lt;/body&gt; &lt;/html&gt; $ ls .git/objects/72 6ecbd247e7b0156bf6a5e39dc9ce0b1048818c Git 命令深度解析（数一数tree的个数） # 建立仓库 $ git init count_tree $ cd count_tree $ ls -al .git/objects total 0 drwxr-xr-x 4 zhenglongwu staff 128 Sep 18 23:22 . drwxr-xr-x 10 zhenglongwu staff 320 Sep 18 23:22 .. drwxr-xr-x 2 zhenglongwu staff 64 Sep 18 23:22 info drwxr-xr-x 2 zhenglongwu staff 64 Sep 18 23:22 pack ## 空仓库，没有任何对象 ## 创建一个文件夹doc $ mkdir doc $ ls doc $ git status On branch master No commits yet nothing to commit (create/copy files and use &quot;git add&quot; to track) $ git add doc $ git status On branch master No commits yet nothing to commit (create/copy files and use &quot;git add&quot; to track) $ ls -al .git/objects total 0 drwxr-xr-x 4 zhenglongwu staff 128 Sep 18 23:22 . drwxr-xr-x 10 zhenglongwu staff 320 Sep 18 23:22 .. drwxr-xr-x 2 zhenglongwu staff 64 Sep 18 23:22 info drwxr-xr-x 2 zhenglongwu staff 64 Sep 18 23:22 pack ## ==&gt; 空文件夹不是对象, add后也没有效果 ## 在文件夹doc内创建文件 $ cd doc $ echo 'hello world'&gt;readme.txt $ ls readme.txt $ cat readme.txt hello world $ cd .. $ git status On branch master No commits yet Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) doc/ nothing added to commit but untracked files present (use &quot;git add&quot; to track) $ ls -al .git/objects total 0 drwxr-xr-x 4 zhenglongwu staff 128 Sep 18 23:22 . drwxr-xr-x 10 zhenglongwu staff 320 Sep 18 23:37 .. drwxr-xr-x 2 zhenglongwu staff 64 Sep 18 23:22 info drwxr-xr-x 2 zhenglongwu staff 64 Sep 18 23:22 pack ## 在doc文件内创建了文件readme.txt，git status出现untracked文件，Git系统依然没有创建对象。 ## push给Git系统 $ git add doc $ git status On branch master No commits yet Changes to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: doc/readme.txt $ ls -al .git/objects total 0 drwxr-xr-x 5 zhenglongwu staff 160 Sep 18 23:44 . drwxr-xr-x 11 zhenglongwu staff 352 Sep 18 23:44 .. drwxr-xr-x 3 zhenglongwu staff 96 Sep 18 23:44 a0 ... $ ls -al .git/objects/a0 total 8 drwxr-xr-x 3 zhenglongwu staff 96 Sep 18 23:44 . drwxr-xr-x 5 zhenglongwu staff 160 Sep 18 23:44 .. -r--r--r-- 1 zhenglongwu staff 29 Sep 18 23:44 423896973644771497bdc03eb99d5281615b51 $ git cat-file -t a0423896973 blob $ git cat-file -p a0423896973 hello world! #### 中途小结： #### 1.创建空文件夹，Git系统不会有任何变化或提示。 #### 2.在空文件夹内部创建文本文件后，包括文件夹在内Git系统出现untracked file提示, .git/objects中没有创建对象 #### 3.将包含文件的文件夹一起git add后，.git/objects中出现blob对象，即创建的文件。 #### 4.问：git commit之后呢？会有多少创建对象？ ## 实验git commit $ git commit -m'Add readme' [master (root-commit) 4ce2739] Add readme 1 file changed, 1 insertion(+) create mode 100644 doc/readme.txt $ git status On branch master nothing to commit, working tree clean $ git log commit 4ce2739c6af63445b694ef10323fd9c5ece775c2 (HEAD -&gt; master) Author: zhenglong &lt;wzhenglong@yahoo.com&gt; Date: Wed Sep 18 23:55:02 2019 -0400 Add readme $ git log --oneline 4ce2739 (HEAD -&gt; master) Add readme $ ls -al .git/objects total 0 ... drwxr-xr-x 3 zhenglongwu staff 96 Sep 18 23:55 32 drwxr-xr-x 3 zhenglongwu staff 96 Sep 18 23:55 4c drwxr-xr-x 3 zhenglongwu staff 96 Sep 18 23:55 7e drwxr-xr-x 3 zhenglongwu staff 96 Sep 18 23:44 a0 ... ## 比之前的a0文件夹多了32，4c和7e三个文件夹 ## 32文件夹是什么呢？ $ ls -al .git/objects/32 total 8 ... -r--r--r-- 1 zhenglongwu staff 45 Sep 18 23:55 dd8f7c4977eab3ac25d95abefbfa781e58abc7 $ git cat-file -t 32dd8 tree $ git cat-file -p 32dd8 040000 tree 7e007f7273ba9497e7595b1f84875163f9a8903d doc $ git cat-file -p 7e007 100644 blob a0423896973644771497bdc03eb99d5281615b51 readme.txt $ git cat-file -p a0423 hello world! ## 4c文件夹是什么呢？ $ ls -al .git/objects/4c total 8 ... -r--r--r-- 1 zhenglongwu staff 126 Sep 18 23:55 e2739c6af63445b694ef10323fd9c5ece775c2 $ git cat-file -t 4ce27 commit $ git cat-file -p 4ce27 tree 32dd8f7c4977eab3ac25d95abefbfa781e58abc7 author zhenglong &lt;wzhenglong@yahoo.com&gt; 1568865302 -0400 committer zhenglong &lt;wzhenglong@yahoo.com&gt; 1568865302 -0400 Add readme ## 32dd8..就是上面doc的父文件夹的hash值 ## 7e文件夹是什么呢？ $ ls -al .git/objects/7e total 8 ... -r--r--r-- 1 zhenglongwu staff 55 Sep 18 23:55 007f7273ba9497e7595b1f84875163f9a8903d $ git cat-file -t 7e007 tree $ git cat-file -p 7e007 100644 blob a0423896973644771497bdc03eb99d5281615b51 readme.txt $ git cat-file -p a0423 hello world! #### 小结4 #### 1.仓库没有任何commits前，git status：No commits yet. nothing to commit (create/copy files and use &quot;git add&quot; to track) #### 2.仓库有commits后，git status：nothing to commit, working tree clean #### 3.仓库commits后, git创建的objects有： ##### a. 32dd8 tree =&gt; doc的父文件夹 [count_tree] （内容: 一个7e007的tree --&gt; doc） ##### b. 7e007 tree =&gt; readme.txt的父文件夹 [doc] (内容：一个a0423的blob --&gt; readme.txt) ##### c. a0423 blob =&gt; (内容：hello world) ##### d. 4ce27 commit =&gt; (内容：tree 32dd8 的commit记录) #### 完整总结--Git内部行为分析 用户行为 Git行为 1. git init 创建空仓库环境 2. 创建空文件夹doc 无反应 3. 写入一个文件 Git出现untracked file提示 4. git add 文件 Git创建blob对象a0423 (readme.txt) 5. git commit Git创建commit对象4ce27 (add readme) tree对象32dd8 (count_tree) tree对象7e007 (doc) 分离头指针注意事项？ git checkout 到空分支后，即不指定从哪一个commit开始分支 在这个空分支git add甚至git commit后切回原master分支将丢失在空分支的所有新操作。 所有的新操作必须绑定在一个当前指定的一个分支上才行。否则文件丢失。 所有的hash值都是一个指针，指向它的内存空间地址。通过Git解读hash值可以阅读当前的对象的任何信息。 进一步理解head和branch ","link":"https://zl-wu.github.io/post/git-you-er-yuan-ru-men/"}]}