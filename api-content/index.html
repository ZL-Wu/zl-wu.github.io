{"posts":[{"title":"MLA -- collaborative filtering","content":"&quot;Collaborative filtering can be divided into users or items filtering. With its excellent speed and robustness, collaborative filtering is hot in the global Internet field. 😃😃&quot; [toc] In general speaking, Collaborative Filtering is to use some behavior information from a group of people who has similar interests and common experience to recommend new information that one user in the group may be interested in. Individuals give a response to the information (such as scoring) through the cooperative mechanism and record it to acheive the purpose of filtering and then help others to filter the information. Collaborative filtering can be divided into users or items filtering. With its excellent speed and robustness, collaborative filtering is hot in the global Internet field. 😃😃 I. User-Based CF The user-based collaborative filtering algorithm is to discover the user's preference of the product or content based on the user's historical behavior data (such as product purchase, collection, content review or sharing), and measure or score to quantize these preferences. According to the attitudes and preferences of different users towards the same product or content, the relationship between users can be calculated. Then we can recommend products or contents among users with similar preference. To be simiply, if user A and user B all purchased books &quot;x&quot;, &quot;y&quot; and &quot;z&quot;, and also all gave them 5 stars comments. Then user A and user B should belong to a same group. And the book &quot;w&quot; that user A read and like could be recommended to the user B. (This is the benifit of user-based CF.) 1. Find users with similar preferences Let's simulate a situation that 5 users score 2 products. The score here may indicate a real purchase, or it may be a quantitative indicator of the user's different behaviors of the product. For example, the number of times you browse products, recommend products to friends, bookmark, share, or comment. These behaviors can indicate the user's attitude and preference to the product. Product 1 Product 2 User 1 3.3 6.5 User 2 5.8 2.6 User 3 3.6 6.3 User 4 3.4 5.8 User 5 5.2 3.1 From the data in the table above, it is a bit hard to figure out the relationship among these five users. However, if we plot all these five points in a scatter chart, the conclusion is obvious. User 1, 3 and 4 is one group, and User 2 and 5 is another group. # Python code for drawing the scatter plot above import matplotlib.pyplot as plt import numpy as np x = [3.3, 5.8, 3.6, 3.4, 5.2] y = [6.5, 2.6, 6.3, 5.8, 3.1] colors = np.random.rand(5) plt.scatter(x,y,c=colors,alpha=0.8) plt.xlim(0,7); plt.xlabel(&quot;Product 1&quot;) plt.ylim(0,7); plt.ylabel(&quot;Product 2&quot;) for i in range(5): plt.annotate(&quot;User &quot;+str(i+1), xy=(x[i],y[i]), xytext = (x[i]+0.1, y[i]+0.0)) # 这里xy是需要标记的坐标，xytext是对应的标签坐标 plt.show() 2. Similarity Calculation Although the scatter plot is intuitive, it cannot be put into production. Therefore, we need to accurately measure the relationship of users through real numbers, and complete the recommendation of products based on these relationship conclusions. 2.1. Euclidean distance evaluation Euclidean distance is a relatively simple method to evaluate the relationship among users. The principle is to calculate the distance of each two points in the scatter chart. Distance Reciprocal User 1 &amp; 2 4.63 0.22 User 1 &amp; 3 0.36 2.77 User 1 &amp; 4 0.71 1.41 User 1 &amp; 5 3.89 0.26 User 2 &amp; 3 4.30 0.23 User 2 &amp; 4 4.00 0.25 User 2 &amp; 5 0.78 1.28 User 3 &amp; 4 0.54 1.86 User 3 &amp; 5 3.58 0.28 User 4 &amp; 5 3.24 0.31 From the calculation above, if we defined a threshold, we can easily cluster these users. 2.2. Pearson correlation evaluation Pearson correlation evaluation is another method to calculate the relationship between users. It is a bit more complicated than the calculation of Euclidean distance, but Pearson correlation evaluation can give a better result when the score data is not standardized. The formula of Pearson correlation coefficient between variable x and y is: ρxy=cov(x,y)σxσy=E[(x−μx)(y−μy)]σxσy\\begin{aligned} \\rho_{xy} &amp;= \\frac{cov(x, y)}{\\sigma_x \\sigma_y} \\\\ &amp;= \\frac{E[(x-\\mu_x)(y-\\mu_y)]}{\\sigma_x \\sigma_y} \\end{aligned}ρxy​​=σx​σy​cov(x,y)​=σx​σy​E[(x−μx​)(y−μy​)]​​ μx\\mu_xμx​ and μy\\mu_yμy​ are the mean value of variable x and variable y. ρxy\\rho_{xy}ρxy​ is tne overall correlation coefficient. If we estimate the covariance and standard deviation of the sample, we can also get the sample's Pearson correlation coefficient &quot;r&quot;. r=∑i=1n(Xi−X‾)(Yi−Y‾)∑i=1n(Xi−X‾)2∑i=1n(Yi−Y‾)2=1n−1∗∑i=1n(Xi−X‾)(Yi−Y‾)σXσY=1n−1∗∑i=1n[(Xi−X‾σX)(Yi−Y‾σY)]\\begin{aligned} r &amp;= \\frac{\\sum_{i=1}^n(X_i - \\overline{X})(Y_i - \\overline{Y})} {\\sqrt{\\sum_{i=1}^n(X_i-\\overline{X})^2}\\sqrt{\\sum_{i=1}^n(Y_i-\\overline{Y})^2}} \\\\ &amp;= \\frac{1}{n-1} * \\frac{\\sum_{i=1}^n(X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sigma_X \\sigma_Y} \\\\ &amp;= \\frac{1}{n-1} * \\sum_{i=1}^n \\left[ \\left(\\frac{X_i-\\overline{X}}{\\sigma_X}\\right) \\left(\\frac{Y_i-\\overline{Y}}{\\sigma_Y}\\right) \\right] \\end{aligned}r​=∑i=1n​(Xi​−X)2​∑i=1n​(Yi​−Y)2​∑i=1n​(Xi​−X)(Yi​−Y)​=n−11​∗σX​σY​∑i=1n​(Xi​−X)(Yi​−Y)​=n−11​∗i=1∑n​[(σX​Xi​−X​)(σY​Yi​−Y​)]​ (Xi−X‾σX)\\left(\\frac{X_i-\\overline{X}}{\\sigma_X}\\right)(σX​Xi​−X​) is the standard score (Z) of sample variable X. The Pearson correlation coefficient always falls in -1 and 1. The Pearson coefficient is symmetrical: corr(X,Y) = corr(Y,X). An important mathematical characteristic of the Pearson correlation coefficient is that any change of the position and scale of the two variables will not cause the change of the coefficient. That is, if we move X to a+bX, and move Y to c+dY, (a,b,c,d are all constants), the correlation coefficient of X and Y doesn't change. μx=E(X)\\mu_x = E(X) μx​=E(X) σx2=E[(X−E(X))2]=E[(X2−2E(X)X+E2(X))]=E(X2)−2E2(X)+E2(X)=E(X2)−E2(X)=E(X2)−μX2\\begin{aligned} \\sigma_x^2 &amp;= E[(X-E(X))^2] \\\\ &amp;= E[(X^2-2E(X)X+E^2(X))] \\\\ &amp;= E(X^2)-2E^2(X)+E^2(X) \\\\ &amp;= E(X^2)-E^2(X) \\\\ &amp;= E(X^2)-\\mu_X^2 \\end{aligned}σx2​​=E[(X−E(X))2]=E[(X2−2E(X)X+E2(X))]=E(X2)−2E2(X)+E2(X)=E(X2)−E2(X)=E(X2)−μX2​​ E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y)−E(X)E(Y)+E(X)E(Y)=E(XY)−E(X)E(Y)\\begin{aligned} &amp;\\quad\\ E[(X-E(X))(Y-E(Y))] \\\\ &amp;= E(XY)-E(X)E(Y)-E(X)E(Y)+E(X)E(Y) \\\\ &amp;= E(XY)-E(X)E(Y) \\end{aligned}​ E[(X−E(X))(Y−E(Y))]=E(XY)−E(X)E(Y)−E(X)E(Y)+E(X)E(Y)=E(XY)−E(X)E(Y)​ Therefore, the Pearson correlation coefficient can be also writen as: ρxy=E(XY)−E(X)E(Y)E(X2)−E2(X)E(Y2)−E2(Y)\\rho_{xy} = \\frac{E(XY)-E(X)E(Y)}{\\sqrt{E(X^2)-E^2(X)}\\sqrt{E(Y^2)-E^2(Y)}} ρxy​=E(X2)−E2(X)​E(Y2)−E2(Y)​E(XY)−E(X)E(Y)​ And the sample's correlation coefficient is: r=∑i=1nxiyi−nxˉyˉ(n−1)sxsy=n2[∑xiyin−∑xi∑yin2]n2∑xi2n−(∑xin)2∑yi2n−(∑yin)2=n∑xiyi−∑xi∑yin∑xi2−(∑xi)2n∑yi2−(∑yi)2\\begin{aligned} r &amp;= \\frac{\\sum_{i=1}^nx_iy_i - n\\bar{x}\\bar{y}}{(n-1)s_xs_y} \\\\ &amp;= \\frac{n^2 \\left[\\frac{\\sum x_iy_i}{n} - \\frac{\\sum x_i\\sum y_i}{n^2} \\right]} {n^2 \\sqrt{\\frac{\\sum x_i^2}{n} - \\left(\\frac{\\sum x_i}{n}\\right)^2} \\sqrt{\\frac{\\sum y_i^2}{n} - \\left(\\frac{\\sum y_i}{n}\\right)^2}} \\\\ &amp;= \\frac{n \\sum x_iy_i - \\sum x_i \\sum y_i} {\\sqrt{n \\sum x_i^2 - (\\sum x_i)^2} \\sqrt{n \\sum y_i^2 - (\\sum y_i)^2}} \\end{aligned}r​=(n−1)sx​sy​∑i=1n​xi​yi​−nxˉyˉ​​=n2n∑xi2​​−(n∑xi​​)2​n∑yi2​​−(n∑yi​​)2​n2[n∑xi​yi​​−n2∑xi​∑yi​​]​=n∑xi2​−(∑xi​)2​n∑yi2​−(∑yi​)2​n∑xi​yi​−∑xi​∑yi​​​ Suppose X and Y are the variables that represents various rating data to a same set of products from User X and User Y. As we all know, different users has different attitudes, someone is conservative and someone is aggresive to their rating. A good thing of Pearson correlation coefficient is that it could remove these difference and standardize variable X and Y. And the coefficient measures how does Y change when X increase? increase or decrease? Exactly the same change means 1, and completely opposite change means -1. Give a example, which is closer to the real case. Product 1 Product 2 Product 3 Product 4 Product 5 User A 3.3 6.5 2.8 3.4 5.5 User B 3.5 5.8 3.1 3.6 5.1 User C 5.6 3.3 4.5 5.2 3.2 User D 5.4 2.8 4.1 4.9 2.8 User E 5.2 3.1 4.7 5.3 3.1 We calculated the similarity data between users by calculating the ratings of 5 products by 5 users above. Here you can see that users A&amp;B, C&amp;D, C&amp;E and D&amp;E have a high similarity. Next, we can recommend products to users based on similarity. Similarity User A&amp;B 0.9998 User A&amp;C -0.8478 User A&amp;D -0.8418 User A&amp;E -0.9152 User B&amp;C -0.8417 User B&amp;D -0.8353 User B&amp;E -0.9100 User C&amp;D 0.9990 User C&amp;E 0.9763 User D&amp;E 0.9698 3. Recommend products for users Continue the above example, if we want to recommend some new products to User C. We can find out the most similar user of him or her at first, that is, user D at above table. Then we can browse and screen out those products that User D likes but User C doesn't try before, and then recomemnd to User C. This is a very intuitive way, however, in real case, there might be some more complicated calculation to weight and sort these new products, then choose the best one product to recommend. For example, find out 2 or 3 most similar users and combine their rating records to weight new products to recommend. 4. Pros and cons of the Algorithm Data sparsity. A large-scale e-commerce recommendation system usually has a lot of items, and users may buy less than 1% of the items. The overlap of items bought by different users is low, resulting in the algorithm unable to find a user's neighbors, that is, similar preferences Users. Algorithm scalability. The calculation amount of the nearest neighbor algorithm increases with the increase of the number of users and items, and is not suitable for use in a large amount of data. 5. Code Example (Python) Dataset Example: There are 138493 users' records of rating to the different movies they have watched. Try to use User-Based CF Algorithm to recommend movies to any user. userId movieId rating 1 2 3.5 1 29 3.5 1 32 3.5 1 47 3.5 1 50 3.5 ... ... ... 138493 68319 4.5 138493 68954 4.5 138493 69526 4.5 138493 69644 3.0 138493 70286 5.0 138493 71619 2.5 Solution: https://github.com/ZL-Wu/python-code-pool/blob/master/collaborative_filtering.ipynb II. Item-Based CF Item-Based CF's principle is basically the same as that of User-Based CF. In User-Based CF, consider the User as Item, and Item as User. According to the various rating data from various users to several product, we can calculate the similarity between each two products. Then for each user, this algorithm can recommend new and most similar product based on those products he or she used before. 1. Advantages Can filter information that the machine is difficult to analyze automatically, such as artwork, music, etc. Share the experience of others, avoid incomplete or inaccurate content analysis, and can filter based on some complex and difficult to express concepts (such as information quality, personal taste). The ability to recommend new information. It can be found that the content is completely dissimilar, and the content of the recommended information is not expected by the user in advance. You can discover the user's potential interest preferences that you have not yet discovered. Recommend personalization and high degree of automation. Can effectively use the feedback information of other similar users. Accelerate the speed of personalized learning. 2. Disadvantages New User Problem (New User Problem) The quality of the recommendation at the beginning of the system is poor. New Item Problem The quality depends on the historical data set. Sparsity System scalability (Scalability). III. Conclusion According to the different data source, Recommendation Engines can be can be divided into three categories: Demographic-based Recommendation Content-based Recommendation Collaborative Filtering-based Recommendation (IBM's introduction of recommendation engines) Content-based Recommendation is based on the metadata of item or content to find the relation among them. For example, we define various feature of movies, such as category, length and so on, then build a model to group similar movies. Finally, recommend users new movies by looking for similar movies of the user's favorite movies. Collaborative Filtering-based Recommendation can also be divided into three categories: User-based CF: User A likes Items 1,2,3. User C likes Items 1,2. And User A and User C are similar. Therefore, recommend Item 3 to User C. Item-based CF: User A likes Items 1,2,3. User C likes Item 1,2. And Item 1 and Item 3 are similar. Therefore, recommend Item3 to User C. Model-based CF: Based on the sample user preference information, train a recommendation model, and then make prediction recommendations based on real-time user preference information. Content-based Recommendation needs to know the contents and details of items or products. However, CF Recommendation is totally a statistical model. We don't need to know the details about the product and the content. What we did is only research user's behavior. IV. Reference https://www.zhihu.com/question/19971859 https://www.jianshu.com/p/d15ba37755d1 https://baike.baidu.com/item/协同过滤/4732213?fr=aladdin https://wiki.mbalib.com/wiki/长尾理论 https://baike.baidu.com/item/皮尔逊相关系数/12712835?fr=aladdin https://baike.baidu.com/item/协方差 ","link":"https://zl-wu.github.io/post/mla-collaborative-filtering/"},{"title":"MLA -- Decision Tree","content":"Decision Tree is one of classical algorithms in machine learning. It can be used as both a classification model and a regression model. It is also suitable as a ensemble model, such as Random Forest. The history of Desicion Tree has three stages: ID3, C4.5 and CART [toc] 1. The Information Theory Foundation of Decision Tree ID3 It all originated from a simple &quot;if...else...&quot; statment that every programmer almost uses every day. Data researchers want to use a certain condition (if...else...) to split dataset into two distinct subsets. Then there are two questions we need to consider: A dataset usually has lots of features. So how to choose the feature that we need to use first in &quot;if...else...&quot;? And second, third features? How to quantitatively evaluate the quality of a certain binary division on the feature? In 1970s, a genius Ross Quinlan invented a method to guide and evaluate the process of decision tree by using entropy in information theory. As soon as the methods came out, its simplicity and efficiency cause a sensation. Quinlan called it ID3 algorithm. Entropy measures the uncertainty of things, the more uncertain things, the greater the entropy of it. Specifically, the expression of the entropy of a random discrete variable X is as follows: H(X)=−∑i=1npilog(pi)H(X) = - \\sum_{i=1}^np_ilog(p_i) H(X)=−i=1∑n​pi​log(pi​) n represents the &quot;n&quot; kinds of discrete values. (counts of unique value of X) pip_ipi​ is the probability that X takes the value &quot;i&quot;. log usually is the logarithm based on 2 or e. For example, if X only has two kind of values, the entropy is the largest when the probabilities of this two values are 1/2, which means X has the largest uncertainty. H(X) = -(1/2*log(1/2) + 1/2*log(1/2)) = log(2) if the probabilities of this two values are 1/3 and 2/3, H(X) = -(1/3*log(1/3) + 2/3*log(2/3)) = log(3) - 2/3*log(2) &lt; log(2) Then multivariable entropy: H(X,Y)=−∑i=1np(xi,yi)log(p(xi,yi))H(X,Y) = - \\sum_{i=1}^n p(x_i,y_i)log(p(x_i,y_i)) H(X,Y)=−i=1∑n​p(xi​,yi​)log(p(xi​,yi​)) And conditional entropy H(X|Y), which is the uncertainty of X after knowing the uncertainty of Y: H(X∣Y)=−∑i=1np(xi,yi)log(p(xi∣yi))=∑j=1np(yj)H(X∣yj)\\begin{aligned} H(X|Y) &amp;= - \\sum_{i=1}^n p(x_i,y_i)log(p(x_i|y_i)) \\\\ &amp;= \\sum_{j=1}^n p(y_j)H(X|y_j) \\end{aligned} H(X∣Y)​=−i=1∑n​p(xi​,yi​)log(p(xi​∣yi​))=j=1∑n​p(yj​)H(X∣yj​)​ H(X) represents the uncertainty of X. H(X|Y) represents the left uncertainty of X after knowing the Y. Therfore, [H(X) - H(X|Y)] represents the degree of reduction in uncertainty of X after knowing the Y. It is also called mutual information in information theory, which is wrote as I(X,Y). (互信息) In decision tree ID3 algorithm, mutual info is called information gain (信息增益). ID3 algorithm uses information gain to determin what feature should be used in current node to build the decidion tree. The larger the information gain, the better it is for classification in current node. From the graph above, we can easily figure out the relationship among them. The ellipse on the left represents H(X) [uncertainty of X]. And the ellipse on the left represents H(Y). The overlapping section in the middle is the mutual information (or information gain) I(x,y). The left section of left ellipse H(X|Y) The union of two ellipse is H(X,Y) If Y is &quot;a&quot;, almost all X is &quot;b&quot;. This means when Y is &quot;a&quot;, the uncertainty of X is very low (H(X|Y) is very small, I(X,Y) is large). Because the randomness of the X value is very small when Y is &quot;a&quot;. Then X is suitabale as a feature of classification of Y. 2. Decision Tree ID3 Algorithm Principle Take an example for ID3 algorithm to see how it uses information gain to build a decision tree model. Suppose there are 15 samples data D, the output is 0 or 1, and 9 of them are 1, 6 of them are 0. There is a feature A in the sample, the values are A1, A2 or A3. When A is A1, the output has 3 &quot;1&quot; and 2 &quot;0&quot; When A is A2, the output has 2 &quot;1&quot; and 3 &quot;0&quot; When A is A3, the output has 4 &quot;1&quot; and 1 &quot;0&quot; The entropy of sample D is: H(D)=−(915log2915+615log2615)=0.971H(D) = - (\\frac{9}{15}log_2\\frac{9}{15} + \\frac{6}{15}log_2\\frac{6}{15}) = 0.971 H(D)=−(159​log2​159​+156​log2​156​)=0.971 The conditional entropy of sample D in the feature A is: H(D∣A)=515H(DA1)+515H(DA2)+515H(DA3)=−515(35log235+25log225)−515(25log225+35log235)−515(45log245+15log215)=0.888\\begin{aligned} H(D|A) &amp;= \\frac{5}{15}H(D_{A1}) + \\frac{5}{15}H(D_{A2}) + \\frac{5}{15}H(D_{A3}) \\\\ &amp;= -\\frac{5}{15}(\\frac{3}{5}log_2\\frac{3}{5}+\\frac{2}{5}log_2\\frac{2}{5}) -\\frac{5}{15}(\\frac{2}{5}log_2\\frac{2}{5}+\\frac{3}{5}log_2\\frac{3}{5}) -\\frac{5}{15}(\\frac{4}{5}log_2\\frac{4}{5}+\\frac{1}{5}log_2\\frac{1}{5}) \\\\ &amp;= 0.888 \\end{aligned} H(D∣A)​=155​H(DA1​)+155​H(DA2​)+155​H(DA3​)=−155​(53​log2​53​+52​log2​52​)−155​(52​log2​52​+53​log2​53​)−155​(54​log2​54​+51​log2​51​)=0.888​ Then the information gain H(D) - H(D|A) = 0.971 - 0.888 = 0.083 The specific algorithm process looks like: Input is &quot;m” samples, the sample output set is &quot;D&quot;. Each sample has &quot;n&quot; discrete features, the feature set is &quot;A&quot;. The final output of ID3 algorithm is a decision tree &quot;T&quot; The process is: Initialize the threshold of information gain ϵ\\epsilonϵ Read the output set D. If all output value are same &quot;Di&quot;, return a single node tree T, marked as category &quot;Di&quot;. Read the feature set A. If A is null, return a single node tree T, marked as the category that has the largest counts number in D. [argmax(counts(Di))] Traverse and Calculate the information gain of each feature in A (n features). Select the feature &quot;AgA_gAg​&quot; which has the largest information gain. If the information gain of &quot;AgA_gAg​&quot; is less than the threshold ϵ\\epsilonϵ, return a single node tree T, marked as the category that has the largest counts number in D. [argmax(counts(Di))] If not (else), according to the different values in AgA_gAg​, split the total sample into several different categories subset DjD_jDj​. Each category is a child node. And return the tree T with multiple nodes. For each child nodes, let D=Dj,A=A−{Ag}D=D_j, A=A-\\{A_g\\}D=Dj​,A=A−{Ag​}, then recursively call the step 2-6 to get and return the subtree TiT_iTi​. 3. Deficiency of decision tree ID3 algorithm Althogh ID3 algorithm proposed new ideas, there are still many areas worthy of improvement. (1) ID3 doesn't consider continuous features, such as length, density or other continuous features. This greatly limits the use of ID3. (2) ID3 uses the features of large information gain to preferentially establish the nodes of the decision tree. However, under the same conditions, the feature with more kinds of values has larger information gain. For example, if the feature A has 2 kinds of values, each of them is 1/2, H(A)=log(2); if the feature A has 3 kinds of values, each of them is 1/3, H(A)=log(3). How to correct this problem? (3) ID3 doesn't consider the condition of missing values. (4) ID3 doesn't consider the overfitting problem. Ross Quinlan has improved the ID3 algorithm based on the above deficiencies. This is C4.5 algorithm. The reason that why the new algorithm wasn't named ID4 or IDn: Decision tree was too popular when it came out, then lots of people started the second innovation and occupied ID4 and ID5 soon. So Quinlan took a new path and named it the C4.0 algorithm. And later, the advanced version was C4.5 algorithm. 4. Improvement of the Decision Tree C4.5 algorithm According to the 4 deficiencies of ID3 above, C4.5 improved them through the following: (1) Cannot handle continuous features: The idea of C4.5 is to discretize continuous features. For example, the feature A of m samples has m values, sort from small to large as a1,a2,...,ama_1, a_2, ..., a_ma1​,a2​,...,am​. C4.5 take the average value between two neighbor values as a dividing point, then there are (m-1) dividing points. The i-th dividing point TiT_iTi​ is Ti=ai+ai+12T_i=\\frac{a_i+a_{i+1}}{2}Ti​=2ai​+ai+1​​. For these (m-1) dividing points, C4.5 calculates the information gain using each dividding point as the binary classification point. Finally, select the point with the largest information gain as the binary discrete classification point for the continuous feature. Pay attention: unlike the discrete attribute, if the current node is a continuous attribute, then this attribute can also participate in the generation and selection process of child nodes. (2) Bias of features with more kinds of values: C4.5 inroduces a new variable of information gain ratio IR(X,Y)I_R(X,Y)IR​(X,Y). It is the ratio of information gain and feature entropy. IR(D,A)=I(A,D)HA(D)I_R(D,A) = \\frac{I(A,D)}{H_A(D)} IR​(D,A)=HA​(D)I(A,D)​ D is the output set of samples, A is the feature set. And the feature entropy is: HA(D)=−∑i=1n∣Di∣∣D∣log2∣Di∣∣D∣H_A(D) = -\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|} HA​(D)=−i=1∑n​∣D∣∣Di​∣​log2​∣D∣∣Di​∣​ n is the number of categories of feature A. DiD_iDi​ is the number of samples corresponding to the i-th value of feature A. D is the total number of all samples. The feature with more kinds of values has a larger feature entropy. It serves as a denominator to correct the problem that information gain tends to be biased toward features with more values. (3) Problem of missing values: There are two sub problems needed to be solved: a. How to choose the feature to build chind node when some features of the sample is missing? b. If the feature is selected, how to deal with the samples with missing values on this feature? a. Feature Choosing with missing value For a featur A with missing values, C4.5 set a weight for each sample (including those with missing value in A), the initial weight could be 1. Then C4.5 splits the data into 2 subsets, one is the data D1 without missing value, the other one is the data D2 with missing value. For no missing value data D1, calculate the information gain ratio with weight, and then multiply by a coefficient, which is the ratio of weighted samples with no feature A missing and weighted total samples. b. Data Spliting with missing value in A C4.5 can divide the samples with missing value A into all child nodes at the same time. But the weight of this sample is reassigned according to the proportion of the number of samples of each child node. For example, suppose there is a sample &quot;ms&quot; with missing value in feature A. The feature A has 3 kinds of value: A1, A2 and A3, and the number of samples are respectively 2,3,4, that is, the proportion of A1, A2 and A3 are 2/9, 3/9 and 4/9. Then the corresponding weights of missing value sample are adjusted to 2/9, 3/9 and 4/9. (4) Overfitting problem: C4.5 introduced regularization coefficients for preliminary pruning, which will be discussed in detail later. 5. Deficiency of decision tree C4.5 algorithm Althogh C4.5 algorithm improves ID3 a lot, there are still some areas worthy of improvement. (1) ID3 and C4.5 algorithms generate a multi-fork tree. Discrete feature with more than 2 kinds of value leads multiple child nodes. In many cases, the binary tree model in the computer will be more efficient than the multi-tree operation. Binary tree can improve efficiency. (2) C4.5 can only be used for classification. If the decision tree can be also used for regression, its scope of use can be expanded. (3) C4.5 uses the entropy model, there are lots of time-consuming logarithmic operations. If it is continuous feature, there is also a time-consuming sorting operation. It would be better if the model simplification can reduce the computational intensity without sacrificing too much accuracy. (4) Decision tree algorithm is very easy to overfit, the generated decision tree must be pruned. C4.5's pruning method still has room for optimization. There are two main ideas for pruning: pre-pruning, which is to decide whether to pruning when a decision tree is generated. post-pruning, that is, the decision tree is generated first, and then pruned tree through cross-validation. In the next section when we talk about the CART tree, we will specifically introduce the idea of reducing the branch of the decision tree, which mainly uses post-pruning plus cross-validation to select the most suitable decision tree. These 4 problems has been improved in CART algorithm. So if not consider integrated learning at present, in the ordinary decision tree algorithm, the CART algorithm is considered to be the better algorithm. 6. CART Classification Tree Algorithm -- Gini Coefficient Let's review at first, as we all know now: ID3 uses information gain to select features. I(D,A)=H(D)−H(D∣A)I(D,A) = H(D)-H(D|A)I(D,A)=H(D)−H(D∣A) C4.5 uses information gain ratio to select features. IR(D,A)=I(A.D)HA(D)I_R(D,A)=\\frac{I(A.D)}{H_A(D)}IR​(D,A)=HA​(D)I(A.D)​ Both of them are entropy models based on the information theory, H(X)=−∑i=1npi∗log(pi)H(X) = -\\sum_{i=1}^n p_i*log(p_i)H(X)=−∑i=1n​pi​∗log(pi​), which has lots of logarithmic computation and decrease the efficiency a lot. Is there any method we can simplify the model to increase the efficiency but without sacrificing too much accuracy? Yes, It is Gini Coefficient of CART. The Gini Coefficient represents the impureness of the node. The smaller the Gini, the lower the unpurification, the better classfication. This is opposite to Information Gain (Ratio) Specifically, in a classification problem, supposed there are K categories, and the probability of i-th category is pip_ipi​. Then the Gini coefficient is: Gini(p)=∑i=1Kpi(1−pi)=1−∑i=1Kpi2Gini(p) = \\sum_{i=1}^K p_i(1-p_i) = 1-\\sum_{i=1}^Kp_i^2 Gini(p)=i=1∑K​pi​(1−pi​)=1−i=1∑K​pi2​ If there is a binary classification, and the probability of first category is p, the Gini Coefficient is super easy: Gini(p)=2p(1−p)Gini(p) = 2p(1-p)Gini(p)=2p(1−p) For a sample D, if it has K categories, and the counts of i-th category is ∣Ci∣|C_i|∣Ci​∣, and the total number of D is |D|, then the Gini Coefficient of sample D is: Gini(D)=1−∑i=1K(∣Ci∣∣D∣)2Gini(D) = 1 - \\sum_{i=1}^K \\left(\\frac{|C_i|}{|D|}\\right)^2 Gini(D)=1−i=1∑K​(∣D∣∣Ci​∣​)2 And if the feature A (binary discrete or continuous) split the data into two subsets D1 and D2 by a certain value a of A. Then in the condition of feature A, the Gini Coefficient of D is: Gini(D,A)=∣D1∣∣D∣Gini(D1)+∣D2∣∣D∣Gini(D2)Gini(D,A) = \\frac{|D_1|}{|D|}Gini(D_1) + \\frac{|D_2|}{|D|}Gini(D_2) Gini(D,A)=∣D∣∣D1​∣​Gini(D1​)+∣D∣∣D2​∣​Gini(D2​) Comparing with entropy model formulas, the Quadratic computation of Gini Coefficient is easier a lot than logrithmic computation, especially in the binary classification. And so, Compared with the entropy model, how does the Gini coefficient perform? Take a look at the plot below. As we can see from the plot, the curves of Gini and Entropy (scaled) are very close, and the error is only slightly larger near the angle of 45 degrees. Therefore, the Gini can be used as an approximate substitute for the entropy model. In fact, the CART use Gini to select features of decision tree. And for further simplification, CART only divides each node to binary child nodes but not multi-nodes, so that the CART classification tree algorithm builds a binary tree instead of a multi-tree. In this way, the calculation of the Gini coefficient can be further simplified, and a more elegant binary tree model can be established. 7. CART Algorithm on continuous features and discrete features continuous features The idea of CART on continuous features is same with C4.5. Suppose the sample m has m values in continous feature A, CART sorts them from small to large, a1,a2,...,ama_1,a_2,...,a_ma1​,a2​,...,am​ and finds out (m-1) dividing points, which is the average value of each pair of neighbor points. For example, the i-th dividing point TiT_iTi​ is ai+ai+12\\frac{a_i+a_{i+1}}{2}2ai​+ai+1​​. CART calculates the Gini Coefficient value of each one of the dividing points, and selects the point with the lowest Gini Coefficient value. Pay attention: Unlike ID3, if the current node is the continuous feature, it will be still used in the following steps to create other child nodes. discrete features Unlike ID3 and C4.5, CART uses a non-stop binary classification, no matter how many kinds of values the feature has. For example, if the feature A has A1, A2, A3 three kinds of values, ID3 or C4.5 will build three child nodes {A1},{A2} and {A3}. But CART will consider 3 binary child nodes conditions: {A1} and {A2,A3}, {A2} and {A1,A3}, {A3} and {A1,A2}, then select the condition with lowest Gini Coefficient. And if CART selects {A2} and {A1,A3}, due to the value of feature A is not completely separated this time, CART will have the opportunity to continue to select feature A to divide A1 and A3 in the child node. Pay attention: In ID3 or C4.5, discrete feature will only participate in the establishment of a node once, because they builds multi-division tree. Unlike them, CART will reuse each discrete feature. Regardless of continuous features or discrete features, after the current node uses this feature to build a subtree, CART will still use this feature in the future subtree establishment. 8. CART classification tree establishment algorithm specific process The input of CART algorithm are training set D, threshold of Gini Coefficient, threshold of number of samples The outpuf of CART is the decision tree T CART algorithm starts from the root node, and uses the training set to recursively build the CART tree: For the dataset D of current node, if the number of records is smaller than the threshold number of samples, or there are no features, then return the decision subtree T, and current node stops recursive. Calculate the Gini Coefficient of dataset D of current node, Gini(D)Gini(D)Gini(D). If the Gini Coefficient is smaller than the threshold, then return the decision subtree T, and current node stops recursive. Calculate the Gini Coefficient of each feature value of the current node on the data set D, Gini(D,A)Gini(D,A)Gini(D,A). The processing of missing values is the same as that described in the C4.5 algorithm. Among all calculated Gini Coefficients of the pair of each feature and each feature's possible dividing point, choose the feature with the appropriate dividing point value that has smallest Gini Coefficient. According to this optimal feature and optimal feature value (dividing point), the dataset of current node is divided into two nodes D1 and D2, and the left and right nodes of the current node are established at the same time, the dataset D of the left node is D1, and the dataset D of the right node is D2. Steps 1-4 are recursively called on each of the left and right child nodes to generate a decision tree. When doing the prediction on the generated decision tree, if the sample &quot;P&quot; in prediction set falls into a certain leaf node, and there are multiple training samples in each leaf node, then the category prediction of sample &quot;P&quot; is the category with the highest probability in this leaf node. 9. CART regression tree building algorithm One advantage of CART is that it can be used not only as a classification model but also as a regression model. If the output of sample is the discrete value, we should train a classification tree. However, if the output of sample is the continuous value, we should train a regression tree. The algorithms for building CART regression trees and CART classification trees are mostly similar, so here we only discuss the differences between the algorithms for building CART regression trees and CART classification trees. The main difference between the establishment and prediction of CART regression tree and CART classification tree: Different methods for processing features. Different ways of making predictions after the decision tree is established. In feature selection and division, CART classification tree uses the Gini Coefficient to measure the pros and cons of each division node, which is suitable for classification problem. However, for the regression problem, we use variance measurement methods. The goal of the CART regression tree is that: For the data sets D1 and D2 divided on both sides of the arbitrary division feature &quot;A&quot; and the division point &quot;s&quot; Find the feature and feature division point corresponding to the minimum mean square deviation of each set of D1 and D2, and the minimum sum of mean square errors of D1 and D2. min⁡A,s[min⁡c1∑xi∈D1(A,s)(yi−c1)2+min⁡c2∑xi∈D2(A,s)(yi−c2)2]\\min_{A,s}\\left[\\min_{c_1} \\sum_{x_i \\in D_1(A,s)}(y_i-c_1)^2 + \\min_{c_2} \\sum_{x_i \\in D_2(A,s)}(y_i-c_2)^2 \\right] A,smin​⎣⎡​c1​min​xi​∈D1​(A,s)∑​(yi​−c1​)2+c2​min​xi​∈D2​(A,s)∑​(yi​−c2​)2⎦⎤​ (A,s) is the optimal feature and optimal division point of optimal feature. This is what we need in building decision tree. c1c_1c1​ and c2c_2c2​ are the average output value of dataset D1 and D2. (A,s) should minimize the above formula (Loss Function) In the prediction after the decision tree is established, the output of the regression tree is not a category. It uses the mean or median of the output value in final leaves to predict the output. Except for the above, there is no difference between CART regression tree and CART classification tree building algorithm and prediction. 10. Pruning of CART tree algorithm Since the decision tree algo is easy to overfitting after training, resulting in poor generalization ability. In order to preventing the overfitting problem, we need to prune the CART tree, that is, similar to regularization of the linear regression to increase the generalization ability. The pruning algorithm of the CART tree can be summarized in two steps: Generate various pruned decision tree from the original decision tree. Use cross-validation to test the prediction ability for all pruned tree. And the tree with the best generalization prediction ability is selected as the final CART tree. a. loss function of decision tree The pruning strategies of the CART regression tree and the CART classification tree are that one uses the mean square error and one uses the Gini coefficient when measuring the loss. There is a Classification tree T, |T| is the number of leaf nodes in the tree. &quot;t&quot; is one of leaf node of tree T: NtN_tNt​ is the number of samples in leaf node &quot;t&quot;, Ht(T)H_t(T)Ht​(T) is the entropy of leaf node &quot;t&quot;. In leaf node &quot;t&quot;, the number of k-th category samples is NtkN_{tk}Ntk​, k=1,2,...,K. (There are K categories in the nod) α≥0\\alpha \\ge 0α≥0 is the regularization coefficient Then the loss function of this classification decision tree Cα(T)C_{\\alpha}(T)Cα​(T) is: Cα(T)=∑t=1∣T∣NtHt(T)+α∣T∣C_{\\alpha}(T) = \\sum_{t=1}^{|T|}N_tH_t(T) + \\alpha|T| Cα​(T)=t=1∑∣T∣​Nt​Ht​(T)+α∣T∣ The entropy of node &quot;t&quot;, Ht(T)H_t(T)Ht​(T) is: Ht(T)=−∑k=1KNtkNtlog(NtkNt)H_t(T) = - \\sum_{k=1}^K \\frac{N_{tk}}{N_t} log(\\frac{N_{tk}}{N_t}) Ht​(T)=−k=1∑K​Nt​Ntk​​log(Nt​Ntk​​) We defined C(T) as the prediction error of the model on the training data, which can also represents the degree of fitting between the model and the training data. C(T)=∑t=1∣T∣NtHt(T)=−∑t=1∣T∣∑k=1KNtk∗log(NtkNt)\\begin{aligned} C(T) &amp;= \\sum_{t=1}^{|T|}N_tH_t(T) \\\\ &amp;= - \\sum_{t=1}^{|T|} \\sum_{k=1}^K N_{tk}*log(\\frac{N_{tk}}{N_t}) \\end{aligned}C(T)​=t=1∑∣T∣​Nt​Ht​(T)=−t=1∑∣T∣​k=1∑K​Ntk​∗log(Nt​Ntk​​)​ Then the loss function Cα(T)C_{\\alpha}(T)Cα​(T) can be written as: Cα(T)=C(T)+α∣T∣C_{\\alpha}(T) = C(T) + \\alpha|T| Cα​(T)=C(T)+α∣T∣ Whether it is Classification Tree (CT) or Regression Tree (RT), their loss function formula is same: $ C_{\\alpha}(T) = C(T) + \\alpha|T| $. The only difference is the calculation of prediction error C(T), which uses entroy or Gini coefficient in CT, and mean squared error in RT. Emphasize again, the loss function of decision tree: Cα(T)=C(T)+α∣T∣C_{\\alpha}(T) = C(T) + \\alpha|T| Cα​(T)=C(T)+α∣T∣ C(T) is the prediction error of the decision tree, or the degree of fitting between training data and the tree model. |T| is the number of leaf nodes in the tree, which can represent the complexity of the tree model. So we can easily find that, C(T) and |T| are two controdictory values. A very small C(T) means that the degree of fitting between the model and the training data is very high, and the prediction result of the training set is very accurate (but the performance of test data is hard to say). Then complexity |T| should be very large. Similarly, a simple tree with low |T| (low complexity) has large C(T) (high prediction error in training set). The goal is to balance the accuracy and complexity of the model so that the combined loss function of the two is minimal. α≥0\\alpha \\ge 0α≥0 is also an important coefficient in the loss function. α\\alphaα is large =&gt; prompt to choose a simpler tree (small |T|) α\\alphaα is small =&gt; prompt to choose a more complex tree (large |T|) α\\alphaα is 0 =&gt; No pruning, current tree is the optimal. Only consider the fitting between model and data, and ignore the model complexity. (easily cause overfitting) α\\alphaα is +∞+\\infty+∞ =&gt; All pruning, only the root node is left. So pruning means when α\\alphaα is determined, select a Tree model with the least loss function value Cα(T)C_{\\alpha}(T)Cα​(T). General speaking, the larger the α\\alphaα is, the stronger the pruning is, and the smaller the optimal tree is (compared with the original decision tree) For a fixed α\\alphaα, there must be a unique subtree that minimizes the loss function Cα(T)C_{\\alpha}(T)Cα​(T). b. idea of pruning All above is the method of measuring the loss function of pruning tree. And the next is the idea of pruning. For any subtree TtT_tTt​ at the node t, if no pruning, its original loss is: Cα(Tt)=C(Tt)+α∣Tt∣C_{\\alpha}(T_t) = C(T_t) + \\alpha|T_t| Cα​(Tt​)=C(Tt​)+α∣Tt​∣ ∣Tt∣|T_t|∣Tt​∣ is the total number of leaf nodes of the node &quot;t&quot;. If prune the node &quot;t&quot; and only the root node is left, its loss after pruning is: Cα(t)=C(t)+αC_{\\alpha}(t) = C(t) + \\alpha Cα​(t)=C(t)+α Since, only the root node as one leaf node is left after pruning. α∣t∣=α\\alpha|t| = \\alphaα∣t∣=α Subtree &quot;TtT_tTt​&quot; is more complex than subtree &quot;t&quot;, the error C(Tt)≤C(t)C(T_t) \\le C(t)C(Tt​)≤C(t). So, if α\\alphaα is 0 or very small, Cα(Tt)≤Cα(t)C_{\\alpha}(T_t) \\le C_{\\alpha}(t)Cα​(Tt​)≤Cα​(t). When α\\alphaα increases to a certain degree, it will meet: Cα(Tt)=Cα(t)C_{\\alpha}(T_t) = C_{\\alpha}(t) Cα​(Tt​)=Cα​(t) If α\\alphaα continues to increase, then Cα(Tt)≥Cα(t)C_{\\alpha}(T_t) \\ge C_{\\alpha}(t)Cα​(Tt​)≥Cα​(t). In other words, the critical value of α\\alphaα derived from the equality before and after pruning is: C(Tt)+α∣T∣=C(t)+αα=C(t)−C(Tt)∣T∣−1\\begin{aligned} C(T_t) + \\alpha|T| &amp;= C(t) + \\alpha \\\\ \\alpha &amp;= \\frac{C(t)-C(T_t)}{|T|-1} \\end{aligned}C(Tt​)+α∣T∣α​=C(t)+α=∣T∣−1C(t)−C(Tt​)​​ In the pruning of node &quot;t&quot; Before pruning, lost of &quot;t&quot;: Cα(Tt)C_{\\alpha}(T_t)Cα​(Tt​) After pruning, lost of &quot;t&quot;: Cα(t)C_{\\alpha}(t)Cα​(t) There are 3 conditions of loss value after pruning: increase, unchange or decrease. If the loss value doesn't increase after pruning, that is, if Cα(Tt)≥Cα(t)C_{\\alpha}(T_t) \\ge C_{\\alpha}(t)Cα​(Tt​)≥Cα​(t), then pruning the subtree is better. It shows that complexity plays a key role, and the loss function plays a small role. In simple terms, the leaf nodes before pruning do not improve the accuracy but bring more complicated trees. c. CART pruning algorithm process Since, for a fixed α\\alphaα, there must be a unique subtree that minimizes the loss function Cα(T)C_{\\alpha}(T)Cα​(T). We can mark this subtree as TαT_{\\alpha}Tα​ Breiman has proved that the tree can be pruned recursively. Increase α\\alphaα from small to large, a0&lt;a1&lt;...&lt;an&lt;+∞a_0 &lt; a_1 &lt; ... &lt; a_n &lt; +\\inftya0​&lt;a1​&lt;...&lt;an​&lt;+∞, producing a series of intervals [ai,ai+1)[a_i,a_{i+1})[ai​,ai+1​), i=0,1,2,...,n. The optimal subtree sequence obtained by pruning is {T0,T1,...,TnT_0,T_1,...,T_nT0​,T1​,...,Tn​} Then in the optimal subtree sequence {T0,T1,...,TnT_0,T_1,...,T_nT0​,T1​,...,Tn​}, select the optimal subtree TαT_{\\alpha}Tα​ through cross-validation. The CART pruning algorithm: Input: the original decision tree T0T_0T0​ obtained by the CART tree algorithm in section 8. Output: Optimal decision subtree TαT_{\\alpha}Tα​ The algorithm process: Initialize αmin=+∞\\alpha_{min}=+\\inftyαmin​=+∞, Set of optimal subtree w={T0}w = \\{T_0\\}w={T0​} Calculate Cα(Tt),∣Tt∣,αC_{\\alpha}(T_t), |T_t|, \\alphaCα​(Tt​),∣Tt​∣,α Calculate the training error loss function Cα(Tt)C_{\\alpha}(T_t)Cα​(Tt​) of each internal node &quot;t&quot; from the leaf node from bottom to top. (The regression tree is the mean square error, and the classification tree is the Gini coefficient). Calculate the total number of leaf nodes of &quot;t&quot;, ∣Tt∣|T_t|∣Tt​∣, and Calculate the threshold of regularization cofficient α=min⁡{C(t)−C(Tt)∣Tt∣−1,αmin}\\alpha= \\min \\left\\{\\frac{C(t)-C(T_t)}{|T_t|-1}, \\alpha_{min} \\right\\}α=min{∣Tt​∣−1C(t)−C(Tt​)​,αmin​} Update the αmin=α\\alpha_{min} = \\alphaαmin​=α Get the set M of α\\alphaα values for all nodes. Chosse the smallest regularization coefficient value αk\\alpha_kαk​ from the set M, then access the internal nodes &quot;t&quot; from top to bottom. If C(t)−C(Tt)∣Tt∣−1≤αk\\frac{C(t)-C(T_t)}{|T_t|-1} \\le \\alpha_{k}∣Tt​∣−1C(t)−C(Tt​)​≤αk​, pruning is performed. And determine the value of the leaf node t. If it is a classification tree, it is the category with the highest probability; if it is a regression tree, it is the average of all sample outputs. The optimal subtree corresponding to the regularization coefficient αk\\alpha_{k}αk​ is obtained TkT_kTk​ Update the optimal subtree set w=w∪Tkw = w \\cup T_kw=w∪Tk​, and regularization coefficient set M=M−{αk}M=M-\\{\\alpha_k\\}M=M−{αk​} If www is not null, back to step 4. Else, all optimal subtrees with different α\\alphaα have been obtained in set www. Use cross-validation to test the prediction accuracy for all pruned tree in optimal subtree set www. And the tree with the best generalization prediction ability is returned as the final CART tree. 11. Summary of CART algorithm Algorithms Model Support Tree Structure Feature Selection Continuous Feature Missing Value Pruning ID3 Classification General Tree (Multitree) Information Gain Not Support Not Support Not Support C4.5 Classification General Tree (Multitree) Information Gain Ratio Support Support Support CART Classification &amp; Regression Binary Tree Gini Coefficient; Mean Square Error Support Support Support The CART algorithm still has some shortcomings: Whether it is ID3, C4.5 or CART, when making feature selection, they all choose the best one feature to build the decision tree node. However, in some case, the classification decision should not be determined by only one certain feature, but should be determined by a set of features. The decision tree obtained in this way is more accurate, which is called a multi-variate decision tree. When selecting the optimal feature, the multivariate decision tree does not select one certain optimal feature, but selects an optimal linear combination of features to make a decision. The representative of multi-variate decision tree algorithm is &quot;OC1&quot;. If the sample changes a little bit, it will cause a dramatic change in the tree structure. This can be solved by ensemble models, such as random forest in integrated learning. 12. Summary of Decision Tree Advantages of Decision Tree: The generated decision tree is very simple and intuitive. Basically no preprocessing is needed, such as normalization in advance, and missing values preprocessing. The cost in prediction of decision tree is O(log2m)O(log_2m)O(log2​m), m is the number of samples. Both discrete and continuous values can be processed. Many algorithms only focus on discrete values or continuous values. It can deal with multi categories classification problem. Compared with the black box classification model such as neural network, the decision tree can be well explained logically. Cross-validated pruning can be used to filter the model, thereby improving the generalization ability. High tolerance for some abnormal points. Disadvantages of Decision Tree: Decision tree algorithm is very easy to overfit, resulting in weak generalization ability. It can be improved by setting the minimum sample number of nodes and limiting the depth of decision tree. The decision tree structure will change dramatically when sample changes a little bit. This can be solved by ensemble models such as random forest. Finding the optimal decision tree is an NP-hard problem. We usually get into the local optimal by heuristic method. It can be improved by methods such as integrated learning. For some more complex relationships, decision trees are difficult to learn, such as XOR. There is no way for this. Generally, this relationship can be solved by using neural network classification methods. If the sample ratio of certain features is too large, generating decision trees tends to favor these features. This can be improved by adjusting the sample weights. ","link":"https://zl-wu.github.io/post/mla_decision_tree/"},{"title":"MLA -- Logistic Regression (sklearn instruction)","content":"This is a summary of using logistic regression libraries in sklearn. Focus on the matters that should be paid attention to in the tuning. Reference: sklearn's linear model Sklearn-LR code example In the previous article Logistic Regression Principle, it summarized the principle of logistic regression. Here is a summary of using logistic regression libraries in sklearn. Focus on the matters that should be paid attention to in the tuning. 1. Overview There are mainly 3 Class of logistic regression: LogisticRegression LogisticRegressionCV logistic_regression_path The main difference between LogisticRegression and LogisticRegressionCV is that LogisticRegressionCV uses cross-validation to select the regularization coefficient C. LogisticRegression needs to specify a regularization coefficient each time. Except this coefficient C, LogisticRegression and LogisticRegressionCV are basically used in the same way. The logistic_regression_path class is special. After fitting the data, it cannot directly make predictions, and can only select the appropriate logistic regression coefficient and regularization coefficient for the fitted data. It is mainly used in model selection. This class is generally not used. 2. Regularization Parameter: Penalty LogisticRegression and LogisticRegressionCV have regularization terms by default. Parameter penalty can be 'l1','l2','elasticnet','none'. Default value is 'l2'. l1l_1l1​: min⁡w,c∥w∥1+C∑i=1mlog(exp(−yi(XiTw+c))+1)\\min_{w,c}\\|w\\|_1 + C\\sum_{i=1}^mlog(exp(-y_i(X_i^Tw+c))+1) w,cmin​∥w∥1​+Ci=1∑m​log(exp(−yi​(XiT​w+c))+1) l2l_2l2​: min⁡w,c12wTw+C∑i=1mlog(exp(−yi(XiTw+c))+1)\\min_{w,c}\\frac{1}{2}w^Tw + C\\sum_{i=1}^mlog(exp(-y_i(X_i^Tw+c))+1) w,cmin​21​wTw+Ci=1∑m​log(exp(−yi​(XiT​w+c))+1) elasticnet: a combination of L1 and L2, and minimizes the following cost function: min⁡w,c1−ρ2wTw+ρ∥w∥1+C∑i=1mlog(exp(−yi(XiTw+c))+1)\\min_{w,c} \\frac{1-\\rho}{2}w^Tw + \\rho\\|w\\|_1 + C\\sum_{i=1}^mlog(exp(-y_i(X_i^Tw+c))+1) w,cmin​21−ρ​wTw+ρ∥w∥1​+Ci=1∑m​log(exp(−yi​(XiT​w+c))+1) When tuning the parameters, if the main purpose is to solve the overfitting, it is generally enough to choose L2 regularization for the penalty. However, if L2 regularization is still overfitting, that is, if the prediction performance is poor with L2, then L1 regularization can be considered. In addition, if the model has lots of features, we hope that some unimportant features' coefficients be zeroed, so that the model coefficients can be sparse, L1 regularization can also be used in this case. The choice of penalty parameter will affect the choice of loss function optimization algorithm (solver parameter): If penalty=&quot;l2l_2l2​&quot; (Ridge Regression), there are 4 optional algorithms (solver): &quot;newton-cg&quot; &quot;lbfgs&quot; &quot;liblinear&quot; &quot;sag&quot; If penalty=&quot;l1l_1l1​&quot; (Lasso Regression), there are only 1 optional algorithms (solver): solver=&quot;liblinear&quot;. Because the loss function with L1 regularization is not derivable, only &quot;liblinear&quot; can solve it. the other three methods [&quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;sag&quot;] requires the first or second continuous derivative of the loss function. 3. Optimization Algorithm Parameter: Solver There are 5 algorithms can be choosed to optimize Logistic Regression's Loss Function: &quot;newton-cg&quot;: A type of Newton's method family, iteratively optimizes the loss function by using the second derivative matrix of the loss function, the Hessian matrix. &quot;lbfgs&quot;: A kind of quasi-Newton method, iteratively optimizes the loss function by using the second derivative matrix of the loss function, the Hessian matrix. &quot;liblinear&quot;: Coordinate Axis Descent method &quot;sag&quot;: the random average gradient descent, a variant of the gradient descent method. The difference from the ordinary gradient descent method is that only a part of the samples are used to calculate the gradient in each iteration, which is suitable when there are many sample data. (like Mini-Batch Gradient Descent) &quot;saga&quot;: The “sag” solver uses Stochastic Average Gradient descent 6. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large. The “saga” solver 7 is a variant of “sag” that also supports the non-smooth penalty=&quot;l1&quot;. This is therefore the solver of choice for sparse multinomial logistic regression. It is also the only solver that supports penalty=&quot;elasticnet&quot;. &quot;sag&quot; uses only a part of the samples, so don't choose it when the sample size is small. And if the sample size is very large, such as greater than 100,000, &quot;sag&quot; is the first choice. But sag cannot be used for L1 regularization, so when you have a large number of samples and need L1 regularization, you have to make a trade-off. Either reduce the sample size by sampling the samples, or return to L2 regularization. Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing. Algorithm to use in the optimization problem. For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones. For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes. ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty ‘liblinear’ and ‘saga’ also handle L1 penalty ‘saga’ also supports ‘elasticnet’ penalty ‘liblinear’ does not support setting penalty='none' References L-BFGS-B – Software for Large-scale Bound-constrained Optimization Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales. http://users.iems.northwestern.edu/~nocedal/lbfgsb.html LIBLINEAR – A Library for Large Linear Classification https://www.csie.ntu.edu.tw/~cjlin/liblinear/ SAG – Mark Schmidt, Nicolas Le Roux, and Francis Bach Minimizing Finite Sums with the Stochastic Average Gradient https://hal.inria.fr/hal-00860051/document SAGA – Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014). SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives https://arxiv.org/abs/1407.0202 Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent methods for logistic regression and maximum entropy models. Machine Learning 85(1-2):41-75. https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf 4. Classification Type Parameter: Multi_class There are three values can be choose of the parameter multi_class: &quot;ovr&quot;: one-vs-rest(OvR) &quot;multinomial&quot;: many-vs-many(MvM) &quot;auto&quot; If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. ‘multinomial’ is unavailable when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’. The idea of OvR is very simple. No matter how many catogeries in logistic regression classification model, we can treat them as many binary logistic regression. For example, if there are K categories, we view each one of K categories as postive and the rest are negative. Finally we get K-1 classifiers. (like cross validation) The idea of MvM is relatively complicated. For example, if there are T categories, we pick two categories's data from the samples each time, and noted them as T1 and T2 classes (there are CT2C_T^2CT2​ ways). Use T1 as a positive class, and T2 as a negative class to perform binary logistic regression to obtain model parameters. Finally, We need $T(T-1) / 2 $ classifications in total. As we can see, OvR is relatively simple, but the classification performance is relatively poor.(here refers to the distribution of most samples, OvR may be better under certain sample distributions). The MvM classification is relatively accurate, but the classification speed is not as fast as OvR. If choose &quot;ovr&quot;, all 5 solvers can be chose: liblinear, newton-cg, lbfgs, sag, saga. If choose &quot;multinomial&quot;, except &quot;liblinear&quot;, we can use all other 4 solvers. 5. Type Weight Parameter: class_weight class_weight: dict or ‘balanced’, default=None The class_weight parameter is used to indicate the various categories' weights in the classification model. If None, all categories' weights are the same. Or we can input the weights of each category manually. For a binary model of 0 and 1 as an example, we can define the class_weight = {0: 0.9, 1: 0.1}, so that the weight of type 0 is 90%, and the weight of type 1 is 10%. If class_weight=&quot;balanced&quot;, then the library will calculate the weight based on the training sample size. The larger the sample size of a certain type, the lower the weight, and the smaller the sample size, the higher the weight. So what does class_weight do? In the classification model, we often encounter two types of problems: cost of misclassification. For example, it is very expensive to classify legal users and illegal users and classify illegal users as legal users. We prefer to classify legal users as illegal users. At this time, we can manually re-screen, but we do not want to classify illegal users as legal users . At this time, we can appropriately increase the weight of illegal users. the sample is highly unbalanced. For example, we have 10,000 binary sample data of legal users and illegal users. There are 9995 legal users and only 5 illegal users. If we do n’t consider the weight, we can divide all Of the test sets are predicted as legitimate users, so the prediction accuracy rate is 99.95% in theory, but it does not make any sense. At this time, we can choose balanced to let the class library automatically increase the weight of illegal user samples. And for the second type of sample imbalance, we can also consider using the sample weight parameter mentioned in the next section: sample_weight instead of class_weight. 6. sample_weight Due to the sample imbalance, the sample is not an unbiased estimate of the overall sample, which may lead to a decline in our model's predictive ability. In this case, we can try to solve this problem by adjusting the sample weight. There are two ways to adjust the sample weight. The first is to use balanced in class_weight. The second is to adjust the weight of each sample by sample_weight when calling the fit function. When doing logistic regression in scikit-learn, if the above two methods are used, then the real weight of the sample is class_weight * sample_weight. ","link":"https://zl-wu.github.io/post/mla-logistic-regression-sklearn-instruction/"},{"title":"MLA -- Regularization Lasso Regression","content":"&quot;What is the lasso regression? Add L2 norm into loss function.&quot; Norm is a commonly used concept in mathematics, which is also often encountered in machine learning. The first thing that needs to be clear is that Norm is a function. We usually use it to measure the value of the vector in machine learning. The norm is defined as: ∥x∥p=(∑i=1m∣xi∣p)1/p\\|x\\|_p = \\left(\\sum_{i=1}^m |x_i|^p\\right)^{1/p} ∥x∥p​=(i=1∑m​∣xi​∣p)1/p Common Norm: L2L^2L2 Norm: When p is 2, L2L^2L2 Norm is also called Euclidean norm. It represents the distance from the original to the points determined by the vector x. It is applied very frequently in machine learning. ∥x∥2=(∑i=1m∣xi∣2)1/2\\|x\\|_2 = \\left(\\sum_{i=1}^m |x_i|^2\\right)^{1/2} ∥x∥2​=(i=1∑m​∣xi​∣2)1/2 Square L2L^2L2 Norm: It is square of L2L^2L2 norm, $ |x|_2^2$. The advantage is that it is obviously easier to calculate, which can be simply calculated by the dot product of XTXX^TXXTX. L1L^1L1 Norm: In some case, L2L^2L2 Norm is not very popular, because it grows very slowly near the origin. Sometimes, it is important to distinguish elements that happen to be zero and non-zero but have very small value. In this case, we can use L1L^1L1 Norm: ∥x∥1=∑i=1m∣xi∣\\|x\\|_1 = \\sum_{i=1}^m |x_i| ∥x∥1​=i=1∑m​∣xi​∣ L∞L^\\inftyL∞ Norm: It is also called Maximum norm, which is the absolute value of the element with largest amplitude in the vector: ∣∣x∣∣∞=max∣xi∣||x||_\\infty = max|x_i| ∣∣x∣∣∞​=max∣xi​∣ Regularization in Regression 1. Linear Regression Review The norm form of linear regression: hθ(X)=Xθh_\\theta(X) = X\\theta hθ​(X)=Xθ The loss function that we need to minimize: J(θ)=12(Xθ−Y)T(Xθ−Y)J(\\theta) = \\frac{1}{2}(X\\theta - Y)^T(X\\theta - Y) J(θ)=21​(Xθ−Y)T(Xθ−Y) Gradient Descent:θ=θ−βXT(Xθ−Y)\\theta = \\theta - \\beta X^T(X\\theta - Y) θ=θ−βXT(Xθ−Y) Least Square:θ=(XTX)−1XTY\\theta = (X^TX)^{-1}X^TY θ=(XTX)−1XTY 2. Ridge Regression Review Since applying linear regression directly may produce verfitting problem, we need to add the regularization term. When L2L^2L2 Norm is added, it is called Ridge Regression. J(θ)=12(Xθ−Y)T(Xθ−Y)+12α∥θ∥22J(\\theta) = \\frac{1}{2}(X\\theta - Y)^T(X\\theta - Y) + \\frac{1}{2}\\alpha\\|\\theta\\|_2^2 J(θ)=21​(Xθ−Y)T(Xθ−Y)+21​α∥θ∥22​ α\\alphaα is the constant coeffient, which is used to adjust the weights of linear regression term and regularization term. ∥θ∥2\\|\\theta\\|_2∥θ∥2​ is the L2 norm of θ\\thetaθ vector. Ridge Regression is very simialr to normal linear regression. Gradient Descent: θ=θ−β(XT(Xθ−Y)+αθ)=θ(1−αβ)−β(XT(Xθ−Y))\\begin{aligned} \\theta &amp;= \\theta - \\beta(X^T(X\\theta-Y)+\\alpha\\theta) \\\\ &amp;=\\theta(1-\\alpha\\beta) - \\beta(X^T(X\\theta-Y)) \\end{aligned} θ​=θ−β(XT(Xθ−Y)+αθ)=θ(1−αβ)−β(XT(Xθ−Y))​ Least Square: θ=(XTX+αE)−1XTY\\theta = (X^TX + \\alpha E)^{-1}X^TY θ=(XTX+αE)−1XTY Ridge regression reduces the regression coefficients without abandoning any variables. When the coefficient is close to 0, it is equivalent to weakening the significance of its feature. But this model still has a lot of variables, and the model is poorly interpretable. Is there a compromise? That is, it can prevent overfitting and overcome the shortcomings of the Ridge regression model with many variables? Yes, this is the Lasso regression mentioned below. 2. Lasso Regression Similar as Ridge Regreesion, Lasso Regression also adds a Norm term in the loss function to try to avoid overfitting. When L1L^1L1 Norm is added, it is called Lasso Regression. J(θ)=12m(Xθ−Y)T(Xθ−Y)+α∥θ∥1J(\\theta) = \\frac{1}{2m}(X\\theta - Y)^T(X\\theta - Y) + \\alpha\\|\\theta\\|_1 J(θ)=2m1​(Xθ−Y)T(Xθ−Y)+α∥θ∥1​ m is the count of sample data. α\\alphaα is the constant coeffient, which is used to adjust the weights of linear regression term and regularization term. It needs to be tuned. ∥θ∥1\\|\\theta\\|_1∥θ∥1​ is the L1 norm of θ\\thetaθ vector. Lasso regression makes some coefficients smaller, and even some coefficients with smaller absolute values directly become 0, so it is especially suitable for the reduction of the number of parameters and the selection of parameters, so it is used to estimate the linear model of sparse parameters. However, there is a big problem with Lasso regression. Its loss function is not continuous and differentiable. Since the L1 norm uses the sum of absolute values, the loss function is not derivative. That is to say, Least Square method, Gradient Descent method, Newton method and Quasi-Newton method all failed in this case. So how can we find the minimum value of the loss function with this L1 norm? There are two new methods to get extreme value: Coordinate Descent Least Angle Regression, LARS 3. Coordinate Descent method for Lasso Regression As the name suggests, Coordinate Descent is the descent in the direction of the coordinate axis, different from the gradient direction in the gradient descent. But both are iterative methods in a heuristic way. Algorithm Process: Intialize θ\\thetaθ as θ(0)\\theta^{(0)}θ(0). 0 represents the current iteration is 0. In k-th iteration, we calculate θi(k)\\theta_i^{(k)}θi(k)​ started from θ1(k)\\theta_1^{(k)}θ1(k)​ to θn(k)\\theta_n^{(k)}θn(k)​: θi(k)=arg min⁡θiJ(θ1(k),θ2(k),...,θi−1(k),θi,θi+1(k−1),...,θn(k−1))\\theta_i^{(k)}=\\argmin_{\\theta_i} J(\\theta_1^{(k)},\\theta_2^{(k)},...,\\theta_{i-1}^{(k)},\\theta_{i},\\theta_{i+1}^{(k-1)},...,\\theta_{n}^{(k-1)} ) θi(k)​=θi​argmin​J(θ1(k)​,θ2(k)​,...,θi−1(k)​,θi​,θi+1(k−1)​,...,θn(k−1)​) &gt;In this case, $J(\\theta)$ only has one variable $\\theta_i$, and others are all constant. Hence, the minimum value of $J(\\theta)$ can be easily obtained by differentiation. Let's be more specific, in k-th iteration: θ1(k)∈arg min⁡θ1J(θ1,θ2(k−1),...,θn(k−1))θ2(k)∈arg min⁡θ2J(θ1(k),θ2,θ3(k−1)...,θn(k−1))...θn(k)∈arg min⁡θnJ(θ1(k),θ2(k),...,θn−1(k),θn)\\begin{aligned} \\theta_1^{(k)} &amp;\\in \\argmin_{\\theta_1}J(\\theta_1, \\theta_2^{(k-1)},...,\\theta_n^{(k-1)}) \\\\ \\theta_2^{(k)} &amp;\\in \\argmin_{\\theta_2}J(\\theta_1^{(k)}, \\theta_2, \\theta_3^{(k-1)}...,\\theta_n^{(k-1)}) \\\\ ... \\\\ \\theta_n^{(k)} &amp;\\in \\argmin_{\\theta_n}J(\\theta_1^{(k)}, \\theta_2^{(k)}, ...,\\theta_{n-1}^{(k)},\\theta_n) \\end{aligned} θ1(k)​θ2(k)​...θn(k)​​∈θ1​argmin​J(θ1​,θ2(k−1)​,...,θn(k−1)​)∈θ2​argmin​J(θ1(k)​,θ2​,θ3(k−1)​...,θn(k−1)​)∈θn​argmin​J(θ1(k)​,θ2(k)​,...,θn−1(k)​,θn​)​ Comparing the θ(k)\\theta^{(k)}θ(k) vector with θ(k−1)\\theta^{(k-1)}θ(k−1) vector, if the changes are small enough, then θ(k)\\theta^{(k)}θ(k) is the final return. Otherwise, jumping to Step 2 and continuing (k+1)-th iteration. 4. Least Angle Regression method for Lasso Regression Before introducing Least Angle Regression, let ’s look at two preliminary algorithms: (Unfinished) 4.1 Forward Selection Algorithm 4.2 Forward Stagewise Algorithm 4.3 Least Angle Regression Algorithm (LARS) 5. Conclusion Reference: LEAST ANGLE REGRESSION By BRADLEY EFRON [web]( ","link":"https://zl-wu.github.io/post/mla-regularization-lasso-regression/"},{"title":"MLA -- Logistic Regression","content":"&quot;Logistic Regression is a kind of very classical and basic classification algorithm.&quot; Logistic Regression is a kind of classification algorithm, it can implement binary classification and multiple classification. It is also a supervised learning algorithm, it implements a mapping from a given data set to 0 and 1 in binary case. Although it is a classification model, the principle of regression still remains in the model. 1. From Linear Regression to Logistic Regression In Linear Regression model, the linear relationship between output vector Y and input sample matrix X is: $ Y = X\\theta $. In this case, Y is continuous, hence, it is a regression model. If we want the output Y is the descrete value (for classification), one method is to do a second transformation on Y, g(Y). Through the function g(Y), all continous value can be mapped to descrete value. For example, Y belongs to category A when it is in a real number interval, and category B when it is in another real number interval. 2. Binary Logistic Regression model In Logistic Regression, we usually use a special function g(Y) to transform continous value Y: g(z)=11+e−zg(z) = \\frac{1}{1+e^{-z}} g(z)=1+e−z1​ This function has some very good properties, which are suitable for classification probability model: g(z)∈(0,1)g(z) \\in (0,1)g(z)∈(0,1) When z tends to positive infinity +∞+\\infty+∞, g(z) tends to 1. When z tends to negative infinity −∞-\\infty−∞, g(z) tends to 0. A good derivative propertiy:g′(z)=g(z)(1−g(z))g&#x27;(z) = g(z)(1-g(z)) g′(z)=g(z)(1−g(z)) If z=xθz=x\\thetaz=xθ, the general form of logistic regression model is: hθ(x)=11+e−xθh_\\theta(x) = \\frac{1}{1+e^{-x\\theta}} hθ​(x)=1+e−xθ1​ x is one 1xn data vector to be predicted. (n features) θ\\thetaθ is a nx1 parameters vector. xθx\\thetaxθ is a linear regression scalar. hθ(x)h_\\theta(x)hθ​(x) can be understood as the probability of one certain category. We have this correspondence with our binary sample output y (assuming 0 and 1): If hθ(x)h_\\theta(x)hθ​(x) &gt; 0.5 (xθx\\thetaxθ &gt; 0), predict y = 1. If hθ(x)h_\\theta(x)hθ​(x) &lt; 0.5 (xθx\\thetaxθ &lt; 0), predict y = 0. The smaller the value of hθ(x)h_\\theta(x)hθ​(x), the higher the probability of being classified as 0. Conversely, the larger the value of hθ(x)h_\\theta(x)hθ​(x), the higher the probability of being classified as 1. If it is close to the critical point (0.5), the classification accuracy will decrease. In matrix expression: hθ(X)=11+e−Xθh_\\theta(X) = \\frac{1}{1+e^{-X\\theta}} hθ​(X)=1+e−Xθ1​ hθ(X)h_\\theta(X)hθ​(X) is the output of logistic regression model. mx1 dimensions. X is the input sample data features matrix. mxn dimensions. θ\\thetaθ is the model coefficients for classification. nx1 dimensions. After understanding the Logistic Regression model of binary classification, we have to look at the loss function of the model, our goal is to minimize the loss function to get the corresponding model coefficients θ\\thetaθ. 3. Loss Function of Binary Logistic Regression Since the linear regression model is continuous, its loss function is Square Sum of Error. However, the logistic funtion is not continuous, SSE is not feasible in this case (will prove it later in the article). Now, we can use Maximum Likelihood Method to derive the loss function. Suppose the output sample is two value: 0 or 1, then it follows Bernoulli distribution. P(y=1∣x,θ)=hθ(x)P(y=0∣x,θ)=1−hθ(x)\\begin{aligned} &amp;P(y=1|x,\\theta) = h_\\theta(x) \\\\ &amp;P(y=0|x,\\theta) = 1- h_\\theta(x) \\end{aligned} ​P(y=1∣x,θ)=hθ​(x)P(y=0∣x,θ)=1−hθ​(x)​ Combine above two formula, the probability distribution function is: P(y∣x,θ)=hθ(x)y(1−hθ(x))1−yP(y|x,\\theta) = h_\\theta(x)^y(1-h_\\theta(x))^{1-y} P(y∣x,θ)=hθ​(x)y(1−hθ​(x))1−y y∈{0,1}y \\in \\{0,1\\}y∈{0,1} Through likelihood function maxmization, we can derive the model coefficient parameters θ\\thetaθ that we need. Assume that the samples are independent and identically distributed, Likelihood Function is: L(θ)=∏i=1m(hθ(x(i)))y(i)(1−hθ(x(i)))1−y(i) L(\\theta) = \\prod_{i=1}^m (h_\\theta(x^{(i)}))^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{1-y^{(i)}} L(θ)=i=1∏m​(hθ​(x(i)))y(i)(1−hθ​(x(i)))1−y(i) m is the counts of sample The function that inverts the logarithmization of the likelihood function, that is, the loss function is: J(θ)=−ln(L(θ))=−∑i=1m(y(i)ln(hθ(x(i)))+(1−y(i))ln(1−hθ(x(i))))\\begin{aligned} J(\\theta) &amp;= -ln(L(\\theta)) \\\\ &amp;=-\\sum_{i=1}^m (y^{(i)}ln(h_\\theta(x^{(i)})) + (1-y^{(i)})ln(1-h_\\theta(x^{(i)}))) \\end{aligned} J(θ)​=−ln(L(θ))=−i=1∑m​(y(i)ln(hθ​(x(i)))+(1−y(i))ln(1−hθ​(x(i))))​ In Matrix Expression: J(θ)=−YTln(hθ(X))−(E−Y)Tln(E−hθ(X))J(\\theta) = -Y^Tln(h_\\theta(X)) - (E-Y)^Tln(E-h_\\theta(X)) J(θ)=−YTln(hθ​(X))−(E−Y)Tln(E−hθ​(X)) E is an all 1 vector. 4. Optimization method of Loss Function There are lots of method to minimize the loss function of Logistic Regression model. Most common methods include gradient descent, coordinate descent, and Newton's method. Only each iteration's formula of gradient descent method is derived here. 4.1 Algebraic Expression J(θ)=−∑i=1m(y(i)ln(hθ(x(i)))+(1−y(i))ln(1−hθ(x(i))))J(\\theta) = -\\sum_{i=1}^m (y^{(i)}ln(h_\\theta(x^{(i)})) + (1-y^{(i)})ln(1-h_\\theta(x^{(i)}))) J(θ)=−i=1∑m​(y(i)ln(hθ​(x(i)))+(1−y(i))ln(1−hθ​(x(i)))) ∂∂θjJ(θ)=−∂∑i=1m(y(i)ln(hθ(x(i)))+(1−y(i))ln(1−hθ(x(i))))∂θj=−∑i=1my(i)hθ(x(i))∂hθ(x(i))∂θj−1−y(i)1−hθ(x(i))∂hθ(x(i))∂θj=−∑i=1m(y(i)hθ(x(i))−1−y(i)1−hθ(x(i)))∂hθ(x(i))∂θj=−∑i=1my(i)−hθ(x(i))hθ(x(i))(1−hθ(x(i)))hθ′(x(i))∂Xθ∂θj=−∑i=1my(i)−hθ(x(i))hθ(x(i))(1−hθ(x(i)))hθ(x(i))(1−hθ(x(i)))xj(i)=−∑i=1m(y(i)−hθ(x(i)))xj(i)=∑i=1m(hθ(x(i))−y(i))xj(i)\\begin{aligned} \\frac{\\partial}{\\partial\\theta_j}J(\\theta) &amp;= -\\frac{\\partial\\sum_{i=1}^m (y^{(i)}ln(h_\\theta(x^{(i)})) + (1-y^{(i)})ln(1-h_\\theta(x^{(i)})))}{\\partial\\theta_j} \\\\ &amp;= - \\sum_{i=1}^m \\frac{y^{(i)}}{h_\\theta(x^{(i)})} \\frac{\\partial h_\\theta(x^{(i)})}{\\partial\\theta_j} - \\frac{1-y^{(i)}}{1-h_\\theta(x^{(i)})} \\frac{\\partial h_\\theta(x^{(i)})}{\\partial\\theta_j} \\\\ &amp;= - \\sum_{i=1}^m (\\frac{y^{(i)}}{h_\\theta(x^{(i)})} - \\frac{1-y^{(i)}}{1-h_\\theta(x^{(i)})}) \\frac{\\partial h_\\theta(x^{(i)})}{\\partial\\theta_j} \\\\ &amp;= - \\sum_{i=1}^m \\frac{y^{(i)} - h_\\theta(x^{(i)})}{h_\\theta(x^{(i)})(1-h_\\theta(x^{(i)}))} h&#x27;_\\theta(x^{(i)})\\frac{\\partial X\\theta}{\\partial\\theta_j} \\\\ &amp;= - \\sum_{i=1}^m \\frac{y^{(i)} - h_\\theta(x^{(i)})}{h_\\theta(x^{(i)})(1-h_\\theta(x^{(i)}))} h_\\theta(x^{(i)})(1-h_\\theta(x^{(i)})) x_j^{(i)} \\\\ &amp;= - \\sum_{i=1}^m (y^{(i)} - h_\\theta(x^{(i)}))x_j^{(i)} \\\\ &amp;= \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\end{aligned} ∂θj​∂​J(θ)​=−∂θj​∂∑i=1m​(y(i)ln(hθ​(x(i)))+(1−y(i))ln(1−hθ​(x(i))))​=−i=1∑m​hθ​(x(i))y(i)​∂θj​∂hθ​(x(i))​−1−hθ​(x(i))1−y(i)​∂θj​∂hθ​(x(i))​=−i=1∑m​(hθ​(x(i))y(i)​−1−hθ​(x(i))1−y(i)​)∂θj​∂hθ​(x(i))​=−i=1∑m​hθ​(x(i))(1−hθ​(x(i)))y(i)−hθ​(x(i))​hθ′​(x(i))∂θj​∂Xθ​=−i=1∑m​hθ​(x(i))(1−hθ​(x(i)))y(i)−hθ​(x(i))​hθ​(x(i))(1−hθ​(x(i)))xj(i)​=−i=1∑m​(y(i)−hθ​(x(i)))xj(i)​=i=1∑m​(hθ​(x(i))−y(i))xj(i)​​ In gradient descent iteration: θj=θj−α∑i=1m(hθ(x(i))−y(i))xj(i)\\theta_j = \\theta_j - \\alpha\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} θj​=θj​−αi=1∑m​(hθ​(x(i))−y(i))xj(i)​ j = 0,1,2,...,n 4.2 Matrix Expression J(θ)=−YTln(hθ(X))−(E−Y)Tln(E−hθ(X))J(\\theta) = -Y^Tln(h_\\theta(X)) - (E-Y)^Tln(E-h_\\theta(X)) J(θ)=−YTln(hθ​(X))−(E−Y)Tln(E−hθ​(X)) ∂∂θJ(θ)=XT[1hθ(X)⊙hθ(X)⊙(E−hθ(X)⊙(−Y))]+XT[1E−hθ(X)⊙hθ(X)⊙(E−hθ(X)⊙(E−Y))]=XT(hθ(X)−Y)\\begin{aligned} \\frac{\\partial}{\\partial\\theta}J(\\theta) &amp;= X^T \\left[\\frac{1}{h_\\theta(X)} \\odot h_\\theta(X) \\odot (E-h_\\theta(X) \\odot (-Y)) \\right] + X^T \\left[\\frac{1}{E-h_\\theta(X)} \\odot h_\\theta(X) \\odot (E-h_\\theta(X) \\odot (E-Y)) \\right] \\\\ &amp;= X^T(h_\\theta(X) - Y) \\end{aligned} ∂θ∂​J(θ)​=XT[hθ​(X)1​⊙hθ​(X)⊙(E−hθ​(X)⊙(−Y))]+XT[E−hθ​(X)1​⊙hθ​(X)⊙(E−hθ​(X)⊙(E−Y))]=XT(hθ​(X)−Y)​ It used the chain rules of vector differentiation: ∂∂Xlog(X)=1/X\\frac{\\partial}{\\partial X}log(X)=1/X∂X∂​log(X)=1/X ∂∂zg(z)=g(z)(1−g(z))\\frac{\\partial}{\\partial z}g(z) = g(z)(1-g(z))∂z∂​g(z)=g(z)(1−g(z)), [g(z) is the sigmoid function] ∂Xθ∂θ=X\\frac{\\partial X\\theta}{\\partial \\theta}= X∂θ∂Xθ​=X The iteration formula of θ\\thetaθ is : θ=θ−αXT(hθ(X)−Y)\\theta = \\theta - \\alpha X^T(h_\\theta(X)-Y) θ=θ−αXT(hθ​(X)−Y) 5. Regularization of Logistic Regression In order to avoiding overfitting, we need to consider regularization. Most common methods are L1 and L2 regularization. L1L_1L1​ regularization J(θ)=−YTln(hθ(X))−(E−Y)Tln(E−hθ(X))+α∥θ∥1J(\\theta) = -Y^Tln(h_\\theta(X)) - (E-Y)^Tln(E-h_\\theta(X)) + \\alpha\\|\\theta\\|_1 J(θ)=−YTln(hθ​(X))−(E−Y)Tln(E−hθ​(X))+α∥θ∥1​ This L1 loss function is not derivable, we can find the parameters that minimize funtion based on two methods: Coordinate Descent or Least Angle Regression L2L_2L2​ regularization J(θ)=−YTln(hθ(X))−(E−Y)Tln(E−hθ(X))+12α∥θ∥22J(\\theta) = -Y^Tln(h_\\theta(X)) - (E-Y)^Tln(E-h_\\theta(X)) + \\frac{1}{2}\\alpha\\|\\theta\\|_2^2 J(θ)=−YTln(hθ​(X))−(E−Y)Tln(E−hθ​(X))+21​α∥θ∥22​ The optimization method of L2 loss function is similar to ordinary logistic regression. 6. Promotion of binary : Multiple Logistic Regression Binary Case P(y=1∣x,θ)=hθ(x)=11+e−xθ=exθ1+exθP(y=0∣x,θ)=1−hθ(x)=11+exθ\\begin{aligned} &amp;P(y=1|x,\\theta) = h_\\theta(x) = \\frac{1}{1+e^{-x\\theta}} = \\frac{e^{x\\theta}}{1+e^{x\\theta}} \\\\ &amp;P(y=0|x,\\theta) = 1-h_\\theta(x) = \\frac{1}{1+e^{x\\theta}} \\end{aligned} ​P(y=1∣x,θ)=hθ​(x)=1+e−xθ1​=1+exθexθ​P(y=0∣x,θ)=1−hθ​(x)=1+exθ1​​ y can only be 0 or 1, then: ln(P(y=1∣x,θ)P(y=0∣x,θ))=xθln\\left(\\frac{P(y=1|x,\\theta)}{P(y=0|x,\\theta)}\\right) = x\\theta ln(P(y=0∣x,θ)P(y=1∣x,θ)​)=xθ Suppose there is a K classification model, the value of the sample output y is 1,2,...,K. According to the experience of binary classification, we get: ln(P(y=1∣x,θ)P(y=K∣x,θ))=xθ1ln(P(y=2∣x,θ)P(y=K∣x,θ))=xθ2...ln(P(y=K−1∣x,θ)P(y=K∣x,θ))=xθK−1\\begin{aligned} &amp;ln\\left(\\frac{P(y=1|x,\\theta)}{P(y=K|x,\\theta)}\\right) = x\\theta_1 \\\\ &amp;ln\\left(\\frac{P(y=2|x,\\theta)}{P(y=K|x,\\theta)}\\right) = x\\theta_2 \\\\ &amp;... \\\\ &amp;ln\\left(\\frac{P(y=K-1|x,\\theta)}{P(y=K|x,\\theta)}\\right) = x\\theta_{K-1} \\\\ \\end{aligned} ​ln(P(y=K∣x,θ)P(y=1∣x,θ)​)=xθ1​ln(P(y=K∣x,θ)P(y=2∣x,θ)​)=xθ2​...ln(P(y=K∣x,θ)P(y=K−1∣x,θ)​)=xθK−1​​ There are K-1 equations above, and the sum of all probability is 1: ∑i=1KP(y=i∣x,θ)=1\\sum_{i=1}^KP(y=i|x,\\theta) = 1 i=1∑K​P(y=i∣x,θ)=1 Then there are K equations now. Solving this K linear equations, the probability distribution of K logistic regression is as follows: P(y=k∣x,θ)=exθk1+∑t=1K−1exθtk=1,2,...,K−1P(y=K∣x,θ)=11+∑t=1K−1exθt\\begin{aligned} &amp;P(y=k|x,\\theta) = \\frac{e^{x\\theta_k}}{1+\\sum_{t=1}^{K-1}e^{x\\theta_t}} &amp;k=1,2,...,K-1 \\\\ &amp;P(y=K|x,\\theta) = \\frac{1}{1+\\sum_{t=1}^{K-1}e^{x\\theta_t}} \\end{aligned} ​P(y=k∣x,θ)=1+∑t=1K−1​exθt​exθk​​P(y=K∣x,θ)=1+∑t=1K−1​exθt​1​​k=1,2,...,K−1 The loss function derivation and optimization of multiple logistic regression is similar to that of binary logistic regression. 7. Conclusion Logistic regression, especially binary logistic regression, is a very common model. The training speed is very fast. Although it is not as mainstream as the support vector machine (SVM), it is enough to solve normal classification problems. The training speed is also faster than SVM. Question: For logistic regression, why is the Square Sum of Error non-convex and not suitable as the loss function? Suppose we use SSE as logistic regression's loss function: J(θ)=12∑i=1m(y^i−yi)2J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(\\hat{y}_i - y_i)^2 J(θ)=21​i=1∑m​(y^​i​−yi​)2 y^i=11+e−xiθ\\hat{y}_i = \\frac{1}{1+e^{-x_i\\theta}} y^​i​=1+e−xi​θ1​ To determine whether J is a convex function, it depends on whether its second derivative is greater than 0. ∂∂θjJ(θ)=∑i=1m(y^i−yi)∂y^i∂θj=∑i=1m(y^i−yi)y^i(1−y^i)xj(i)=∑i=1m(−y^i3+(yi+1)y^i2−yiy^i)xj(i)\\begin{aligned} \\frac{\\partial}{\\partial\\theta_j}J(\\theta) &amp;= \\sum_{i=1}^m(\\hat{y}_i - y_i)\\frac{\\partial\\hat{y}_i}{\\partial\\theta_j} \\\\ &amp;= \\sum_{i=1}^m(\\hat{y}_i - y_i)\\hat{y}_i(1-\\hat{y}_i)x_j^{(i)} \\\\ &amp;= \\sum_{i=1}^m(-\\hat{y}_i^3 + (y_i+1)\\hat{y}_i^2 - y_i\\hat{y}_i)x_j^{(i)} \\end{aligned} ∂θj​∂​J(θ)​=i=1∑m​(y^​i​−yi​)∂θj​∂y^​i​​=i=1∑m​(y^​i​−yi​)y^​i​(1−y^​i​)xj(i)​=i=1∑m​(−y^​i3​+(yi​+1)y^​i2​−yi​y^​i​)xj(i)​​ ∂2∂θjJ(θ)=∑i=1m(−3y^i2+2(yi+1)y^i−yi)xj(i)∂y^∂θj=∑i=1m[−3y^i2+2(yi+1)y^i−yi]y^i(1−y^i)(xj(i))2\\begin{aligned} \\frac{\\partial^2}{\\partial\\theta_j}J(\\theta) &amp;= \\sum_{i=1}^m(-3\\hat{y}_i^2 + 2(y_i+1)\\hat{y}_i - y_i)x_j^{(i)}\\frac{\\partial\\hat{y}}{\\partial\\theta_j} \\\\ &amp;= \\sum_{i=1}^m\\left[-3\\hat{y}_i^2 + 2(y_i+1)\\hat{y}_i - y_i\\right]\\hat{y}_i(1-\\hat{y}_i)(x_j^{(i)})^2 \\end{aligned} ∂θj​∂2​J(θ)​=i=1∑m​(−3y^​i2​+2(yi​+1)y^​i​−yi​)xj(i)​∂θj​∂y^​​=i=1∑m​[−3y^​i2​+2(yi​+1)y^​i​−yi​]y^​i​(1−y^​i​)(xj(i)​)2​ y^∈(0,1)\\hat{y} \\in (0,1)y^​∈(0,1), hence, y^i(1−y^i)(xj(i))2\\hat{y}_i(1-\\hat{y}_i)(x_j^{(i)})^2y^​i​(1−y^​i​)(xj(i)​)2 &gt; 0. Therefore, the positive and negative property of the second derivative of J is determined by the term [−3y^i2+2(yi+1)y^i−yi][-3\\hat{y}_i^2 + 2(y_i+1)\\hat{y}_i - y_i][−3y^​i2​+2(yi​+1)y^​i​−yi​] And yi∈{0,1}y_i \\in \\{0,1\\}yi​∈{0,1} when yi=0y_i=0yi​=0, the term is [−3y^i2+2y^i][-3\\hat{y}_i^2 + 2\\hat{y}_i][−3y^​i2​+2y^​i​]. The condition of this term being greater than 0 is y^&lt;2/3\\hat{y}&lt;2/3y^​&lt;2/3 when yi=1y_i=1yi​=1, the term is [−3y^i2+4y^i−1]=−(3y^i−1)(y^i−1)[-3\\hat{y}_i^2 + 4\\hat{y}_i - 1]=-(3\\hat{y}_i-1)(\\hat{y}_i-1)[−3y^​i2​+4y^​i​−1]=−(3y^​i​−1)(y^​i​−1). The condition of this term being greater than 0 is y^&lt;1/3\\hat{y}&lt;1/3y^​&lt;1/3 As we can see, only when y^∈(0,13)\\hat{y} \\in (0,\\frac{1}{3})y^​∈(0,31​), we are sure the second derivative is greater than 0. The second derivative of J is not strictly greater than 0, so J is not a convex function. And J (SSE) is not suitable as Loss Function. ","link":"https://zl-wu.github.io/post/mla-logistic-regression/"},{"title":"MLA -- Linear Regression Principle","content":"&quot;Linear Regression is the beginning and the most basic algorithm.&quot; Linear Regression is the most basic question in Machine Learning. Here is a simple summary of Linear Regression Algorithm principle. 1. Linear Regression Question If we have m sample data, each sample has n features and one result value: (x1(1),x2(1),...,xn(1),y1),(x1(2),x2(2),...,xn(2),y2),...,(x1(m),x2(m),...,xn(m),ym)(x^{(1)}_1,x^{(1)}_2,...,x^{(1)}_n, y_1), (x^{(2)}_1,x^{(2)}_2,...,x^{(2)}_n, y_2), ..., (x^{(m)}_1,x^{(m)}_2,...,x^{(m)}_n, y_m)(x1(1)​,x2(1)​,...,xn(1)​,y1​),(x1(2)​,x2(2)​,...,xn(2)​,y2​),...,(x1(m)​,x2(m)​,...,xn(m)​,ym​) Now question is if we get a new data with only n features: $ (x{(a)}_1,x{(a)}_2,...,x^{(a)}_n) $, how can we predict its result value $ y_a $? And what is it? If $ y_a $ is the continuous value, it is a regression question. And if $ y_a $ is the discrete value, it is a classification question. 2. Linear Regression Model If we decide to solve this question with linear regression, then we assume the data model is in this form: $ h_\\theta(x_0,x_1,..,x_n) = \\theta_0 + \\theta_1x_1 + ... + \\theta_nx_n $ θi\\theta_iθi​ (i=0,1,...,n) is the coefficient of each feature, which is also the parameter of the model we need to estimate. If we assume x0=1x_0=1x0​=1, then model can be wrote in a simpler way: $ h_\\theta(x_0,x_1,..,x_n) = \\sum_{i=0}^n \\theta_ix_i $ If we use matrix representation, Model will become more simple and elegant: $ h_\\theta(X) = X\\theta $ X is a m×nm \\times nm×n matrix (m records and n features). θ\\thetaθ is a n×1n \\times 1n×1 vector (n coefficients of n features). hθ(X)h_\\theta(X)hθ​(X) is a m×1m\\times 1m×1 vector (m prediction y value of m sample records) Once we have determined the model prototype, we need to calculate the Loss Function. Generally, we use Mean Square Error as the loss function for linear regression model. The algebratic representation of the loss function is as follows: J(θ0,θ1,...,θn)=∑i=1m(hθ(x0(i),x1(i),..,xn(i))−yi)2J(\\theta_0,\\theta_1,...,\\theta_n) = \\sum_{i=1}^m (h_\\theta(x_0^{(i)},x_1^{(i)},..,x_n^{(i)}) - y_i)^2 J(θ0​,θ1​,...,θn​)=i=1∑m​(hθ​(x0(i)​,x1(i)​,..,xn(i)​)−yi​)2 In Matix Representation: J(θ)=12(Xθ−Y)T(Xθ−Y)J(\\theta) = \\frac{1}{2} (X\\theta - Y)^T(X\\theta - Y) J(θ)=21​(Xθ−Y)T(Xθ−Y) Y is a m×1m\\times 1m×1 vector (m actual y value of m sample records) 3. Linear Regression Algorithm Once the loss function is known, our goal is to find out the parameter θ\\thetaθ that could minimize the value of loss function. There are two methods: Gradient Descent method Least Square method If we choose Gradient Descent method, the iteration formula of θ\\thetaθ is: θ=θ−αXT(Xθ−Y)\\theta = \\theta - \\alpha X^T(X\\theta-Y) θ=θ−αXT(Xθ−Y) If we choose Least Square method, the formula of θ\\thetaθ is: θ=(XTX)−1XTY\\theta = (X^TX)^{-1}X^TY θ=(XTX)−1XTY 4.1. Generalization of linear regression: Polynomial Regression If the model prototype exists not only to the first power, but also to the second power, n-th power, the model becomes Polynomial Regression. For example, if the sample data has 2 features: (x1(1),x2(1),y1),(x1(2),x2(2),y2),...,(x1(m),x2(m),ym)(x_1^{(1)}, x_2^{(1)}, y_1), (x_1^{(2)}, x_2^{(2)}, y_2),...,(x_1^{(m)}, x_2^{(m)}, y_m) (x1(1)​,x2(1)​,y1​),(x1(2)​,x2(2)​,y2​),...,(x1(m)​,x2(m)​,ym​) Assume the model is in this form: hθ(x1,x2)=θ0+θ1x1+θ2x2+θ3x12+θ4x22+θ5x1x2h_\\theta(x_1,x_2) = \\theta_0+\\theta_1x_1+\\theta_2x_2 + \\theta_3x_1^2 + \\theta_4x_2^2 + \\theta_5x_1x_2 hθ​(x1​,x2​)=θ0​+θ1​x1​+θ2​x2​+θ3​x12​+θ4​x22​+θ5​x1​x2​ If we define: x0=1,x1=x1,x2=x2,x3=x12,x4=x22,x5=x1x2x_0=1, x_1=x_1, x_2=x_2, x_3=x_1^2, x_4=x_2^2, x_5=x_1x_2x0​=1,x1​=x1​,x2​=x2​,x3​=x12​,x4​=x22​,x5​=x1​x2​, then the Polynomial model is back to linear regression: hθ(x1,x2)=θ0+θ1x1+θ2x2+θ3x3+θ4x4+θ5x5h_\\theta(x_1,x_2) = \\theta_0+\\theta_1x_1+\\theta_2x_2 + \\theta_3x_3 + \\theta_4x_4 + \\theta_5x_5 hθ​(x1​,x2​)=θ0​+θ1​x1​+θ2​x2​+θ3​x3​+θ4​x4​+θ5​x5​ Therefore, the solution is to build a 5 features sample data (x1,x2,x12,x22,x1x2)(x_1, x_2, x_1^2, x_2^2, x_1x_2)(x1​,x2​,x12​,x22​,x1​x2​) for the 2 features sample data (x1,x2)(x_1,x_2)(x1​,x2​). Then we use this 5 features data to train the linear regression model. 4.2. Generalization of linear regression: Generalized Linear Regression In 4.1 section, we generized the feature side of the sample data. Now we try to generize the y value of the sample data. For example, if the Y does not have a linear relationship X, but ln(Y) does: ln(Y)=Xθln(Y) = X\\theta ln(Y)=Xθ In this case, use ln(y) instead of y, we can still deal with the problem with linear regression model. We generalize In(y), assuming this function is a monotonically differentiable function 𝐠 (.), Then the generalized generalized linear regression form is: g(Y)=Xθg(Y) = X\\theta g(Y)=Xθ 5. Regularization of linear regression In order to preventing overfitting of the model, we often add regularization terms when building the linear model. There are generally L1 regularization and L2 regularization. 5.1 L1 regularization L1 regularization of linear regression is usually called Lasso Regression. The difference between it and general linear regression is that an L1 regularization term is added to the loss function. The L1 regularized term has a constant coefficient α\\alphaα to adjust the weight of mean square error term and the regularized term of the loss function: J(θ)=12(Xθ−Y)T(Xθ−Y)+α∣∣θ∣∣1J(\\theta) = \\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y) + \\alpha||\\theta||_1 J(θ)=21​(Xθ−Y)T(Xθ−Y)+α∣∣θ∣∣1​ α\\alphaα is a constant coefficient and needs to be tuned. ∣∣θ∣∣1||\\theta||_1∣∣θ∣∣1​ is the L1 norm. Lasso regression can make the coefficients of some features smaller, or even some coefficients with smaller absolute values directly become 0. Enhance the generalization ability of the model. The solution methods of Lasso regression are generally: Coordinate Descent Least Angle Regression Please check Regularization of Regression-Summary of Lasso Regression 5.2 L2 regularization L2 regularization of linear regression is usually called Ridge Regression. It adds an L2 regularization term to the loss function: J(θ)=12(Xθ−Y)T(Xθ−Y)+12α∣∣θ∣∣22J(\\theta) = \\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y) + \\frac{1}{2}\\alpha||\\theta||_2^2 J(θ)=21​(Xθ−Y)T(Xθ−Y)+21​α∣∣θ∣∣22​ α\\alphaα is a constant coefficient and needs to be tuned. ∣∣θ∣∣2||\\theta||_2∣∣θ∣∣2​ is the L2 norm. (not equal to L1 norm) Ridge regression reduces the regression coefficient without abandoning any feature, making the model relatively stable, but compared with Lasso regression, this will leave the model with a lot of features and poor interpretability. The solution of Ridge regression is relatively simple, and the least square method is generally used. Here is a matrix derivation form using least squares, which is similar to ordinary linear regression. Let the derivative of J(θ)J(\\theta)J(θ) be 0 and get the following formula: XT(Xθ−Y)+αθ=0X^T(X\\theta - Y) + \\alpha\\theta = 0 XT(Xθ−Y)+αθ=0 Then: θ=(XTX+αE)−1XTY\\theta = (X^TX + \\alpha E)^{-1} X^TY θ=(XTX+αE)−1XTY E is the Identity matrix ","link":"https://zl-wu.github.io/post/mla-linear-regression-principle/"},{"title":"MLA -- Least Square method","content":"&quot;A standard approach in regression analysis to approximate the solution of overdetermined systems.&quot; The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals made in the results of every single equation. It is used to do model fitting and find the extreme value of the loss function. 1. Least Square principle The general form of least squares is simple: f=∑(y−y^)2f = \\sum (y - \\widehat{y})^2 f=∑(y−y​)2 f is the objective function y is the observation value (actual value) y^\\widehat{y}y​ is the theoretical value predicted from the model The objective function is also the loss function commonly used in the machine learning. Our goal is to get a fitting model when the objective function is minimized. Give a simple example of the simplest linear regression, we have m samples with only one feature: (x1,y1)(x2,y2),...,(xm,ym)(x_1,y_1)(x_2,y_2),...,(x_m,y_m)(x1​,y1​)(x2​,y2​),...,(xm​,ym​) Hypothesis model is: hθ(x)=θ0+θ1xh_\\theta(x) = \\theta_0 + \\theta_1x hθ​(x)=θ0​+θ1​x Loss function is: J(θ0,θ1)=∑i=1m(yi−hθ(xi))2=∑i=1m(yi−θ0−θ1xi)2\\begin{aligned} J(\\theta_0,\\theta_1) &amp;= \\sum_{i=1}^{m}(y_i - h_\\theta(x_i))^2 \\\\ &amp;= \\sum_{i=1}^{m}(y_i - \\theta_0 - \\theta_1x_i)^2 \\end{aligned} J(θ0​,θ1​)​=i=1∑m​(yi​−hθ​(xi​))2=i=1∑m​(yi​−θ0​−θ1​xi​)2​ Least Square method needs to get (θ0,θ1)(\\theta_0,\\theta_1)(θ0​,θ1​) that minimizes the value of J(θ0,θ1)J(\\theta_0,\\theta_1)J(θ0​,θ1​) 2. Least Squre solution 2.1 Algebraic way If we need to minimumize J(θ0,θ1)J(\\theta_0,\\theta_1)J(θ0​,θ1​), our method is to calculate the partial derivatives of θ0\\theta_0θ0​ and θ1\\theta_1θ1​ respectively, and make their partial derivatives all 0 to form a linear equation set for two variables. Partial derivation of J(θ0,θ1)J(\\theta_0,\\theta_1)J(θ0​,θ1​) to θ0\\theta_0θ0​: −2∑i=1m(yi−θ0−θ1xi)=0-2\\sum_{i=1}^{m}(y_i-\\theta_0-\\theta_1x_i) = 0 −2i=1∑m​(yi​−θ0​−θ1​xi​)=0 Partial derivation of J(θ0,θ1)J(\\theta_0,\\theta_1)J(θ0​,θ1​) to θ1\\theta_1θ1​: −2∑i=1m(yi−θ0−θ1xi)xi=0-2\\sum_{i=1}^{m}(y_i-\\theta_0-\\theta_1x_i)x_i = 0 −2i=1∑m​(yi​−θ0​−θ1​xi​)xi​=0 Now: {∑i=1m(yi−θ0−θ1xi)=0∑i=1m(yi−θ0−θ1xi)xi=0\\begin{cases} \\sum_{i=1}^{m}(y_i-\\theta_0-\\theta_1x_i) = 0\\\\ \\sum_{i=1}^{m}(y_i-\\theta_0-\\theta_1x_i)x_i = 0\\\\ \\end{cases} {∑i=1m​(yi​−θ0​−θ1​xi​)=0∑i=1m​(yi​−θ0​−θ1​xi​)xi​=0​ ={∑i=1myi−mθ0−θ1∑i=1mxi=0∑i=1myixi−θ0∑i=1mxi−θ1∑i=1mxi2=0=\\begin{cases} \\sum_{i=1}^{m}y_i - m\\theta_0-\\theta_1\\sum_{i=1}^{m}x_i = 0\\\\ \\sum_{i=1}^{m}y_ix_i -\\theta_0\\sum_{i=1}^{m}x_i-\\theta_1\\sum_{i=1}^{m}x_i^2 = 0\\\\ \\end{cases} ={∑i=1m​yi​−mθ0​−θ1​∑i=1m​xi​=0∑i=1m​yi​xi​−θ0​∑i=1m​xi​−θ1​∑i=1m​xi2​=0​ Then: θ0=∑xi2×∑yi−∑xi×∑xiyim∑xi2−(∑xi)2 \\theta_0 = \\frac {\\sum x_i^2 \\times \\sum y_i - \\sum x_i \\times \\sum x_iy_i} {m\\sum x_i^2-(\\sum x_i)^2} θ0​=m∑xi2​−(∑xi​)2∑xi2​×∑yi​−∑xi​×∑xi​yi​​ θ1=m∑xiyi−∑xi×∑yim∑xi2−(∑xi)2 \\theta_1 = \\frac { m\\sum x_iy_i - \\sum x_i \\times \\sum y_i} {m\\sum x_i^2-(\\sum x_i)^2} θ1​=m∑xi2​−(∑xi​)2m∑xi​yi​−∑xi​×∑yi​​ If the sample has more than 1 features, which is Multiple Linear Regression. We still use the rule that partial derivatives equal to 0 to form parametric equation set, then calculate these unknown parameters. The principle remains the same. 2.2 Matrix way The matrix expression is more concise than the algebra expression, and it can also replace loops (In essence, the nature of computing has not changed). So many books and machine learning libraries now use the matrix method to do the least square. Here we use the above multiple linear regression example to describe the matrix method solution. hθ(x1,x2,...,xn)=θ0+θ1x1+...+θnxnh_\\theta(x_1,x_2,...,x_n)=\\theta_0+\\theta_1x_1+...+\\theta_nx_nhθ​(x1​,x2​,...,xn​)=θ0​+θ1​x1​+...+θn​xn​. If in matrix expression: hθ(X)=Xθh_\\theta(X) = X\\theta hθ​(X)=Xθ X is a m×nm \\times nm×n matrix (m records and n features). θ\\thetaθ is a n×1n \\times 1n×1 vector (n coefficients of n features). hθ(X)h_\\theta(X)hθ​(X) is a m×1m\\times 1m×1 vector (m prediction y value of m sample records) The loss function is: J(θ)=12(Xθ−Y)T(Xθ−Y)J(\\theta) = \\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y) J(θ)=21​(Xθ−Y)T(Xθ−Y) Y is a m×1m\\times 1m×1 vector (m actual y value of m sample records) 1/2 is mainly to facilitate the calculation of later derivatives. Let the derivative to be 0: ∂∂θJ(θ)=XT(Xθ−Y)=0\\frac{\\partial}{\\partial\\theta}J(\\theta) = X^T(X\\theta-Y) = 0 ∂θ∂​J(θ)=XT(Xθ−Y)=0 It based on two Matrix derivative chain rule formula: Formula 1: ∂∂X(XTX)=2X\\frac{\\partial}{\\partial X}(X^TX) = 2X∂X∂​(XTX)=2X Formula 2: $ \\nabla_xf(AX+B) = A^T $ Then: XTXθ=XTYθ=(XTX)−1XTY\\begin{array}{cc} X^TX\\theta = X^TY\\\\\\\\ \\theta = (X^TX)^{-1}X^TY \\end{array} XTXθ=XTYθ=(XTX)−1XTY​ (XTX)−1XTY(X^TX)^{-1}X^TY(XTX)−1XTY is our answer. 3. Limitations and applicable scenarios of least square method Least Square answer is simple and efficient, it has no iterations camparing with gradient descent. But it still has some limitations when we use it: Least Square needs to calculate the inverse matrix of XTXX^TXXTX, however, it is possible that inverse matrix does not exist. In this case, there is no way to use Least Square directly, but Gradient Descent. Or, we can also try to remove redundant features by clean the sample data. Let the determinant of XTXX^TXXTX not be 0, and then continue to use least squares. When the features n of sample is very large, the calculating the inverse matrix of XTXX^TXXTX is a very time-consuming work, even not feasible. In this case, it's better to use Gradient Descent. (If n &gt; 1000, suggest not use Least Square). Or, we can also use Principal Component Analysis (PCA) to reduce the dimension of features and then use the least square method. If the fitting function (objective model) is not linear, Least Square cannot be used. But Gradient Descent can still be used. Or, we can convert the fitting function to linearity through some techniques before Least Square can be used. There are some special situation: When the sample size m is small and less than the feature number n, then the fitting equation is underdetermined, and the commonly used optimization methods cannot fit the data. When the sample size m is equal to the feature number n, it can be solved by the equation system. When the sample size m is greater than n, the fitting equation is overdetermined, which is the scenario where we usually use least squares. ","link":"https://zl-wu.github.io/post/mla-least-square-method/"},{"title":"MLA -- Gradient Descent method","content":"When improving the model parameters of machine learning algorithms, that is, unconstrained optimalization problem. Gradient Descent method is one of most common used methods. The other one is the least square method. Here is a complete summary of the gradient descent method. 1. What is gradient? In calculus, Gradient is to find the partial derivatives of the parameters of the multivariate function, and write all partial derivative of the obtained parameters in the form of one vector. For the function f(x, y) as an example, finding the partial derivatives of x and y, then the gradient vector is $ (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})^T $, AKA grad f(x, y) or $ \\nabla f(x,y)$ The specific gradient vector value at point (x0, y0) is $ (\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial y_0})^T $ or $ \\nabla f(x_0,y_0)$ If there are three parameters, then gradient vector is $ (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z})^T $, and so on. In geometrically speaking, the meaning of gradient is the direction where the function f(x,y) changes fastest. At point (x0, y0), the direction of gradient vector $(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial y_0})^T $ is where f(x,y) changes fastest. If we keep following the gradient vector direction, we will reach the maximum or minimum point of function faster and easier. 2. Gradient Descent Assume we are somewhere on a large mountain, because we don’t know how to go down the mountain, so we decided to take one step at a time. That is, we can take a step down from the current position's steepest direction, and then continue to calculate the gradient of the new position and take a step toward the position where the step is located at the steepest and easiest to descend. Going on like this step by step until we feel that we have reached the foot of the mountain. Of course, if we go on like this, we may not be able to reach the foot of the mountain, but to a certain foot in a local part of the mountain. As can be seen from the above explanation, gradient descent may not necessarily find the global optimal solution, but may be a local optimal solution. Of course, if the loss function is convex, the solution obtained by the gradient descent method must be the global optimal solution. 3. Concepts of gradient descent Learining Rate: The step size determines the length of each step in the negative direction of the gradient during the gradient descent iteration. Feature: Refers to the input part of the sample data. (x0(1),x1(1),...,xn(1),y1)(x_0^{(1)},x_1^{(1)},...,x_n^{(1)},y_1)(x0(1)​,x1(1)​,...,xn(1)​,y1​) Hypothesis Function: In supervised learning, the hypothesis function used to fit the input samples, denoted as hθ(X)h_\\theta(X)hθ​(X). For example, the linear regression hypothesis model is hθ(x1,x2,...,xn)=θ0+θ1x1+θ2x2+...+θnxnh_\\theta(x_1,x_2,...,x_n) = \\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_nhθ​(x1​,x2​,...,xn​)=θ0​+θ1​x1​+θ2​x2​+...+θn​xn​ Loss Function: In order to evaluate how well the model fits the data, a loss function is usually used to measure the degree of fit. The minimization of the loss function means that the degree of fitting is the best and the corresponding model parameters are the optimal parameters. In linear regression model, the loss function usually squares the difference between the sample output (yi) and the hypothesis function output (y hat). If there is a linear regression model for m samples: (x1i,x2i,...,xni,yi),i=(1,2,...,m)(x_1^{i},x_2^{i},...,x_n^{i},y_i),i=(1,2,...,m)(x1i​,x2i​,...,xni​,yi​),i=(1,2,...,m), the loss function is: J(θ0,θ1,...,θn)=∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)2J(\\theta_0,\\theta_1,...,\\theta_n) = \\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)^2 J(θ0​,θ1​,...,θn​)=i=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)2 x(i)x^{(i)}x(i) is the feature of the i-th sample data. yiy_iyi​ is the output of the i-th sample data. hθ(x1(i),x2(i),...,xn(i))h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})hθ​(x1(i)​,x2(i)​,...,xn(i)​) is the Hypothesis Function. (== y^\\widehat{y}y​) 4. Gradient Descent Algorithm The algorithm of gradient descent method can have two expressions: algebraic method and matrix method (also called vector method). 4.1 Algebraic Expression 1. Prerequite: Set Hypothesis Function and Loss Function of the optimization model. Assume Hypothesis Function is the linear regression model. Then the loss function is: J(θ0,θ1,...,θn)=12m∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)2J(\\theta_0,\\theta_1,...,\\theta_n) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)^2 J(θ0​,θ1​,...,θn​)=2m1​i=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)2 1/2m is a new factor we added. 1/m means average. 1/2 is mainly to facilitate the calculation of later partial derivatives. We know that m is a constant greater than zero, so it has no effect on the nature of the function gradient. It does not change the θ\\thetaθ when we get the minimum value of J(θ)J(\\theta)J(θ). After all, what we need is only the θ\\thetaθ, not the minimum value of J(θ)J(\\theta)J(θ). 2. Initialize some algorithm parameters: Initialize all (θ0,θ1,...,θn)(\\theta_0,\\theta_1,...,\\theta_n)(θ0​,θ1​,...,θn​). Learning Rate α\\alphaα. (Step Size) Algorithm termination distance ε\\varepsilonε. If the distance of gradient descent is less than ε\\varepsilonε, algorithm stoppes. For example, in the absence of any prior knowledge, we can simply intialize all θ\\thetaθ to 0 and α\\alphaα to 1. Then Optimize them again when tuning is needed. 3. Algorithm Process: Step 1: Calculate the gradient of current position. For θj\\theta_jθj​, its gradient expression is:∂∂θjJ(θ0,θ1,...,θn)\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n) ∂θj​∂​J(θ0​,θ1​,...,θn​) j is (0,1,2,...,n), represents the j-th parameter. Step 2: Multiply Gradient by Learning Rate α\\alphaα (step size) to get the distance of gradient descent at current position. That is, α∂∂θjJ(θ0,θ1,...,θn)\\alpha\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n)α∂θj​∂​J(θ0​,θ1​,...,θn​) Step 3: If all θj\\theta_jθj​'s distance of gradient descent is less than ε\\varepsilonε, algorithm stopped. And current (θ0,θ1,...,θn)(\\theta_0,\\theta_1,...,\\theta_n)(θ0​,θ1​,...,θn​) is returned as the final result. Step 4: Update all θj\\theta_jθj​ and jump to Step 1 for a new loop. The update expression is as follows: θj=θj−α∂∂θjJ(θ0,θ1,...,θn)\\theta_j = \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n) θj​=θj​−α∂θj​∂​J(θ0​,θ1​,...,θn​) The algorithm is very simple and elegant, the key point is how to calculate ∂∂θjJ(θ0,θ1,...,θn)\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n)∂θj​∂​J(θ0​,θ1​,...,θn​): J(θ0,θ1,...,θn)=12m∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)2J(\\theta_0,\\theta_1,...,\\theta_n) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)^2 J(θ0​,θ1​,...,θn​)=2m1​i=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)2 ∂∂θjJ(θ0,θ1,...,θn)=1m∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\frac{\\partial}{\\partial\\theta_j} J(\\theta_0,\\theta_1,...,\\theta_n) = \\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} ∂θj​∂​J(θ0​,θ1​,...,θn​)=m1​i=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ θj=θj−α∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\theta_j = \\theta_j - \\alpha\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} θj​=θj​−αi=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ 4.2 Matrix Expression Algebraic Expression and Matrix Expression are essentially the same, but the representation of Matrix will be more concise. Suppose we have m sample data: (x1(1),x2(1),...,xn(1),y1),(x1(2),x2(2),...,xn(2),y2),...,(x1(m),x2(m),...,xn(m),ym)(x_1^{(1)},x_2^{(1)},...,x_n^{(1)},y_1),(x_1^{(2)},x_2^{(2)},...,x_n^{(2)},y_2),...,(x_1^{(m)},x_2^{(m)},...,x_n^{(m)},y_m)(x1(1)​,x2(1)​,...,xn(1)​,y1​),(x1(2)​,x2(2)​,...,xn(2)​,y2​),...,(x1(m)​,x2(m)​,...,xn(m)​,ym​) 1. Prerequite: Set Hypothesis Function and Loss Function of the optimization model. Assume Hypothesis Function is the linear regression model: hθ(X)=Xθh_\\theta(X) = X\\theta hθ​(X)=Xθ J(θ)=12(Xθ−Y)T(Xθ−Y)J(\\theta) = \\frac{1}{2}(X\\theta-Y)^T(X\\theta-Y) J(θ)=21​(Xθ−Y)T(Xθ−Y) X is a m×nm \\times nm×n matrix (m records and n features). θ\\thetaθ is a n×1n \\times 1n×1 vector (n coefficients of n features). hθ(X)h_\\theta(X)hθ​(X) is a m×1m\\times 1m×1 vector (m prediction y value of m sample records) Y is a m×1m\\times 1m×1 vector (m actual y value of m sample records) 2. Initialize some algorithm parameters: Same as 4.1 section. 3. Algorithm Process: Same as 4.1 section. ∂∂θJ(θ)=XT(Xθ−Y)\\frac{\\partial}{\\partial\\theta}J(\\theta) = X^T(X\\theta-Y) ∂θ∂​J(θ)=XT(Xθ−Y) θ=θ−α∂∂θJ(θ)=θ−αXT(Xθ−Y)\\begin{aligned} \\theta &amp;= \\theta - \\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta)\\\\ &amp;= \\theta - \\alpha X^T(X\\theta-Y) \\end{aligned} θ​=θ−α∂θ∂​J(θ)=θ−αXT(Xθ−Y)​ Matrix derivative chain rule: Formula 1: ∂∂X(XTX)=2X\\frac{\\partial}{\\partial X}(X^TX) = 2X∂X∂​(XTX)=2X Formula 2: $ \\nabla_xf(AX+B) = A^T $ 5. Tuning of gradient descent algorithm Learning Rate: If the step size is too large, the iteration will be too fast, and it may even miss the optimal solution. The step size is too small, the iteration speed is too slow, and the algorithm cannot end for a long time. So the step size of the algorithm needs to be run multiple times to get a better value. θ\\thetaθ initialization: different intial parameter value may lead to different minimum value. Hence, the gradient descent only finds the local minimum. if the loss function is a convex function, it must be the global optimal solution. It is necessary to run the algorithm with different initial values multiple times, which can increase the possibility that we skip the local minimum to reach the global minimum. Normalized: The data range of different features is different, if some features' data value are very high, and some others' are very low, it may lead to a very slow iteration. In order to reduce the influence of the feature value, the feature data can be normalized. For each xj feature's data, we transform it to:xj=xj−xj‾std(xj)x_j = \\frac{x_j - \\overline{x_j}}{std(x_j)} xj​=std(xj​)xj​−xj​​​ xjx_jxj​ becomes a new feature with new expectation 0 and new variance 1, the iteration speed can be greatly accelerated. 6. Gradient Descent Family (BGD, SGD, MBGD) 6.1 BGD (Batch Gradient Descent) Batch Gradient Descent is the most commonly used form. It always uses all m sample data when we update parameter θ\\thetaθ once. As discuss above: θj=θj−α∑i=1m(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\theta_j = \\theta_j - \\alpha\\sum_{i=1}^m(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} θj​=θj​−αi=1∑m​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ For each iteration of one of θ\\thetaθ, we use all m samples to caculate gradient. 6.2 SGD (Stochastic Gradient Descent) Stochastic Gradient Descent is similar to the BGD, the difference is that SGD use only one random sample to calculate gradient: θj=θj−α(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\theta_j = \\theta_j - \\alpha(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} θj​=θj​−α(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ BGD and SGD are two extremes. One uses all data for gradient descent, and one uses only one sample data for gradient descent. Advantages and disadvantages are very prominent. Traning Speed: SGD is very fast, since it uses only one sample to iterate each time. While the BGD cannot satisfy the training speed when the sample size is large. Accuracy: SGD uses only one sample, most likely resulting in a not optimal solution. convergence Speed: SGD uses one sample at a interation, the iteration direction changes greatly, and it cannot quickly converge to the local optimal solution. Is there a moderate method to neutralize the advantages and disadvantages of BGD and SGD? Yes, MBGD 6.3 MBGD (Mini-batch Gradient Descent) The Mini-batch Gradient Descent is a compromise between the BGD and the SGD, that is, for all m samples, we use k samples to iterate, 0&lt;k&lt;m0&lt;k&lt;m0&lt;k&lt;m. Of course, the value of k can be adjusted according to the sample data. The iteration formula is: θj=θj−α∑i=tt+k−1(hθ(x1(i),x2(i),...,xn(i))−yi)xj(i)\\theta_j = \\theta_j - \\alpha\\sum_{i=t}^{t+k-1}(h_\\theta(x_1^{(i)},x_2^{(i)},...,x_n^{(i)})-y_i)x_j^{(i)} θj​=θj​−αi=t∑t+k−1​(hθ​(x1(i)​,x2(i)​,...,xn(i)​)−yi​)xj(i)​ 7. Comparison of gradient descent method and other optimization algorithms In machine learning, there are generally these optimization algorithms: Gradient Descent Least Square Newton's method and Quasi-Newton's method Gradient Descent and Lease Square: Gradient Descent needs step size, Lease Square doesn't. Gradient Descent is an iterative solution, and the Least Square is an analytical solution. If the sample size is not very large, and there is an analytical solution, the Least Square has an advantage over the Gradient Descent, and the LS calculation speed is fast. However, if the sample size is large, Least Square requires a super large inverse matrix, which is difficult or slow to solve the analytical solution. Then iterative Gradient Descent is better. Gradient Descent and Newton's method/Quasi-Newton's method: Both of them are iterative solution. Gradient Descent is gradient solution. Newton's method is solved by using the inverse or pseudo-inverse matrix of the second-order Hessian matrix. Newton method / Quasi-Newton's method converges faster. But each iteration takes longer than gradient descent Python implementation Example f(x)=x2+1f(x)=x^2+1f(x)=x2+1 &quot;&quot;&quot; One Dimensional Gradient Descent Example -- One feature &quot;&quot;&quot; def func_1d(x): &quot;&quot;&quot; Objective Function :param x: independent variable, scalar :return: dependent variable, scalar &quot;&quot;&quot; return x ** 2 + 1 def grad_1d(x): &quot;&quot;&quot; Gradient of Objective Function :param x: independent variable, scalar :return: dependent variable, scalar &quot;&quot;&quot; return x * 2 def gradient_descent_1d(grad, cur_x=0.1, learning_rate=0.01, precision=0.0001, max_iters=10000): &quot;&quot;&quot; One-dimensional gradient descent algorithm :param grad: Gradient Fuction of Objective Function :param cur_x: Current x value :param learning_rate: step size :param precision: Set convergence accuracy (stop condition) :param max_iters: max number of iterations :return: local minumum x* &quot;&quot;&quot; for i in range(max_iters): grad_cur = grad(cur_x) if abs(grad_cur) &lt; precision: # when the gradient approaches 0, it is regarded as convergence break cur_x = cur_x - grad_cur * learning_rate print(&quot;The&quot;, i, &quot;-th iteration's x value: &quot;, cur_x) print(&quot;Local Minumum x =&quot;, cur_x) return cur_x if __name__ == '__main__': gradient_descent_1d(grad_1d, cur_x=10, learning_rate=0.2, precision=0.000001, max_iters=10000) f(x,y)=−e−(x2+y2)f(x,y) = -e^{-(x^2+y^2)}f(x,y)=−e−(x2+y2) &quot;&quot;&quot; 2-Dimensional Gradient Descent example -- 2 features &quot;&quot;&quot; import math import numpy as np def func_2d(x): &quot;&quot;&quot; Objective Function :param x: independent variable, 2d vector :return: dependent variable, scalar &quot;&quot;&quot; return - math.exp(-(x[0] ** 2 + x[1] ** 2)) def grad_2d(x): &quot;&quot;&quot; Gradient of the Objective Function :param x: independent variable, 2d vector :return: dependent variable, 2d vector &quot;&quot;&quot; deriv0 = 2 * x[0] * math.exp(-(x[0] ** 2 + x[1] ** 2)) deriv1 = 2 * x[1] * math.exp(-(x[0] ** 2 + x[1] ** 2)) return np.array([deriv0, deriv1]) def gradient_descent_2d(grad, cur_x=np.array([0.1, 0.1]), learning_rate=0.01, precision=0.0001, max_iters=10000): &quot;&quot;&quot; 2D gradient descent algorithm :param grad: Gradient Fuction of Objective Function :param cur_x: Current x value (Parameter Initialization) :param learning_rate: step size :param precision: Set convergence accuracy (stop condition) :param max_iters: max number of iterations :return: Local minimum x* &quot;&quot;&quot; print(f&quot;{cur_x} as the Initialization...&quot;) for i in range(max_iters): grad_cur = grad(cur_x) if np.linalg.norm(grad_cur, ord=2) &lt; precision: # when the gradient approaches 0, it is regarded as convergence break cur_x = cur_x - grad_cur * learning_rate print(&quot;The&quot;, i, &quot;-th iteration's x value: &quot;, cur_x) print(&quot;Local minimum x =&quot;, cur_x) return cur_x if __name__ == '__main__': gradient_descent_2d(grad_2d, cur_x=np.array([1, -1]), learning_rate=0.2, precision=0.000001, max_iters=10000) f(x1,x2,x3)=3x1+4x2+5x3+6f(x_1,x_2,x_3) = 3x_1+4x_2+5x_3+6f(x1​,x2​,x3​)=3x1​+4x2​+5x3​+6 &quot;&quot;&quot; Parameter Estimation Multiple linear regression gradient descent example (Matrix Method) Dataset(4 points on the function): x1, x2, x3, b, y 1, 1, 2, 1, 23 2, 4, 3, 1, 43 3, 2, 4, 1, 43 4, 3, 1, 1, 35 &quot;&quot;&quot; import numpy as np def gradient_descent(theta, X, Y, lr=0.01, precision=0.0001, max_iters=10000): &quot;&quot;&quot; matrix gradient descent algorithm :param theta: parameter estimation of the objective function :param X: sample data, independent data, mxn matrix :param Y: sample data, dependent data, mx1 matrix :param lr: step size :param precision: Set convergence accuracy (stop condition) :param max_iters: max number of iterations :return: Local minimum theta* &quot;&quot;&quot; print(f&quot;{theta} as the Initialization...&quot;) for i in range(max_iters): distance = lr*X.T*(X*theta-Y) if np.linalg.norm(distance, ord=2) &lt; precision: # when the gradient approaches 0, it is regarded as convergence print(&quot;total iterations:&quot;, i) break theta = theta-distance print(&quot;Local minimum theta =\\n&quot;, theta) return theta x = np.mat([[1,1,2,1],[2,4,3,1],[3,2,4,1],[4,3,1,1]]) y = np.mat([[23],[43],[43],[35]]) theta = np.mat([[0],[0],[0],[0]]) gradient_descent(theta, x, y) ##### [[0] [0] [0] [0]] as the Initialization... total iterations: 1852 Local minimum theta = [[3.00521152] [4.00294095] [5.00623515] [5.96159285]] &quot;&quot;&quot; Additional code -- np.linalg.norm linalg = linear algebra &quot;&quot;&quot; x_norm=np.linalg.norm(x, ord=None, axis=None, keepdims=False) import numpy as np x = np.array([ [0, 3, 4], [1, 6, 4]]) # np.sqrt(0+9+16+1+36+16) print(&quot;Default(sqrt of square sum of all elements, does not return matrix), keepdims=False:\\n&quot;,np.linalg.norm(x)) print(&quot;Sqrt of square sum of all elements, return matrix, keepdims=True:\\n&quot;,np.linalg.norm(x,keepdims=True)) print(&quot;***********************&quot;) # np.sqrt(0+9+16), np.sqrt(1+36+16) print(&quot;Matrix's row vector, l2 norm, axis=1:\\n&quot;,np.linalg.norm(x,axis=1,keepdims=True)) # np.sqrt(0+1), np.sqrt(9+36), np.sqrt(16+16) print(&quot;Matrix's column vector, l2 norm, axis=0:\\n&quot;,np.linalg.norm(x,axis=0,keepdims=True)) print(&quot;***********************&quot;) # max(0+1, 3+6, 4+4) print(&quot;Matrix's l1 norm, ord=1:&quot;,np.linalg.norm(x,ord=1,keepdims=True)) print(&quot;Matrix's l2 norm, ord=2:&quot;,np.linalg.norm(x,ord=2,keepdims=True)) # max(0+3+4, 1+6+4) print(&quot;Matrix's ∞ norm, ord=np.inf:&quot;,np.linalg.norm(x,ord=np.inf,keepdims=True)) print(&quot;***********************&quot;) print(&quot;Matrix's row vector, l1 norm, ord=1, axis=1:\\n&quot;,np.linalg.norm(x,ord=1,axis=1,keepdims=True)) print(&quot;Matrix's row vector, l1 norm, ord=1, axis=1, keepdims=False:\\n&quot;,np.linalg.norm(x,ord=1,axis=1,keepdims=False)) ######### Output ######### &quot;&quot;&quot; Default(sqrt of square sum of all elements, does not return matrix), keepdims=False: 8.831760866327848 Sqrt of square sum of all elements, return matrix, keepdims=True: [[8.83176087]] *********************** Matrix's row vector, l2 norm, axis=1: [[5. ] [7.28010989]] Matrix's column vector, l2 norm, axis=0: [[1. 6.70820393 5.65685425]] *********************** Matrix's l1 norm, ord=1: [[9.]] Matrix's l2 norm, ord=2: [[8.70457079]] Matrix's ∞ norm, ord=np.inf: [[11.]] *********************** Matrix's row vector, l1 norm, ord=1, axis=1: [[ 7.] [11.]] Matrix's row vector, l1 norm, ord=1, axis=1, keepdims=False: [ 7. 11.] &quot;&quot;&quot; parameter description computation ord=1 l1l_1l1​ norm |x1x_1x1​|+|x2x_2x2​|+...+|xnx_nxn​| Default l2l_2l2​ norm x12+x22+...+xn2\\sqrt{x_1^2+x_2^2+...+x_n^2}x12​+x22​+...+xn2​​ ord=∞ l∞l_∞l∞​ norm max( |xix_ixi​| ) ord=1: Maximum of each column's sum ord=∞: Maximum of each row's sum ord=2: Find the eigenvalue, then find the maximum square of the eigenvalue. ans = np.mat(x).T*np.mat(x) [x,y] = np.linalg.eig(ans)) max(np.sqrt(x)) axis: process rule axis=0: processing by column vectors and finds the norm of multiple column vectors axis=1: processing by row vectors and finds the norm of multiple row vectors axis=None: Matrix's norm keepdims: whether to maintain the two-dimensional characteristics of the matrix ","link":"https://zl-wu.github.io/post/mla-gradient-descent-method/"},{"title":"MLA -- Naive Bayes theory","content":"&quot;Priori Probability + data = Posterior probability&quot; 条件概率： P(X|Y) or P(Y|X) 联合概率： P(X,Y), P(XY) or P(XnY) 边缘概率： P(X) or P(Y) Let's look at the conditional independence formula first. If X and Y are independent of each other, there are: P(X,Y)=P(X)×P(Y)P(X, Y) = P(X) \\times P(Y) P(X,Y)=P(X)×P(Y) P(X∣Y)=P(X);P(Y∣X)=P(Y);P(X|Y) = P(X) ; P(Y|X) = P(Y) ; P(X∣Y)=P(X);P(Y∣X)=P(Y); Usually P(X|Y) != P(X) unless X and Y are independent of each other. And standard conditional probability formula: P(X∣Y)=P(X,Y)/P(Y)P(X|Y) = P(X, Y) / P(Y) P(X∣Y)=P(X,Y)/P(Y) P(Y∣X)=P(X,Y)/P(X)P(Y|X) = P(X, Y) / P(X) P(Y∣X)=P(X,Y)/P(X) or =&gt; P(Y∣X)=P(X∣Y)×P(Y)P(X)P(Y|X) = \\frac {P(X|Y) \\times P(Y)}{P(X)} P(Y∣X)=P(X)P(X∣Y)×P(Y)​ Then the total probability formula: P(X)=∑kP(X∣Y=Yk)P(Yk)P(X) = \\sum_kP(X|Y = Y_k)P(Y_k) P(X)=k∑​P(X∣Y=Yk​)P(Yk​) ∑kP(Yk)=1\\sum_kP(Y_k) = 1 k∑​P(Yk​)=1 Therefore, The Bayesian formula is easily derived from the above formulas: P(Yk∣X)=P(X∣Yk)×P(Yk)∑kP(X∣Y=Yk)P(Yk)P(Y_k|X) = \\frac {P(X|Y_k) \\times P(Y_k)} {\\sum_k P(X|Y=Y_k)P(Y_k)} P(Yk​∣X)=∑k​P(X∣Y=Yk​)P(Yk​)P(X∣Yk​)×P(Yk​)​ Question: Suppose there is a latent disease among the people. P(disease) = 0.01 P(well) = 0.99 Doctor can detect this disease by some kinds of kit. The test result is &gt;positive(not healthy) and negtive(healthy). However, detection result &gt;may also be wrong. The conditional probability is : P(positive | well) = 0.01 (healthy people detected with positive) P(negtive | well) = 0.99 P(positive | disease) = 0.99 (unhealthy people detected with positive) P(negtive | disease) = 0.01 So, What is probability of P(disease | postive)? that is, if the detection is positive(bad), what is the probabiliy of true disease？ (Answer: P(disease | postive) = 0.5 ) (We can conclude P(X|Y) and P(Y|X) are totally different. P(X|Y) = P(Y|X)&gt;*P(X)/P(Y)) Bayesian interpretation In the Bayesian (or epistemological) interpretation, probability measures a “degree of belief.” Bayes’ theorem then links the degree of belief in a proposition before and after accounting for evidence. For example, suppose it is believed with 50% certainty that a coin is twice as likely to land heads than tails. If the coin is flipped a number of times and the outcomes observed, that degree of belief may rise, fall or remain the same depending on the results. P(A∣B)=P(B∣A)×P(A)P(B)P(A|B) = \\frac {P(B|A) \\times P(A)}{P(B)} P(A∣B)=P(B)P(B∣A)×P(A)​ For proposition A and evidence B, P (A), the prior, is the initial degree of belief in A. P (A | B), the posterior is the degree of belief having accounted for B. the quotient P(B | A)/P(B) represents the support B provides for A. In theory, the probabilistic model classifier is a conditional probability model: P( C|F1,...,Fn ) Based on the Bayes Formula: P(C∣F1,...,Fn)=P(F1,...,Fn∣C)∗P(C)P(F1,...,Fn)P(C|F1,...,Fn) = \\frac {P(F1,...,Fn|C) * P(C)} {P(F1,...,Fn)} P(C∣F1,...,Fn)=P(F1,...,Fn)P(F1,...,Fn∣C)∗P(C)​ P(类别∣特征)=P(特征∣类别)∗P(类别)P(特征)P(类别|特征) = \\frac {P(特征|类别) * P(类别)} {P(特征)} P(类别∣特征)=P(特征)P(特征∣类别)∗P(类别)​ P(C) is the prior probability P(C|F1,...,Fn) is the posterior probability C represents any Group of the classifier. (F1,...,Fn) represents features of data that we need to predict. We can understand it this way, when we don’t know any features of the sample (predict database) that we need to predict, first determine the probability that the sample is a certain category is P(C), then, after knowing the features of the sample, multiply $ \\frac {P(F1,...,Fn|C)} {P(F1,...,Fn)} $ to get the sample's classification prediction. Denominator is P(F1,...,Fn), we can view it as a constant, since it does not depend on C, and value is same for each group probability prediction. Therefore, what we need to concern is the numerator P(F1,...,Fn|C) * P(C), which is equivalent to the joint distribution model P(C,F1,...,Fn) The Naive Bayes model makes a bold assumption here that ALL FEATURES (F1,...,Fn) are independent of each other, so that we can draw: P(F1,...,Fn∣C)=P(F1∣C)∗P(F2∣C)∗...∗P(Fn∣C)P(F1,...,Fn|C) = P(F1|C)*P(F2|C)*...*P(Fn|C) P(F1,...,Fn∣C)=P(F1∣C)∗P(F2∣C)∗...∗P(Fn∣C) Then the numerator P(F1,...,Fn|C) * P(C) is equivalent to: P(C)∗∏i=1nP(Fi∣C)P(C)* \\prod_{i=1}^n P(F_i|C) P(C)∗i=1∏n​P(Fi​∣C) Suppose there are k groups for classification: C1, C2, ... Ck. Then the prediction result formula of Naive Bayes Classifier is: Cresult=arg max⁡C1,C2,...CkP(Ck)∗∏i=1nP(Fi∣Ck)C_{result} = \\argmax_{C1, C2, ... Ck} P(C_k)* \\prod_{i=1}^n P(F_i|C_k) Cresult​=C1,C2,...Ckargmax​P(Ck​)∗i=1∏n​P(Fi​∣Ck​) classify(f1,...fn)=arg max⁡C1,C2,...CkP(C=c)∗∏i=1nP(Fi=fi∣C=c)classify(f1,...fn) = \\argmax_{C1, C2, ... Ck} P(C = c)* \\prod_{i=1}^n P(F_i=f_i|C=c) classify(f1,...fn)=C1,C2,...Ckargmax​P(C=c)∗i=1∏n​P(Fi​=fi​∣C=c) We calculate all the probabilities that the predicted data belong to each group, and then, by comparison, classify the data into the group with the highest probability (the most probable group). Naive Bayes parameter estimation In the previous section, we knew the formula, as long as we compare which group has largest $ P(C = c)* \\prod_{i=1}^n P(F_i=f_i|C=c) $. In this section, we discuss how to calculate this two kinds of probability. P(C=Ck) is easy, by maximum likelihood estimation we can easily get the frequency of occurrence of Ck. If the total number of sample is M, number of occurrences of Ck is Mk. Then P(C=Ck) = Mk/M. Therefore, the prior probability P(C=Ck) is very dependent on the selection of the overall sample. If the size of dataset is not big enough, or the propotion of count of each category is uneven, it will greatly affect the prediction of Naive Bayes Classification. P(Fi=fi | C=Ck) is a bit complicated, it depends on the data type of Fi. If the Fi is the Discrete Value: Polynomial distributionP(Fi=fi∣C=Ck)=MkfiMkP(Fi=fi | C=Ck) = \\frac{M_{kfi}}{M_k} P(Fi=fi∣C=Ck)=Mk​Mkfi​​ Mkfi is the number of occurrences of fi in Mk. Somtime, Mkfi may be 0, which will affect he estimation of posterior probability. The solution is to import Laplace smoothing:P(Fi=fi∣C=Ck)=Mkfi+λMk+MfiλP(Fi=fi | C=Ck) = \\frac{M_{kfi} + \\lambda}{M_k + M_{fi}\\lambda} P(Fi=fi∣C=Ck)=Mk​+Mfi​λMkfi​+λ​ λ is a constant larger than 0, usually 1. Mfi is number of fi value. If the Fi is the Very Sparse discrete values: Bernoulli distributionP(Fi=fi∣C=Ck)=P(Fi=1∣C=Ck)∗fi+(1−P(Fi=1∣C=Ck))∗(1−fi)P(Fi=fi | C=Ck) = P(Fi=1|C=Ck)*fi + (1-P(Fi=1|C=Ck))*(1-fi) P(Fi=fi∣C=Ck)=P(Fi=1∣C=Ck)∗fi+(1−P(Fi=1∣C=Ck))∗(1−fi) fi is 0 or 1. if fi appears in the subset Mk, fi is 1, otherwise 0. If the Fi is a very sparse discrete value, that is, the occurrence probability of each feature is very low, then we can assume Fi in line with Bernoulli distribution. Feature fi appears as 1, does not appear as 0. We only pay attention to whether fi appears, not the number of occurrences. If the Fi is the Continuous Value, we usually take Fi's prior probability as normally distributed: P(Fi=fi∣C=Ck)=12πσk2∗e−(fi−μk)22σk2P(Fi=fi | C=C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} * e^{-\\frac{(fi - \\mu_k)^2}{2\\sigma_k^2}} P(Fi=fi∣C=Ck​)=2πσk2​​1​∗e−2σk2​(fi−μk​)2​ # Continuous Data Code import math pi = math.pi e = math.e def probabilityDensity(mu, sig2, v): res = 1/math.sqrt(2*pi*sig2) * e ** (-(v-mu)**2/(2*sig2)) return format(res,'.4e') Naive Bayes algorithm process Step one: Input the prior probability P(C=Ck), if the P(C=Ck) is Null, P(C=Ck) = (Mk + λ) / (M + Kλ) Step two: Calculate the conditional probability of each feature of the k-th category. Condition 1: Feature i is the Discrete Value:P(Fi=fi∣C=Ck)=Mkfi+λMk+MfiλP(Fi=fi | C=Ck) = \\frac{M_{kfi} + \\lambda}{M_k + M_{fi}\\lambda} P(Fi=fi∣C=Ck)=Mk​+Mfi​λMkfi​+λ​ Condition 2: Feature i is the Sparse Discrete Value:P(Fi=fi∣C=Ck)=P(Fi=1∣C=Ck)∗fi+(1−P(Fi=1∣C=Ck))∗(1−fi)P(Fi=fi | C=Ck) = P(Fi=1|C=Ck)*fi + (1-P(Fi=1|C=Ck))*(1-fi) P(Fi=fi∣C=Ck)=P(Fi=1∣C=Ck)∗fi+(1−P(Fi=1∣C=Ck))∗(1−fi) Condition 1: Feature i is the Continuous Value:P(Fi=fi∣C=Ck)=12πσk2∗e−(fi−μk)22σk2P(Fi=fi | C=C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} * e^{-\\frac{(fi - \\mu_k)^2}{2\\sigma_k^2}} P(Fi=fi∣C=Ck​)=2πσk2​​1​∗e−2σk2​(fi−μk​)2​ Step three: For the instance prediction features (pf1,...pfn), Calculate posterior probability separately: P(C=Ck)∗∏i=1nP(Fi=pfi∣Ck)P(C=C_k) * \\prod_{i=1}^n P(F_i = pf_i | C_k) P(C=Ck​)∗i=1∏n​P(Fi​=pfi​∣Ck​) Determine the classification of prediction data (pf1,...pfn): Cresult=arg max⁡C1,C2,...CkP(Ck)∏i=1nP(Fi=pfi∣Ck)C_{result} = \\argmax_{C_1, C_2,...C_k} P(C_k) \\prod_{i=1}^n P(F_i = pf_i | C_k) Cresult​=C1​,C2​,...Ck​argmax​P(Ck​)i=1∏n​P(Fi​=pfi​∣Ck​) Pros and cons Conclusion Advantages of Naive Bayes: The naive Bayesian model originates from classical mathematical theory and has a stable classification efficiency. Performs well on small-scale data, can handle multi-classification tasks, suitable for incremental training, especially when the amount of data exceeds the memory, we can batch-by-batch incremental training. Less sensitive to missing data, and the algorithm is relatively simple, often used for text classification. Disadvantages of Naive Bayes: In theory, the Naive Bayes model has the smallest error rate compared with other classification methods. However, this is not always the case, because the naive Bayes model gives an output category, and assumes that the attributes are mutually independent. This assumption is often not true in practical applications. When the correlation between them is large, the classification effect is not good. When the attribute correlation is small, Naive Bayes has the best performance. For this, there are algorithms such as semi-Naive Bayes that are moderately improved by considering partial relevance. Need to know the a priori probability, and the priori probability often depends on the hypothesis, there can be many models of hypothesis, so in some cases, due to the hypothesis of the prior model, the prediction effect is not good. Since we decide the classification by prior and data to determine the posterior probability, the classification decision has a certain error rate. Very sensitive to the type of input data: discrete, sparse discrete or contiuous data. ","link":"https://zl-wu.github.io/post/mla-naive-bayes-theory/"}]}